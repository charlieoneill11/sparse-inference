Training on 40.96M total tokens.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.01it/s]
/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loaded pretrained model gpt2-small into HookedTransformer
Moving model to device:  cpu
Moving model to device:  cpu
Using Hugging Face token: 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
Batch [1/5000], Avg Reconstruction Loss: 1.306476, Avg L1 Loss: 20857.691406, Avg L0 Loss: 8457.578125, Avg Total Loss: 3.392246
Traceback (most recent call last):
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 433, in <module>
    main(args)
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 354, in main
    train(
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 261, in train
    checkpoint_path = f"layer_{args.layer}_hidden_{args.projections_up * args.input_dim}_l1_{args.l1_weight}_{save_path}"  # This will always save to the same path
                                                   ^^^^^^^^^^^^^^^^^^^
AttributeError: 'Namespace' object has no attribute 'projections_up'. Did you mean: 'projection_up'?
