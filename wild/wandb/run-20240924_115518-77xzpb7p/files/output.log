Training on 40.96M total tokens.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.03s/it]
/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loaded pretrained model gpt2-small into HookedTransformer
Moving model to device:  cpu
Moving model to device:  cpu
Using Hugging Face token: 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
Batch [1/5000], Avg Reconstruction Loss: 1.306476, Avg L1 Loss: 20857.691406, Avg L0 Loss: 8457.578125, Avg Total Loss: 3.392246
Model checkpoint saved to models/sparse_autoencoder.pth_batch_1_1727142934.pth
Repository 'charlieoneill/sparse-coding' already exists on Hugging Face.
/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.
For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.
  warnings.warn(warning_message, FutureWarning)
Cloning https://huggingface.co/charlieoneill/sparse-coding into local empty directory.
Upload file sparse_autoencoder.pth_batch_1_1727142934.pth: 107MB [00:49, 2.60MB/s]                                                        To https://huggingface.co/charlieoneill/sparse-coding
   2bb75ab..cedf10e  main -> main

Upload file sparse_autoencoder.pth_batch_1_1727142934.pth: 100%|█████████████████████████████████████| 99.1M/99.1M [00:50<00:00, 2.07MB/s]
Model 'sparse_autoencoder.pth_batch_1_1727142934.pth' uploaded to Hugging Face repository 'charlieoneill/sparse-coding' with commit message 'Checkpoint at batch 1'.
Batch [2/5000], Avg Reconstruction Loss: 105.396210, Avg L1 Loss: 11423.847656, Avg L0 Loss: 5038.372070, Avg Total Loss: 106.538597
Model checkpoint saved to models/sparse_autoencoder.pth_batch_2_1727142998.pth
Repository 'charlieoneill/sparse-coding' already exists on Hugging Face.
/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:215: UserWarning: Both `token` and `use_auth_token` are passed to `__init__` with non-None values. `token` is now the preferred argument to pass a User Access Token. `use_auth_token` value will be ignored.
  warnings.warn(
/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.
For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.
  warnings.warn(warning_message, FutureWarning)
/Users/charlesoneill/sparse-inference/wild/./charlieoneill_sparse-coding is already a clone of https://huggingface.co/charlieoneill/sparse-coding. Make sure you pull the latest changes with `repo.git_pull()`.
Upload file sparse_autoencoder.pth_batch_2_1727142998.pth: 107MB [00:47, 2.62MB/s]                                                        To https://huggingface.co/charlieoneill/sparse-coding
   cedf10e..ec0e6fd  main -> main

Upload file sparse_autoencoder.pth_batch_2_1727142998.pth: 100%|█████████████████████████████████████| 99.1M/99.1M [00:49<00:00, 2.11MB/s]
Model 'sparse_autoencoder.pth_batch_2_1727142998.pth' uploaded to Hugging Face repository 'charlieoneill/sparse-coding' with commit message 'Checkpoint at batch 2'.
Traceback (most recent call last):
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 687, in <module>
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 608, in main
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 466, in train
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py", line 627, in run_with_cache
    out, cache_dict = super().run_with_cache(
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/hook_points.py", line 513, in run_with_cache
    model_out = self(*model_args, **model_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py", line 550, in forward
    residual = block(
               ^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/components.py", line 1601, in forward
    mlp_out = self.hook_mlp_out(self.mlp(normalized_resid_mid))  # [batch, pos, d_model]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/components.py", line 1276, in forward
    post_act = self.hook_post(self.act_fn(pre_act))  # [batch, pos, d_mlp]
                              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/utils.py", line 162, in gelu_new
    * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
