Training on 409.6M total tokens.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.07it/s]
Loaded pretrained model gpt2-small into HookedTransformer
Moving model to device:  cpu
Moving model to device:  cpu
Using Hugging Face token: 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
Batch [1/50000], Avg Reconstruction Loss: 1.306476, Avg L1 Loss: 20857.693359, Avg L0 Loss: 8457.578125, Avg Total Loss: 3.392245
Batch [2/50000], Avg Reconstruction Loss: 105.396255, Avg L1 Loss: 11423.849609, Avg L0 Loss: 5038.374512, Avg Total Loss: 106.538643
Batch [3/50000], Avg Reconstruction Loss: 3.150745, Avg L1 Loss: 6858.627930, Avg L0 Loss: 3130.558838, Avg Total Loss: 3.836608
Batch [4/50000], Avg Reconstruction Loss: 6.351226, Avg L1 Loss: 4536.641602, Avg L0 Loss: 2143.622559, Avg Total Loss: 6.804890
Batch [5/50000], Avg Reconstruction Loss: 11.267883, Avg L1 Loss: 2881.411377, Avg L0 Loss: 1394.782227, Avg Total Loss: 11.556025
Batch [6/50000], Avg Reconstruction Loss: 7.548736, Avg L1 Loss: 1868.674683, Avg L0 Loss: 990.445557, Avg Total Loss: 7.735603
Batch [7/50000], Avg Reconstruction Loss: 3.430811, Avg L1 Loss: 1129.874512, Avg L0 Loss: 669.931030, Avg Total Loss: 3.543798
Batch [8/50000], Avg Reconstruction Loss: 1.654572, Avg L1 Loss: 674.669434, Avg L0 Loss: 464.645508, Avg Total Loss: 1.722039
Batch [9/50000], Avg Reconstruction Loss: 1.181916, Avg L1 Loss: 401.330719, Avg L0 Loss: 323.822510, Avg Total Loss: 1.222049
Batch [10/50000], Avg Reconstruction Loss: 1.116335, Avg L1 Loss: 273.301025, Avg L0 Loss: 249.764771, Avg Total Loss: 1.143665
Batch [11/50000], Avg Reconstruction Loss: 1.116303, Avg L1 Loss: 177.475281, Avg L0 Loss: 178.702148, Avg Total Loss: 1.134051
Batch [12/50000], Avg Reconstruction Loss: 1.121131, Avg L1 Loss: 134.794312, Avg L0 Loss: 147.649292, Avg Total Loss: 1.134610
Batch [13/50000], Avg Reconstruction Loss: 1.132427, Avg L1 Loss: 87.338608, Avg L0 Loss: 98.711914, Avg Total Loss: 1.141161
Batch [14/50000], Avg Reconstruction Loss: 1.128516, Avg L1 Loss: 67.249969, Avg L0 Loss: 81.108276, Avg Total Loss: 1.135241
Batch [15/50000], Avg Reconstruction Loss: 1.122051, Avg L1 Loss: 48.853539, Avg L0 Loss: 60.524536, Avg Total Loss: 1.126936
Batch [16/50000], Avg Reconstruction Loss: 1.122277, Avg L1 Loss: 45.212997, Avg L0 Loss: 54.797485, Avg Total Loss: 1.126799
Batch [17/50000], Avg Reconstruction Loss: 1.120138, Avg L1 Loss: 34.403580, Avg L0 Loss: 43.189331, Avg Total Loss: 1.123579
Batch [18/50000], Avg Reconstruction Loss: 1.122832, Avg L1 Loss: 30.868576, Avg L0 Loss: 37.597290, Avg Total Loss: 1.125919
Batch [19/50000], Avg Reconstruction Loss: 1.121513, Avg L1 Loss: 19.011984, Avg L0 Loss: 25.039917, Avg Total Loss: 1.123414
Batch [20/50000], Avg Reconstruction Loss: 1.119780, Avg L1 Loss: 28.616936, Avg L0 Loss: 34.842407, Avg Total Loss: 1.122642
Batch [21/50000], Avg Reconstruction Loss: 1.121116, Avg L1 Loss: 16.359087, Avg L0 Loss: 21.577881, Avg Total Loss: 1.122752
Batch [22/50000], Avg Reconstruction Loss: 1.118868, Avg L1 Loss: 19.142452, Avg L0 Loss: 24.181763, Avg Total Loss: 1.120782
Batch [23/50000], Avg Reconstruction Loss: 1.123500, Avg L1 Loss: 25.041586, Avg L0 Loss: 27.758423, Avg Total Loss: 1.126005
Batch [24/50000], Avg Reconstruction Loss: 1.120976, Avg L1 Loss: 13.312033, Avg L0 Loss: 16.944580, Avg Total Loss: 1.122308
Batch [25/50000], Avg Reconstruction Loss: 1.115351, Avg L1 Loss: 20.565872, Avg L0 Loss: 26.549805, Avg Total Loss: 1.117407
Batch [26/50000], Avg Reconstruction Loss: 1.120424, Avg L1 Loss: 17.128874, Avg L0 Loss: 20.323486, Avg Total Loss: 1.122137
Batch [27/50000], Avg Reconstruction Loss: 1.121933, Avg L1 Loss: 10.496386, Avg L0 Loss: 13.164429, Avg Total Loss: 1.122983
Batch [28/50000], Avg Reconstruction Loss: 1.111124, Avg L1 Loss: 22.078650, Avg L0 Loss: 27.329834, Avg Total Loss: 1.113332
Batch [29/50000], Avg Reconstruction Loss: 1.123513, Avg L1 Loss: 8.206160, Avg L0 Loss: 10.554565, Avg Total Loss: 1.124334
Batch [30/50000], Avg Reconstruction Loss: 1.118701, Avg L1 Loss: 9.056562, Avg L0 Loss: 11.671631, Avg Total Loss: 1.119607
Batch [31/50000], Avg Reconstruction Loss: 1.118145, Avg L1 Loss: 11.469610, Avg L0 Loss: 14.099854, Avg Total Loss: 1.119292
Batch [32/50000], Avg Reconstruction Loss: 1.116644, Avg L1 Loss: 13.361500, Avg L0 Loss: 15.917847, Avg Total Loss: 1.117980
Batch [33/50000], Avg Reconstruction Loss: 1.117068, Avg L1 Loss: 9.905776, Avg L0 Loss: 12.377686, Avg Total Loss: 1.118059
Batch [34/50000], Avg Reconstruction Loss: 1.117065, Avg L1 Loss: 11.031886, Avg L0 Loss: 13.598511, Avg Total Loss: 1.118168
Batch [35/50000], Avg Reconstruction Loss: 1.121162, Avg L1 Loss: 7.079596, Avg L0 Loss: 8.764893, Avg Total Loss: 1.121870
Batch [36/50000], Avg Reconstruction Loss: 1.117869, Avg L1 Loss: 14.049619, Avg L0 Loss: 16.551147, Avg Total Loss: 1.119274
Batch [37/50000], Avg Reconstruction Loss: 1.117947, Avg L1 Loss: 7.925262, Avg L0 Loss: 10.056152, Avg Total Loss: 1.118740
Batch [38/50000], Avg Reconstruction Loss: 1.119837, Avg L1 Loss: 6.511777, Avg L0 Loss: 8.208618, Avg Total Loss: 1.120488
Batch [39/50000], Avg Reconstruction Loss: 1.117553, Avg L1 Loss: 7.281663, Avg L0 Loss: 9.538818, Avg Total Loss: 1.118282
Batch [40/50000], Avg Reconstruction Loss: 1.118139, Avg L1 Loss: 6.746301, Avg L0 Loss: 8.628540, Avg Total Loss: 1.118814
Batch [41/50000], Avg Reconstruction Loss: 1.123048, Avg L1 Loss: 10.918103, Avg L0 Loss: 11.873047, Avg Total Loss: 1.124140
Batch [42/50000], Avg Reconstruction Loss: 1.119345, Avg L1 Loss: 6.141498, Avg L0 Loss: 7.982300, Avg Total Loss: 1.119959
Batch [43/50000], Avg Reconstruction Loss: 1.119237, Avg L1 Loss: 5.406383, Avg L0 Loss: 7.265625, Avg Total Loss: 1.119777
Batch [44/50000], Avg Reconstruction Loss: 1.120968, Avg L1 Loss: 7.481777, Avg L0 Loss: 9.043945, Avg Total Loss: 1.121716
Batch [45/50000], Avg Reconstruction Loss: 1.119194, Avg L1 Loss: 5.568275, Avg L0 Loss: 7.485229, Avg Total Loss: 1.119750
Traceback (most recent call last):
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 1070, in <module>
    main(args)
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 990, in main
    train(
  File "/Users/charlesoneill/sparse-inference/wild/main.py", line 849, in train
    _, cache = transformer.run_with_cache(batch_tokens)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py", line 627, in run_with_cache
    out, cache_dict = super().run_with_cache(
                      ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/hook_points.py", line 513, in run_with_cache
    model_out = self(*model_args, **model_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py", line 550, in forward
    residual = block(
               ^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/components.py", line 1601, in forward
    mlp_out = self.hook_mlp_out(self.mlp(normalized_resid_mid))  # [batch, pos, d_model]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/components.py", line 1276, in forward
    post_act = self.hook_post(self.act_fn(pre_act))  # [batch, pos, d_mlp]
                              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/utils.py", line 162, in gelu_new
    * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
