{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import einops\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader  \n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "class LCA(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dict_size, lambd=0.1, lr=None, max_iter=100, \n",
    "                 fac=0.5, tol=1e-6, device='cuda', verbose=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dict_size = dict_size\n",
    "        self.D = torch.nn.Parameter(torch.randn(input_dim, dict_size))\n",
    "        self.normalize_dictionary()\n",
    "        self.lambd = lambd\n",
    "        self.lr = lr if lr is not None else 1.0 / dict_size\n",
    "        self.max_iter = max_iter\n",
    "        self.facs = [fac, 1/fac]\n",
    "        self.tol = tol\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def normalize_dictionary(self):\n",
    "        with torch.no_grad():\n",
    "            self.D.data = F.normalize(self.D.data, dim=0)\n",
    "\n",
    "    def inference(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        u = torch.zeros(batch_size, self.dict_size, device=self.device)\n",
    "        a = torch.relu(u)\n",
    "        \n",
    "        if isinstance(self.lr, float):\n",
    "            lr = torch.full((batch_size,), self.lr, device=self.device)\n",
    "        else:\n",
    "            lr = self.lr.clone()\n",
    "        \n",
    "        best_loss = torch.full((batch_size,), float('inf'), device=self.device)\n",
    "        \n",
    "        for iter_idx in range(self.max_iter):\n",
    "            rec = torch.matmul(a, self.D.T)\n",
    "            \n",
    "            recon_error = torch.mean((x - rec) ** 2, dim=1)\n",
    "            l1_penalty = self.lambd * torch.mean(torch.abs(a), dim=1)\n",
    "            loss = recon_error + l1_penalty\n",
    "            \n",
    "            if self.verbose and iter_idx % 10 == 0:\n",
    "                avg_loss = loss.mean().item()\n",
    "                avg_recon = recon_error.mean().item()\n",
    "                avg_l1 = l1_penalty.mean().item()\n",
    "                sparsity = (a > 0).float().mean().item() * 100\n",
    "                print(f\"LCA Iteration {iter_idx}: \"\n",
    "                      f\"Loss = {avg_loss:.6f}, \"\n",
    "                      f\"Recon = {avg_recon:.6f}, \"\n",
    "                      f\"L1 = {avg_l1:.6f}, \"\n",
    "                      f\"Sparsity = {sparsity:.1f}%\")\n",
    "            \n",
    "            if torch.max(best_loss - loss) < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(f\"Converged at iteration {iter_idx}\")\n",
    "                break\n",
    "            \n",
    "            best_loss = loss\n",
    "            du = torch.matmul((rec - x), self.D) + self.lambd\n",
    "            \n",
    "            losses = []\n",
    "            u_candidates = []\n",
    "            a_candidates = []\n",
    "            \n",
    "            for fac in self.facs:\n",
    "                lr_expanded = lr.view(-1, 1)\n",
    "                u_new = u - du * (lr_expanded * fac)\n",
    "                a_new = torch.relu(u_new)\n",
    "                rec_new = torch.matmul(a_new, self.D.T)\n",
    "                \n",
    "                recon_error_new = torch.mean((x - rec_new) ** 2, dim=1)\n",
    "                l1_penalty_new = self.lambd * torch.mean(torch.abs(a_new), dim=1)\n",
    "                loss_new = recon_error_new + l1_penalty_new\n",
    "                \n",
    "                losses.append(loss_new)\n",
    "                u_candidates.append(u_new)\n",
    "                a_candidates.append(a_new)\n",
    "            \n",
    "            losses = torch.stack(losses, dim=0)\n",
    "            best_idx = torch.argmin(losses, dim=0)\n",
    "            \n",
    "            u = torch.stack([\n",
    "                u_candidates[best_idx[i]][i] for i in range(batch_size)\n",
    "            ])\n",
    "            \n",
    "            a = torch.relu(u)\n",
    "            lr = lr * torch.tensor([self.facs[idx.item()] for idx in best_idx], \n",
    "                                 device=self.device)\n",
    "        \n",
    "        return a, rec, recon_error.mean(), l1_penalty.mean()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.normalize_dictionary()\n",
    "        a = self.inference(x)\n",
    "        return a\n",
    "\n",
    "def normalized_mse(recon, xs):\n",
    "    mse = F.mse_loss(recon, xs)\n",
    "    mean_xs = xs.mean(dim=0, keepdim=True).expand_as(xs)\n",
    "    mse_mean = F.mse_loss(mean_xs, xs)\n",
    "    epsilon = 1e-8\n",
    "    return mse / (mse_mean + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552553abedf7476a857aaa01d16b1276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lca_model.pth:   0%|          | 0.00/51.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n",
      "LCA Iteration 0: Loss = 220.008682, Recon = 220.008682, L1 = 0.000000, Sparsity = 0.0%\n",
      "LCA Iteration 10: Loss = 116.371948, Recon = 116.361961, L1 = 0.009995, Sparsity = 30.9%\n",
      "LCA Iteration 20: Loss = 29.948868, Recon = 29.928413, L1 = 0.020456, Sparsity = 22.3%\n",
      "LCA Iteration 30: Loss = 9.150305, Recon = 9.126734, L1 = 0.023570, Sparsity = 21.0%\n",
      "LCA Iteration 40: Loss = 2.856406, Recon = 2.831342, L1 = 0.025064, Sparsity = 20.0%\n",
      "LCA Iteration 50: Loss = 0.950766, Recon = 0.925015, L1 = 0.025751, Sparsity = 19.2%\n",
      "LCA Iteration 60: Loss = 0.306013, Recon = 0.279994, L1 = 0.026019, Sparsity = 18.3%\n",
      "LCA Iteration 70: Loss = 0.134977, Recon = 0.108981, L1 = 0.025996, Sparsity = 17.5%\n",
      "LCA Iteration 80: Loss = 0.070541, Recon = 0.044646, L1 = 0.025895, Sparsity = 16.8%\n",
      "LCA Iteration 90: Loss = 0.051264, Recon = 0.025491, L1 = 0.025774, Sparsity = 16.3%\n",
      "\n",
      "Results:\n",
      "Reconstruction loss: 0.018980\n",
      "L1 loss: 0.025655\n",
      "Sparsity: 15.8%\n",
      "Dictionary max/min/mean: 0.187/-0.192/-0.000\n",
      "\n",
      "Feature usage statistics:\n",
      "Mean feature usage: 0.158\n",
      "Std feature usage: 0.255\n",
      "Dead features (never activated): 6172\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "repo_name = \"charlieoneill/sparse-coding-lca\"\n",
    "model_filename = \"lca_model.pth\"\n",
    "input_dim = 768\n",
    "dict_size = 16896\n",
    "device = 'cpu'\n",
    "layer = 9\n",
    "lambd = 0.1  # Sparsity parameter\n",
    "\n",
    "print(\"Loading model...\")\n",
    "# Download and load the model\n",
    "model_path = hf_hub_download(repo_id=repo_name, filename=model_filename)\n",
    "\n",
    "# Initialize model\n",
    "lca = LCA(\n",
    "    input_dim=input_dim,\n",
    "    dict_size=dict_size,\n",
    "    lambd=lambd,\n",
    "    max_iter=100,\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Load saved weights\n",
    "lca.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "lca.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Load transformer and activation store\n",
    "hook_point = \"blocks.8.hook_resid_pre\"\n",
    "saes, _ = get_gpt2_res_jb_saes(hook_point)\n",
    "sparse_autoencoder = saes[hook_point]\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "sparse_autoencoder.cfg.hook_point = f\"blocks.{layer}.attn.hook_z\"\n",
    "sparse_autoencoder.cfg.store_batch_size = 64\n",
    "\n",
    "loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "transformer_model, _, activation_store = loader.load_sae_training_group_session()\n",
    "\n",
    "print(\"Running inference...\")\n",
    "# Get activations and run inference\n",
    "batch_tokens = activation_store.get_batch_tokens().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, cache = transformer_model.run_with_cache(batch_tokens)\n",
    "    X = cache[\"resid_pre\", layer]\n",
    "    X = einops.rearrange(X, \"batch pos d_model -> (batch pos) d_model\")\n",
    "    X = X[:64]  # Ensure batch size of 64\n",
    "    del cache\n",
    "    \n",
    "    # Run LCA inference\n",
    "    S_, X_, recon_loss, l1_loss = lca.inference(X)\n",
    "    \n",
    "    # Calculate sparsity\n",
    "    sparsity = (S_ > 0).float().mean().item() * 100\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Reconstruction loss: {recon_loss.item():.6f}\")\n",
    "    print(f\"L1 loss: {l1_loss.item():.6f}\")\n",
    "    print(f\"Sparsity: {sparsity:.1f}%\")\n",
    "    print(f\"Dictionary max/min/mean: {lca.D.max():.3f}/{lca.D.min():.3f}/{lca.D.mean():.3f}\")\n",
    "    \n",
    "    # Optional: analyze feature usage\n",
    "    feature_usage = (S_ > 0).float().mean(dim=0)\n",
    "    print(f\"\\nFeature usage statistics:\")\n",
    "    print(f\"Mean feature usage: {feature_usage.mean().item():.3f}\")\n",
    "    print(f\"Std feature usage: {feature_usage.std().item():.3f}\")\n",
    "    print(f\"Dead features (never activated): {(feature_usage == 0).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import einops\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# ----------------------------- Configuration -----------------------------\n",
    "\n",
    "# Parameters\n",
    "REPO_NAME = \"charlieoneill/sparse-coding-lca\"\n",
    "MODEL_FILENAME = \"lca_model.pth\"\n",
    "INPUT_DIM = 768\n",
    "DICT_SIZE = 16896\n",
    "SCORES_PATH = \"lca_scores.npy\"\n",
    "FEATURE_INDICES = list(range(100))  # Example: analyze first 100 dictionary elements\n",
    "TOP_K = 10\n",
    "LAYER = 9\n",
    "CONFIG_PATH = \"config.yaml\"\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the appropriate device (CUDA if available, else CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Test CUDA availability\n",
    "            torch.zeros(1).cuda()\n",
    "            return 'cuda'\n",
    "        except:\n",
    "            return 'cpu'\n",
    "    return 'cpu'\n",
    "\n",
    "class LCA(nn.Module):\n",
    "    def __init__(self, input_dim, dict_size, lambd=0.1, lr=None, max_iter=100, \n",
    "                 fac=0.5, tol=1e-6, verbose=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dict_size = dict_size\n",
    "        self.D = nn.Parameter(torch.randn(input_dim, dict_size))\n",
    "        self.normalize_dictionary()\n",
    "        self.lambd = lambd\n",
    "        self.lr = lr if lr is not None else 1.0 / dict_size\n",
    "        self.max_iter = max_iter\n",
    "        self.facs = [fac, 1/fac]\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def normalize_dictionary(self):\n",
    "        with torch.no_grad():\n",
    "            self.D.data = F.normalize(self.D.data, dim=0)\n",
    "\n",
    "    def inference(self, x):\n",
    "        device = x.device\n",
    "        batch_size = x.shape[0]\n",
    "        u = torch.zeros(batch_size, self.dict_size, device=device)\n",
    "        a = torch.relu(u)\n",
    "        \n",
    "        if isinstance(self.lr, float):\n",
    "            lr = torch.full((batch_size,), self.lr, device=device)\n",
    "        else:\n",
    "            lr = self.lr.clone().to(device)\n",
    "        \n",
    "        best_loss = torch.full((batch_size,), float('inf'), device=device)\n",
    "        \n",
    "        # Track optimization metrics\n",
    "        loss_history = []\n",
    "        recon_history = []\n",
    "        l1_history = []\n",
    "        sparsity_history = []\n",
    "        lr_history = []\n",
    "        \n",
    "        print(f\"\\nStarting LCA inference on tensor of shape {x.shape}\")\n",
    "        print(f\"Initial lr: {lr[0].item():.6f}\")\n",
    "        \n",
    "        for iter_idx in range(self.max_iter):\n",
    "            rec = torch.matmul(a, self.D.T)\n",
    "            recon_error = torch.mean((x - rec) ** 2, dim=1)\n",
    "            l1_penalty = self.lambd * torch.mean(torch.abs(a), dim=1)\n",
    "            loss = recon_error + l1_penalty\n",
    "            \n",
    "            # Calculate current metrics\n",
    "            avg_loss = loss.mean().item()\n",
    "            avg_recon = recon_error.mean().item()\n",
    "            avg_l1 = l1_penalty.mean().item()\n",
    "            sparsity = (a > 0).float().mean().item() * 100\n",
    "            avg_lr = lr.mean().item()\n",
    "            \n",
    "            # Store history\n",
    "            loss_history.append(avg_loss)\n",
    "            recon_history.append(avg_recon)\n",
    "            l1_history.append(avg_l1)\n",
    "            sparsity_history.append(sparsity)\n",
    "            lr_history.append(avg_lr)\n",
    "            \n",
    "            if iter_idx % 10 == 0 or iter_idx == self.max_iter - 1:\n",
    "                print(f\"\\nIteration {iter_idx}:\")\n",
    "                print(f\"  Loss: {avg_loss:.6f}\")\n",
    "                print(f\"  Reconstruction Error: {avg_recon:.6f}\")\n",
    "                print(f\"  L1 Penalty: {avg_l1:.6f}\")\n",
    "                print(f\"  Sparsity: {sparsity:.1f}%\")\n",
    "                print(f\"  Learning Rate: {avg_lr:.6f}\")\n",
    "                print(f\"  Active Features: {int((a > 0).sum().item() / batch_size)}/{self.dict_size}\")\n",
    "                \n",
    "                # Print distribution of activations if verbose\n",
    "                if self.verbose:\n",
    "                    with torch.no_grad():\n",
    "                        active_vals = a[a > 0]\n",
    "                        if len(active_vals) > 0:\n",
    "                            print(f\"  Activation stats:\")\n",
    "                            print(f\"    Mean: {active_vals.mean().item():.6f}\")\n",
    "                            print(f\"    Std: {active_vals.std().item():.6f}\")\n",
    "                            print(f\"    Max: {active_vals.max().item():.6f}\")\n",
    "                            print(f\"    Min: {active_vals.min().item():.6f}\")\n",
    "            \n",
    "            if torch.max(best_loss - loss) < self.tol:\n",
    "                print(f\"\\nConverged at iteration {iter_idx}\")\n",
    "                print(f\"Final sparsity: {sparsity:.1f}%\")\n",
    "                print(f\"Final reconstruction error: {avg_recon:.6f}\")\n",
    "                break\n",
    "            \n",
    "            best_loss = loss\n",
    "            du = torch.matmul((rec - x), self.D) + self.lambd\n",
    "            \n",
    "            losses = []\n",
    "            u_candidates = []\n",
    "            a_candidates = []\n",
    "            \n",
    "            for fac in self.facs:\n",
    "                lr_expanded = lr.view(-1, 1)\n",
    "                u_new = u - du * (lr_expanded * fac)\n",
    "                a_new = torch.relu(u_new)\n",
    "                rec_new = torch.matmul(a_new, self.D.T)\n",
    "                \n",
    "                recon_error_new = torch.mean((x - rec_new) ** 2, dim=1)\n",
    "                l1_penalty_new = self.lambd * torch.mean(torch.abs(a_new), dim=1)\n",
    "                loss_new = recon_error_new + l1_penalty_new\n",
    "                \n",
    "                losses.append(loss_new)\n",
    "                u_candidates.append(u_new)\n",
    "                a_candidates.append(a_new)\n",
    "            \n",
    "            losses = torch.stack(losses, dim=0)\n",
    "            best_idx = torch.argmin(losses, dim=0)\n",
    "            \n",
    "            u = torch.stack([\n",
    "                u_candidates[best_idx[i]][i] for i in range(batch_size)\n",
    "            ])\n",
    "            \n",
    "            a = torch.relu(u)\n",
    "            lr = lr * torch.tensor([self.facs[idx.item()] for idx in best_idx], \n",
    "                                device=device)\n",
    "            \n",
    "            # Print learning rate adaptation info if verbose\n",
    "            if self.verbose and (iter_idx % 10 == 0):\n",
    "                fac_counts = torch.bincount(best_idx, minlength=len(self.facs))\n",
    "                print(\"\\nLearning rate adaptation:\")\n",
    "                for i, count in enumerate(fac_counts):\n",
    "                    print(f\"  Factor {self.facs[i]}: chosen {count.item()} times\")\n",
    "        \n",
    "        # Print final statistics\n",
    "        print(\"\\nOptimization complete:\")\n",
    "        print(f\"Initial loss: {loss_history[0]:.6f}\")\n",
    "        print(f\"Final loss: {loss_history[-1]:.6f}\")\n",
    "        print(f\"Loss reduction: {(1 - loss_history[-1]/loss_history[0])*100:.1f}%\")\n",
    "        print(f\"Initial sparsity: {sparsity_history[0]:.1f}%\")\n",
    "        print(f\"Final sparsity: {sparsity_history[-1]:.1f}%\")\n",
    "        print(f\"Initial lr: {lr_history[0]:.6f}\")\n",
    "        print(f\"Final lr: {lr_history[-1]:.6f}\")\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.normalize_dictionary()\n",
    "        self.D = self.D.to(x.device)\n",
    "        a = self.inference(x)\n",
    "        return a\n",
    "\n",
    "# ----------------------------- Helper Functions -----------------------------\n",
    "\n",
    "def load_transformer_model(model_name: str = 'gpt2-small') -> HookedTransformer:\n",
    "    \"\"\"Load the transformer model.\"\"\"\n",
    "    model = HookedTransformer.from_pretrained(model_name)\n",
    "    return model.cpu()\n",
    "\n",
    "def load_tokenized_data(max_length: int = 128, batch_size: int = 64, take_size: int = 102400) -> torch.Tensor:\n",
    "    \"\"\"Load and tokenize the OpenWebText dataset.\"\"\"\n",
    "    def tokenize_and_concatenate(dataset, tokenizer, streaming=False, max_length=1024, column_name=\"text\", add_bos_token=True):\n",
    "        for key in dataset.features:\n",
    "            if key != column_name:\n",
    "                dataset = dataset.remove_columns(key)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "        seq_len = max_length - 1 if add_bos_token else max_length\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            text = examples[column_name]\n",
    "            full_text = tokenizer.eos_token.join(text)\n",
    "            num_chunks = 20\n",
    "            chunk_length = (len(full_text) - 1) // num_chunks + 1\n",
    "            chunks = [full_text[i * chunk_length: (i + 1) * chunk_length] for i in range(num_chunks)]\n",
    "            tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\"input_ids\"].flatten()\n",
    "            tokens = tokens[tokens != tokenizer.pad_token_id]\n",
    "            num_tokens = len(tokens)\n",
    "            num_batches = num_tokens // seq_len\n",
    "            tokens = tokens[: seq_len * num_batches]\n",
    "            tokens = einops.rearrange(tokens, \"(batch seq) -> batch seq\", batch=num_batches, seq=seq_len)\n",
    "            if add_bos_token:\n",
    "                prefix = np.full((num_batches, 1), tokenizer.bos_token_id)\n",
    "                tokens = np.concatenate([prefix, tokens], axis=1)\n",
    "            return {\"tokens\": tokens}\n",
    "\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n",
    "        return tokenized_dataset\n",
    "\n",
    "    transformer_model = load_transformer_model()\n",
    "    dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "    dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "    tokenized_owt = tokenize_and_concatenate(dataset, transformer_model.tokenizer, max_length=max_length, streaming=True)\n",
    "    tokenized_owt = tokenized_owt.shuffle(42)\n",
    "    tokenized_owt = tokenized_owt.take(take_size)\n",
    "    owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "    owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "    return owt_tokens_torch\n",
    "\n",
    "def load_lca(repo_name: str, model_filename: str, input_dim: int, dict_size: int, device: str) -> torch.nn.Module:\n",
    "    \"\"\"Load the LCA model from HuggingFace Hub.\"\"\"\n",
    "    model_path = hf_hub_download(repo_id=repo_name, filename=model_filename)\n",
    "    model = LCA(input_dim=input_dim, dict_size=dict_size)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model.to(device)\n",
    "\n",
    "def compute_scores(lca: torch.nn.Module, transformer_model: HookedTransformer, \n",
    "                  owt_tokens_torch: torch.Tensor, layer: int, device: str) -> np.ndarray:\n",
    "    \"\"\"Compute activation scores for each dictionary element.\"\"\"\n",
    "    print(f\"Computing scores using device: {device}\")\n",
    "    scores = []\n",
    "    batch_size = 64\n",
    "    \n",
    "    for i in tqdm(range(0, owt_tokens_torch.shape[0], batch_size), desc=\"Computing scores\"):\n",
    "        batch_tokens = owt_tokens_torch[i:i + batch_size].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, cache = transformer_model.run_with_cache(\n",
    "                batch_tokens, \n",
    "                stop_at_layer=layer + 1, \n",
    "                names_filter=None\n",
    "            )\n",
    "            X = cache[\"resid_pre\", layer].to(device)\n",
    "            X = einops.rearrange(X, \"batch pos d_model -> (batch pos) d_model\")\n",
    "            del cache\n",
    "            \n",
    "            # Get LCA activations\n",
    "            activations = lca(X)\n",
    "            \n",
    "            # Reshape activations to match the format we need\n",
    "            scores_reshaped = einops.rearrange(\n",
    "                activations, \n",
    "                \"(b pos) n -> b n pos\", \n",
    "                pos=batch_tokens.shape[1]\n",
    "            ).cpu().numpy().astype(np.float16)\n",
    "            \n",
    "            scores.append(scores_reshaped)\n",
    "\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    np.save(SCORES_PATH, scores)\n",
    "    return scores\n",
    "\n",
    "def get_topk_bottomk_logits(dict_element_index: int, lca: torch.nn.Module, \n",
    "                           transformer_model: HookedTransformer, k: int = TOP_K) -> tuple:\n",
    "    \"\"\"Get top-k and bottom-k logits for a dictionary element.\"\"\"\n",
    "    dict_vector = lca.D.data[:, dict_element_index]\n",
    "    W_U = transformer_model.W_U\n",
    "    logits = einops.einsum(W_U, dict_vector, \"d_model vocab, d_model -> vocab\")\n",
    "    top_k_logits = logits.topk(k).indices\n",
    "    bottom_k_logits = logits.topk(k, largest=False).indices\n",
    "    top_k_tokens = [transformer_model.to_string(x.item()) for x in top_k_logits]\n",
    "    bottom_k_tokens = [transformer_model.to_string(x.item()) for x in bottom_k_logits]\n",
    "    return top_k_tokens, bottom_k_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "Moving model to device:  cpu\n",
      "Scores file not found at lca_scores.npy. Computing scores...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73252 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c4863265c34033a12810a949196c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing scores:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting LCA inference on tensor of shape torch.Size([8192, 768])\n",
      "Initial lr: 0.000059\n",
      "\n",
      "Iteration 0:\n",
      "  Loss: 119.724327\n",
      "  Reconstruction Error: 119.724327\n",
      "  L1 Penalty: 0.000000\n",
      "  Sparsity: 0.0%\n",
      "  Learning Rate: 0.000059\n",
      "  Active Features: 0/16896\n",
      "\n",
      "Iteration 10:\n",
      "  Loss: 61.526257\n",
      "  Reconstruction Error: 61.517220\n",
      "  L1 Penalty: 0.009039\n",
      "  Sparsity: 31.7%\n",
      "  Learning Rate: 0.060606\n",
      "  Active Features: 5361/16896\n",
      "\n",
      "Iteration 20:\n",
      "  Loss: 15.271048\n",
      "  Reconstruction Error: 15.252929\n",
      "  L1 Penalty: 0.018119\n",
      "  Sparsity: 22.6%\n",
      "  Learning Rate: 0.060445\n",
      "  Active Features: 3811/16896\n",
      "\n",
      "Iteration 30:\n",
      "  Loss: 4.670774\n",
      "  Reconstruction Error: 4.650534\n",
      "  L1 Penalty: 0.020240\n",
      "  Sparsity: 21.3%\n",
      "  Learning Rate: 0.060728\n",
      "  Active Features: 3593/16896\n",
      "\n",
      "Iteration 40:\n",
      "  Loss: 1.510661\n",
      "  Reconstruction Error: 1.489450\n",
      "  L1 Penalty: 0.021212\n",
      "  Sparsity: 20.3%\n",
      "  Learning Rate: 0.065844\n",
      "  Active Features: 3434/16896\n",
      "\n",
      "Iteration 50:\n",
      "  Loss: 0.471220\n",
      "  Reconstruction Error: 0.449587\n",
      "  L1 Penalty: 0.021633\n",
      "  Sparsity: 19.5%\n",
      "  Learning Rate: 0.086113\n",
      "  Active Features: 3292/16896\n",
      "\n",
      "Iteration 60:\n",
      "  Loss: 0.178109\n",
      "  Reconstruction Error: 0.156431\n",
      "  L1 Penalty: 0.021678\n",
      "  Sparsity: 18.7%\n",
      "  Learning Rate: 0.102260\n",
      "  Active Features: 3153/16896\n",
      "\n",
      "Iteration 70:\n",
      "  Loss: 0.084363\n",
      "  Reconstruction Error: 0.062791\n",
      "  L1 Penalty: 0.021572\n",
      "  Sparsity: 17.9%\n",
      "  Learning Rate: 0.110561\n",
      "  Active Features: 3017/16896\n",
      "\n",
      "Iteration 80:\n",
      "  Loss: 0.052520\n",
      "  Reconstruction Error: 0.031114\n",
      "  L1 Penalty: 0.021407\n",
      "  Sparsity: 17.2%\n",
      "  Learning Rate: 0.105780\n",
      "  Active Features: 2897/16896\n",
      "\n",
      "Iteration 90:\n",
      "  Loss: 0.041696\n",
      "  Reconstruction Error: 0.020455\n",
      "  L1 Penalty: 0.021240\n",
      "  Sparsity: 16.6%\n",
      "  Learning Rate: 0.095574\n",
      "  Active Features: 2799/16896\n",
      "\n",
      "Iteration 99:\n",
      "  Loss: 0.038203\n",
      "  Reconstruction Error: 0.017101\n",
      "  L1 Penalty: 0.021103\n",
      "  Sparsity: 16.1%\n",
      "  Learning Rate: 0.091439\n",
      "  Active Features: 2722/16896\n",
      "\n",
      "Optimization complete:\n",
      "Initial loss: 119.724327\n",
      "Final loss: 0.038203\n",
      "Loss reduction: 100.0%\n",
      "Initial sparsity: 0.0%\n",
      "Final sparsity: 16.1%\n",
      "Initial lr: 0.000059\n",
      "Final lr: 0.091439\n",
      "\n",
      "Starting LCA inference on tensor of shape torch.Size([8192, 768])\n",
      "Initial lr: 0.000059\n",
      "\n",
      "Iteration 0:\n",
      "  Loss: 119.750885\n",
      "  Reconstruction Error: 119.750885\n",
      "  L1 Penalty: 0.000000\n",
      "  Sparsity: 0.0%\n",
      "  Learning Rate: 0.000059\n",
      "  Active Features: 0/16896\n",
      "\n",
      "Iteration 10:\n",
      "  Loss: 61.543159\n",
      "  Reconstruction Error: 61.534122\n",
      "  L1 Penalty: 0.009043\n",
      "  Sparsity: 31.7%\n",
      "  Learning Rate: 0.060606\n",
      "  Active Features: 5359/16896\n",
      "\n",
      "Iteration 20:\n",
      "  Loss: 15.275251\n",
      "  Reconstruction Error: 15.257124\n",
      "  L1 Penalty: 0.018126\n",
      "  Sparsity: 22.6%\n",
      "  Learning Rate: 0.060279\n",
      "  Active Features: 3812/16896\n",
      "\n",
      "Iteration 30:\n",
      "  Loss: 4.672124\n",
      "  Reconstruction Error: 4.651868\n",
      "  L1 Penalty: 0.020256\n",
      "  Sparsity: 21.3%\n",
      "  Learning Rate: 0.060645\n",
      "  Active Features: 3593/16896\n",
      "\n",
      "Iteration 40:\n",
      "  Loss: 1.510712\n",
      "  Reconstruction Error: 1.489480\n",
      "  L1 Penalty: 0.021231\n",
      "  Sparsity: 20.3%\n",
      "  Learning Rate: 0.065816\n",
      "  Active Features: 3434/16896\n",
      "\n",
      "Iteration 50:\n",
      "  Loss: 0.471646\n",
      "  Reconstruction Error: 0.449991\n",
      "  L1 Penalty: 0.021655\n",
      "  Sparsity: 19.5%\n",
      "  Learning Rate: 0.085875\n",
      "  Active Features: 3292/16896\n",
      "\n",
      "Iteration 60:\n",
      "  Loss: 0.179259\n",
      "  Reconstruction Error: 0.157557\n",
      "  L1 Penalty: 0.021702\n",
      "  Sparsity: 18.7%\n",
      "  Learning Rate: 0.101344\n",
      "  Active Features: 3154/16896\n",
      "\n",
      "Iteration 70:\n",
      "  Loss: 0.085324\n",
      "  Reconstruction Error: 0.063728\n",
      "  L1 Penalty: 0.021596\n",
      "  Sparsity: 17.9%\n",
      "  Learning Rate: 0.111758\n",
      "  Active Features: 3018/16896\n",
      "\n",
      "Iteration 80:\n",
      "  Loss: 0.054061\n",
      "  Reconstruction Error: 0.032632\n",
      "  L1 Penalty: 0.021429\n",
      "  Sparsity: 17.2%\n",
      "  Learning Rate: 0.105490\n",
      "  Active Features: 2898/16896\n",
      "\n",
      "Iteration 90:\n",
      "  Loss: 0.042124\n",
      "  Reconstruction Error: 0.020862\n",
      "  L1 Penalty: 0.021263\n",
      "  Sparsity: 16.6%\n",
      "  Learning Rate: 0.096120\n",
      "  Active Features: 2800/16896\n",
      "\n",
      "Iteration 99:\n",
      "  Loss: 0.038262\n",
      "  Reconstruction Error: 0.017137\n",
      "  L1 Penalty: 0.021124\n",
      "  Sparsity: 16.1%\n",
      "  Learning Rate: 0.091028\n",
      "  Active Features: 2722/16896\n",
      "\n",
      "Optimization complete:\n",
      "Initial loss: 119.750885\n",
      "Final loss: 0.038262\n",
      "Loss reduction: 100.0%\n",
      "Initial sparsity: 0.0%\n",
      "Final sparsity: 16.1%\n",
      "Initial lr: 0.000059\n",
      "Final lr: 0.091028\n",
      "\n",
      "Starting LCA inference on tensor of shape torch.Size([8192, 768])\n",
      "Initial lr: 0.000059\n",
      "\n",
      "Iteration 0:\n",
      "  Loss: 119.658295\n",
      "  Reconstruction Error: 119.658295\n",
      "  L1 Penalty: 0.000000\n",
      "  Sparsity: 0.0%\n",
      "  Learning Rate: 0.000059\n",
      "  Active Features: 0/16896\n",
      "\n",
      "Iteration 10:\n",
      "  Loss: 61.507614\n",
      "  Reconstruction Error: 61.498585\n",
      "  L1 Penalty: 0.009030\n",
      "  Sparsity: 31.8%\n",
      "  Learning Rate: 0.060606\n",
      "  Active Features: 5365/16896\n",
      "\n",
      "Iteration 20:\n",
      "  Loss: 15.265021\n",
      "  Reconstruction Error: 15.246920\n",
      "  L1 Penalty: 0.018107\n",
      "  Sparsity: 22.6%\n",
      "  Learning Rate: 0.060478\n",
      "  Active Features: 3814/16896\n",
      "\n",
      "Iteration 30:\n",
      "  Loss: 4.669300\n",
      "  Reconstruction Error: 4.649080\n",
      "  L1 Penalty: 0.020220\n",
      "  Sparsity: 21.3%\n",
      "  Learning Rate: 0.060656\n",
      "  Active Features: 3594/16896\n",
      "\n",
      "Iteration 40:\n",
      "  Loss: 1.509604\n",
      "  Reconstruction Error: 1.488417\n",
      "  L1 Penalty: 0.021188\n",
      "  Sparsity: 20.3%\n",
      "  Learning Rate: 0.065306\n",
      "  Active Features: 3436/16896\n",
      "\n",
      "Iteration 50:\n",
      "  Loss: 0.471176\n",
      "  Reconstruction Error: 0.449568\n",
      "  L1 Penalty: 0.021608\n",
      "  Sparsity: 19.5%\n",
      "  Learning Rate: 0.085958\n",
      "  Active Features: 3294/16896\n",
      "\n",
      "Iteration 60:\n",
      "  Loss: 0.178187\n",
      "  Reconstruction Error: 0.156533\n",
      "  L1 Penalty: 0.021654\n",
      "  Sparsity: 18.7%\n",
      "  Learning Rate: 0.101916\n",
      "  Active Features: 3155/16896\n",
      "\n",
      "Iteration 70:\n",
      "  Loss: 0.081800\n",
      "  Reconstruction Error: 0.060252\n",
      "  L1 Penalty: 0.021548\n",
      "  Sparsity: 17.9%\n",
      "  Learning Rate: 0.109928\n",
      "  Active Features: 3020/16896\n",
      "\n",
      "Iteration 80:\n",
      "  Loss: 0.052931\n",
      "  Reconstruction Error: 0.031551\n",
      "  L1 Penalty: 0.021380\n",
      "  Sparsity: 17.2%\n",
      "  Learning Rate: 0.104249\n",
      "  Active Features: 2898/16896\n",
      "\n",
      "Iteration 90:\n",
      "  Loss: 0.042623\n",
      "  Reconstruction Error: 0.021408\n",
      "  L1 Penalty: 0.021215\n",
      "  Sparsity: 16.6%\n",
      "  Learning Rate: 0.096259\n",
      "  Active Features: 2800/16896\n",
      "\n",
      "Iteration 99:\n",
      "  Loss: 0.038172\n",
      "  Reconstruction Error: 0.017094\n",
      "  L1 Penalty: 0.021077\n",
      "  Sparsity: 16.1%\n",
      "  Learning Rate: 0.092569\n",
      "  Active Features: 2723/16896\n",
      "\n",
      "Optimization complete:\n",
      "Initial loss: 119.658295\n",
      "Final loss: 0.038172\n",
      "Loss reduction: 100.0%\n",
      "Initial sparsity: 0.0%\n",
      "Final sparsity: 16.1%\n",
      "Initial lr: 0.000059\n",
      "Final lr: 0.092569\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCORES_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded pre-computed scores.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lca_scores.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScores file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSCORES_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Computing scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     owt_tokens_torch \u001b[38;5;241m=\u001b[39m load_tokenized_data()\n\u001b[0;32m---> 19\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowt_tokens_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLAYER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load tokenized data\u001b[39;00m\n\u001b[1;32m     22\u001b[0m owt_tokens_torch \u001b[38;5;241m=\u001b[39m load_tokenized_data()\n",
      "Cell \u001b[0;32mIn[9], line 246\u001b[0m, in \u001b[0;36mcompute_scores\u001b[0;34m(lca, transformer_model, owt_tokens_torch, layer, device)\u001b[0m\n\u001b[1;32m    243\u001b[0m batch_tokens \u001b[38;5;241m=\u001b[39m owt_tokens_torch[i:i \u001b[38;5;241m+\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 246\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_at_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     X \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresid_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    252\u001b[0m     X \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model -> (batch pos) d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:627\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    612\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    620\u001b[0m ]:\n\u001b[1;32m    621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    631\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/hook_points.py:513\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    500\u001b[0m     names_filter,\n\u001b[1;32m    501\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    505\u001b[0m )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    508\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    509\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    510\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    511\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    512\u001b[0m ):\n\u001b[0;32m--> 513\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    515\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:550\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    547\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    548\u001b[0m         )\n\u001b[0;32m--> 550\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/components.py:1601\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     mlp_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1598\u001b[0m         resid_mid \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_mid\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m   1599\u001b[0m     )\n\u001b[1;32m   1600\u001b[0m     normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(mlp_in)\n\u001b[0;32m-> 1601\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_out(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_resid_mid\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m     resid_post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_post(resid_mid \u001b[38;5;241m+\u001b[39m mlp_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;66;03m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;66;03m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/components.py:1276\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1272\u001b[0m pre_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_pre(\n\u001b[1;32m   1273\u001b[0m     einsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model, d_model d_mlp -> batch pos d_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_in) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_in\n\u001b[1;32m   1274\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mact_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mact_fn\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ln\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1276\u001b[0m     post_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_post(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_act\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m     mid_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(pre_act))  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformer_lens/utils.py:162\u001b[0m, in \u001b[0;36mgelu_new\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgelu_new\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28minput\u001b[39m: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Implementation of GeLU used by GPT2 - subtly different from PyTorch's\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    163\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Determine device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config = yaml.safe_load(open(CONFIG_PATH))\n",
    "\n",
    "# Load models\n",
    "lca = load_lca(REPO_NAME, MODEL_FILENAME, INPUT_DIM, DICT_SIZE, device)\n",
    "transformer_model = load_transformer_model().to(device)\n",
    "\n",
    "# Load or compute scores\n",
    "try:\n",
    "    scores = np.load(SCORES_PATH)\n",
    "    print(\"Loaded pre-computed scores.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Scores file not found at {SCORES_PATH}. Computing scores...\")\n",
    "    owt_tokens_torch = load_tokenized_data()\n",
    "    scores = compute_scores(lca, transformer_model, owt_tokens_torch, LAYER, device)\n",
    "\n",
    "# Load tokenized data\n",
    "owt_tokens_torch = load_tokenized_data()\n",
    "\n",
    "# Calculate top examples for each dictionary element\n",
    "for feature_idx in FEATURE_INDICES:\n",
    "    print(f\"\\nAnalyzing dictionary element {feature_idx}\")\n",
    "    \n",
    "    # Get top activating examples\n",
    "    feature_scores = scores[:, feature_idx, :]\n",
    "    flat_scores = feature_scores.flatten()\n",
    "    top_k_indices = flat_scores.argsort()[-TOP_K:][::-1]\n",
    "    top_k_scores = flat_scores[top_k_indices]\n",
    "    \n",
    "    # Get batch and sequence indices\n",
    "    top_k_batch_indices = top_k_indices // feature_scores.shape[1]\n",
    "    top_k_seq_indices = top_k_indices % feature_scores.shape[1]\n",
    "    \n",
    "    # Get corresponding tokens and scores\n",
    "    top_k_tokens = [owt_tokens_torch[batch_idx].tolist() for batch_idx in top_k_batch_indices]\n",
    "    top_k_tokens_str = [[transformer_model.to_string(x) for x in token_seq] \n",
    "                        for token_seq in top_k_tokens]\n",
    "    \n",
    "    # Get logits information\n",
    "    top_logits, bottom_logits = get_topk_bottomk_logits(feature_idx, lca, transformer_model)\n",
    "    \n",
    "    print(f\"\\nTop activating tokens for dictionary element {feature_idx}:\")\n",
    "    for i, (tokens, score) in enumerate(zip(top_k_tokens_str, top_k_scores)):\n",
    "        print(f\"\\nExample {i+1} (activation: {score:.4f}):\")\n",
    "        print(\"\".join(tokens))\n",
    "    \n",
    "    print(f\"\\nTop boosted tokens: {', '.join(top_logits)}\")\n",
    "    print(f\"Bottom boosted tokens: {', '.join(bottom_logits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary Teams (2M+1F):\n",
      "--------------------------------------------------\n",
      "\n",
      "Team 1:\n",
      "- Indigo (Indiiiia)\n",
      "- Jackman (Scuba Diver)\n",
      "- Loki (Sandy)\n",
      "\n",
      "Team 2:\n",
      "- Jacqui (Sengupta's First)\n",
      "- Poswell (Anteater)\n",
      "- Sieb (Long Distance King)\n",
      "\n",
      "Team 3:\n",
      "- Gemma (Sengupta's Last)\n",
      "- Wanless (the trashman)\n",
      "- HPL (LPH)\n",
      "\n",
      "Team 4:\n",
      "- Priscilla (Pastaless)\n",
      "- Jack Wu (My Quant)\n",
      "- Sengupta (George)\n",
      "\n",
      "Team 5:\n",
      "- Georgie (Petrol Burner)\n",
      "- Dicko (Trust Fund Baby)\n",
      "- Chuck (Albert II)\n",
      "\n",
      "Team 6:\n",
      "- Audrey (Astronaut)\n",
      "- Tom (East India Trading Company)\n",
      "- Scollingwood (Carlton)\n",
      "\n",
      "Note: Finn is also serving as Caddy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def group_remaining_players(remaining_males: List[Dict], remaining_females: List[Dict]) -> List[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Group remaining players into teams of 2 or 3.\n",
    "    \"\"\"\n",
    "    remaining_teams = []\n",
    "    all_remaining = remaining_males + remaining_females\n",
    "    total_remaining = len(all_remaining)\n",
    "    \n",
    "    if total_remaining == 0:\n",
    "        return []\n",
    "    elif total_remaining <= 4:\n",
    "        # If 4 or fewer players, put them all in one team\n",
    "        remaining_teams.append(all_remaining)\n",
    "    else:\n",
    "        # For 5 or more players, create teams of 3 and 2\n",
    "        num_players = len(all_remaining)\n",
    "        if num_players % 3 == 0:\n",
    "            # If divisible by 3, create all teams of 3\n",
    "            for i in range(0, num_players, 3):\n",
    "                remaining_teams.append(all_remaining[i:i+3])\n",
    "        else:\n",
    "            # Create as many teams of 3 as possible, then one team of 2\n",
    "            num_teams_of_three = num_players // 3\n",
    "            players_in_teams_of_three = num_teams_of_three * 3\n",
    "            \n",
    "            # Create teams of 3\n",
    "            for i in range(0, players_in_teams_of_three, 3):\n",
    "                remaining_teams.append(all_remaining[i:i+3])\n",
    "            \n",
    "            # Create final team with remaining players\n",
    "            if players_in_teams_of_three < num_players:\n",
    "                remaining_teams.append(all_remaining[players_in_teams_of_three:])\n",
    "    \n",
    "    return remaining_teams\n",
    "\n",
    "def create_teams(participants: List[Dict[str, str]], seed: int = 20) -> List[List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Create primary teams of 3 (2M+1F) and handle remaining players.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Filter out uncertain participants and separate by gender\n",
    "    certain_participants = [p for p in participants if '?' not in p['nickname']]\n",
    "    males = [p for p in certain_participants if p['gender'] == 'M']\n",
    "    females = [p for p in certain_participants if p['gender'] == 'F']\n",
    "    \n",
    "    # Shuffle both lists\n",
    "    random.shuffle(males)\n",
    "    random.shuffle(females)\n",
    "    \n",
    "    # Calculate number of complete teams possible\n",
    "    num_possible_teams = min(len(females), len(males) // 2)\n",
    "    \n",
    "    primary_teams = []\n",
    "    # Create complete teams (2 males + 1 female)\n",
    "    for i in range(num_possible_teams):\n",
    "        team = [\n",
    "            females[i],  # One female\n",
    "            males.pop(0),  # First male\n",
    "            males.pop(0)   # Second male\n",
    "        ]\n",
    "        primary_teams.append(team)\n",
    "    \n",
    "    # Handle remaining players\n",
    "    remaining_males = males\n",
    "    remaining_females = females[num_possible_teams:]\n",
    "    remaining_teams = group_remaining_players(remaining_males, remaining_females)\n",
    "    \n",
    "    return primary_teams, remaining_teams\n",
    "\n",
    "# Create complete participant list\n",
    "participants = [\n",
    "    {\"name\": \"Poswell\", \"nickname\": \"Anteater\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Sengupta\", \"nickname\": \"George\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Chuck\", \"nickname\": \"Albert II\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Loki\", \"nickname\": \"Sandy\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Dicko\", \"nickname\": \"Trust Fund Baby\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Jackman\", \"nickname\": \"Scuba Diver\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Jack Wu\", \"nickname\": \"My Quant\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Sieb\", \"nickname\": \"Long Distance King\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Georgie\", \"nickname\": \"Petrol Burner\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Bean\", \"nickname\": \"?\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Wanless\", \"nickname\": \"the trashman\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Jacqui\", \"nickname\": \"Sengupta's First\", \"gender\": \"F\"},\n",
    "    {\"name\": \"HPL\", \"nickname\": \"LPH\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Indigo\", \"nickname\": \"Indiiiia\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Audrey\", \"nickname\": \"Astronaut\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Priscilla\", \"nickname\": \"Pastaless\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Tom\", \"nickname\": \"East India Trading Company\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Scollingwood\", \"nickname\": \"Carlton\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Gemma\", \"nickname\": \"Sengupta's Last\", \"gender\": \"F\"}\n",
    "]\n",
    "\n",
    "# Generate teams\n",
    "primary_teams, remaining_teams = create_teams(participants)\n",
    "\n",
    "# Print all teams\n",
    "print(\"\\nPrimary Teams (2M+1F):\")\n",
    "print(\"-\" * 50)\n",
    "for i, team in enumerate(primary_teams, 1):\n",
    "    print(f\"\\nTeam {i}:\")\n",
    "    for player in team:\n",
    "        print(f\"- {player['name']} ({player['nickname']})\")\n",
    "\n",
    "if remaining_teams:\n",
    "    print(\"\\nAdditional Teams:\")  # Changed from \"Remaining Teams\" for clarity\n",
    "    print(\"-\" * 50)\n",
    "    for i, team in enumerate(remaining_teams, len(primary_teams) + 1):\n",
    "        print(f\"\\nTeam {i}:\")\n",
    "        for player in team:\n",
    "            print(f\"- {player['name']} ({player['nickname']})\")\n",
    "\n",
    "print(\"\\nNote: Finn is also serving as Caddy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary Teams (2M+1F):\n",
      "--------------------------------------------------\n",
      "\n",
      "Team 1:\n",
      "- Jacqui Farrell\n",
      "- Ben Poswell\n",
      "- Lucas Sieb\n",
      "\n",
      "Team 2:\n",
      "- Georgie Forrest\n",
      "- Will Dixson\n",
      "- Charlie O'Neill\n",
      "\n",
      "Team 3:\n",
      "- Indigo Casablanca\n",
      "- Loki Bromilow\n",
      "- Henry Palmerlee\n",
      "\n",
      "Team 4:\n",
      "- Audrey Elwin\n",
      "- Charlie Sengupta\n",
      "- Timothy Jackman\n",
      "\n",
      "Additional Teams:\n",
      "--------------------------------------------------\n",
      "\n",
      "Team 5:\n",
      "- Jack Wu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def group_remaining_players(remaining_males: List[Dict], remaining_females: List[Dict]) -> List[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Group remaining players into teams of 2 or 3.\n",
    "    \"\"\"\n",
    "    remaining_teams = []\n",
    "    all_remaining = remaining_males + remaining_females\n",
    "    total_remaining = len(all_remaining)\n",
    "    \n",
    "    if total_remaining == 0:\n",
    "        return []\n",
    "    elif total_remaining <= 4:\n",
    "        # If 4 or fewer players, put them all in one team\n",
    "        remaining_teams.append(all_remaining)\n",
    "    else:\n",
    "        # For 5 or more players, create teams of 3 and 2\n",
    "        num_players = len(all_remaining)\n",
    "        if num_players % 3 == 0:\n",
    "            # If divisible by 3, create all teams of 3\n",
    "            for i in range(0, num_players, 3):\n",
    "                remaining_teams.append(all_remaining[i:i+3])\n",
    "        else:\n",
    "            # Create as many teams of 3 as possible, then one team of 2\n",
    "            num_teams_of_three = num_players // 3\n",
    "            players_in_teams_of_three = num_teams_of_three * 3\n",
    "            \n",
    "            # Create teams of 3\n",
    "            for i in range(0, players_in_teams_of_three, 3):\n",
    "                remaining_teams.append(all_remaining[i:i+3])\n",
    "            \n",
    "            # Create final team with remaining players\n",
    "            if players_in_teams_of_three < num_players:\n",
    "                remaining_teams.append(all_remaining[players_in_teams_of_three:])\n",
    "    \n",
    "    return remaining_teams\n",
    "\n",
    "def create_teams(participants: List[Dict[str, str]], seed: int = 41) -> List[List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Create primary teams of 3 (2M+1F) and handle remaining players.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Separate by gender\n",
    "    males = [p for p in participants if p['gender'] == 'M']\n",
    "    females = [p for p in participants if p['gender'] == 'F']\n",
    "    \n",
    "    # Shuffle both lists\n",
    "    random.shuffle(males)\n",
    "    random.shuffle(females)\n",
    "    \n",
    "    # Calculate number of complete teams possible\n",
    "    num_possible_teams = min(len(females), len(males) // 2)\n",
    "    \n",
    "    primary_teams = []\n",
    "    # Create complete teams (2 males + 1 female)\n",
    "    for i in range(num_possible_teams):\n",
    "        team = [\n",
    "            females[i],  # One female\n",
    "            males.pop(0),  # First male\n",
    "            males.pop(0)   # Second male\n",
    "        ]\n",
    "        primary_teams.append(team)\n",
    "    \n",
    "    # Handle remaining players\n",
    "    remaining_males = males\n",
    "    remaining_females = females[num_possible_teams:]\n",
    "    remaining_teams = group_remaining_players(remaining_males, remaining_females)\n",
    "    \n",
    "    return primary_teams, remaining_teams\n",
    "\n",
    "# Create participant list with new names\n",
    "participants = [\n",
    "    {\"name\": \"Ben Poswell\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Charlie Sengupta\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Charlie O'Neill\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Loki Bromilow\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Will Dixson\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Timothy Jackman\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Jack Wu\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Lucas Sieb\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Georgie Forrest\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Jacqui Farrell\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Henry Palmerlee\", \"gender\": \"M\"},\n",
    "    {\"name\": \"Indigo Casablanca\", \"gender\": \"F\"},\n",
    "    {\"name\": \"Audrey Elwin\", \"gender\": \"F\"}\n",
    "]\n",
    "\n",
    "# Generate teams\n",
    "primary_teams, remaining_teams = create_teams(participants)\n",
    "\n",
    "# Print all teams\n",
    "print(\"\\nPrimary Teams (2M+1F):\")\n",
    "print(\"-\" * 50)\n",
    "for i, team in enumerate(primary_teams, 1):\n",
    "    print(f\"\\nTeam {i}:\")\n",
    "    for player in team:\n",
    "        print(f\"- {player['name']}\")\n",
    "\n",
    "if remaining_teams:\n",
    "    print(\"\\nAdditional Teams:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, team in enumerate(remaining_teams, len(primary_teams) + 1):\n",
    "        print(f\"\\nTeam {i}:\")\n",
    "        for player in team:\n",
    "            print(f\"- {player['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
