{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=16896, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (decoder): Linear(in_features=16896, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models import SparseAutoencoder  # Ensure this matches the model definition you uploaded\n",
    "from huggingface_hub import hf_hub_download\n",
    "import einops\n",
    "\n",
    "# Set parameters\n",
    "repo_name = \"charlieoneill/sparse-coding\"  # Adjust this with your repo name\n",
    "model_filename = \"sparse_autoencoder.pth\"  # Name of the model file you uploaded\n",
    "input_dim = 768  # Example input dim, adjust based on your model\n",
    "hidden_dim = 22 * input_dim  # Projection up parameter * input_dim\n",
    "\n",
    "# Download the model from Hugging Face Hub\n",
    "model_path = hf_hub_download(repo_id=repo_name, filename=model_filename)\n",
    "\n",
    "# Load the model\n",
    "sae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "sae.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "sae.eval()  # Set the model to evaluation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader  \n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "device = 'cpu'\n",
    "layer = 9\n",
    "l1_weight = 1e-4\n",
    "\n",
    "# Load the transformer model and activation store\n",
    "hook_point = \"blocks.8.hook_resid_pre\"  # Placeholder hook point\n",
    "saes, _ = get_gpt2_res_jb_saes(hook_point)\n",
    "sparse_autoencoder = saes[hook_point]\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "sparse_autoencoder.cfg.hook_point = f\"blocks.{layer}.attn.hook_z\"\n",
    "sparse_autoencoder.cfg.store_batch_size = 64\n",
    "\n",
    "loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "transformer_model, _, activation_store = loader.load_sae_training_group_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.056356366723775864\n",
      "L1 loss: 261.2139892578125\n",
      "L0 loss: 38.2880859375\n"
     ]
    }
   ],
   "source": [
    "from main import loss_fn\n",
    "\n",
    "batch_tokens = activation_store.get_batch_tokens().to(device)\n",
    "        \n",
    "# Obtain activations from the transformer model\n",
    "with torch.no_grad():\n",
    "    _, cache = transformer_model.run_with_cache(batch_tokens)\n",
    "    X = cache[\"resid_pre\", layer]  # Shape: (batch, pos, d_model)\n",
    "    X = einops.rearrange(X, \"batch pos d_model -> (batch pos) d_model\")\n",
    "    del cache\n",
    "    S_, X_ = sae(X)\n",
    "    recon_loss, l1_loss, l0_loss, total_loss = loss_fn(X, X_, S_, l1_weight=l1_weight)\n",
    "\n",
    "print(f\"Reconstruction loss: {recon_loss.item()}\")\n",
    "print(f\"L1 loss: {l1_loss.item()}\")\n",
    "print(f\"L0 loss: {l0_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73252 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800 * 8)##8)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca87375eb844696b6ade9f1633ce152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformer_lens.utils import to_numpy\n",
    "\n",
    "scores = []\n",
    "endoftext_token = model.tokenizer.eos_token\n",
    "batch_size = 64\n",
    "\n",
    "feature_indices = [  0,   8,  15,  25,  32,  37,  39,  43,  50,  51,  55,  61,  63,\n",
    "        65,  70,  74,  81,  93,  94,  97, 112, 117, 118, 119, 120, 122,\n",
    "       128, 130, 139, 150, 152, 153, 154, 156, 163, 172, 173, 177, 183,\n",
    "       191, 202, 206, 207, 215, 216, 218, 224, 229, 243, 260, 264, 273,\n",
    "       275, 284, 287, 293, 297, 298, 299, 301, 302, 306, 308, 309, 325,\n",
    "       330, 332, 335, 337, 338, 345, 347, 354, 356, 358, 370, 371, 373,\n",
    "       381, 384, 389, 402, 413, 417, 420, 421, 426, 436, 438, 449, 451,\n",
    "       453, 456, 462, 463, 464, 465, 467, 472, 481, 485, 488, 492, 509,\n",
    "       513, 517, 524, 527, 538, 541, 548, 552, 560, 563, 566, 568, 573,\n",
    "       575, 577, 579, 581, 582, 585, 587, 590, 594, 596, 603, 605, 621,\n",
    "       624, 626, 628, 634, 636, 638, 639, 650, 656, 657, 669, 676, 677,\n",
    "       687, 688, 693, 699, 709, 712, 715, 718, 728, 730, 738, 742, 743,\n",
    "       751, 766, 772, 774, 776, 778, 781, 782, 786, 787, 789, 794, 796,\n",
    "       797, 798, 802, 805, 808, 810, 821, 822, 823, 828, 829, 845, 851,\n",
    "       864, 873, 874, 878, 879, 881, 898, 907, 908, 917, 924, 926, 930,\n",
    "       931, 933, 936, 938, 953, 954, 975, 981, 983, 987, 988, 996]\n",
    "\n",
    "for i in tqdm(range(0, owt_tokens_torch.shape[0], batch_size)):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(\n",
    "            owt_tokens_torch[i : i + batch_size],\n",
    "            stop_at_layer=layer + 1,\n",
    "            names_filter=None,\n",
    "        )\n",
    "        X = cache[\"resid_pre\", layer].cpu()  # Shape: (batch, pos, d_model)\n",
    "        X = einops.rearrange(X, \"batch pos d_model -> (batch pos) d_model\")\n",
    "        del cache\n",
    "        cur_scores = sae.encoder(X)[:, feature_indices]\n",
    "\n",
    "    cur_scores_reshaped = to_numpy(\n",
    "                                einops.rearrange(cur_scores, \"(b pos) n -> b n pos\", pos=owt_tokens_torch.shape[1])\n",
    "                            ).astype(np.float16)\n",
    "    \n",
    "    scores.append(cur_scores_reshaped)\n",
    "\n",
    "scores = np.concatenate(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102400, 207, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape#, flat_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sum = scores.sum(axis=0)\n",
    "scores_sum = scores_sum.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero activations: 207\n",
      "Indices of non-zero activations: (array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]),)\n"
     ]
    }
   ],
   "source": [
    "# Print how many are non-zero  \n",
    "print(f\"Number of non-zero activations: {np.count_nonzero(scores_sum)}\")\n",
    "\n",
    "# Print indices of non-zero activations\n",
    "non_zero_indices = np.nonzero(scores_sum)\n",
    "# Add 10000 to the indices to get the actual token indices\n",
    "# non_zero_indices = non_zero_indices[0] + 10_000\n",
    "print(f\"Indices of non-zero activations: {non_zero_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-zero activations:\n",
    "16,  24, 131, 420, 486, 520, 638, 651, 755, 901, 1067 1170 1350 1564 1571 1582 1805 2018 2115 2291 2310 3247 3709 3847\n",
    " 3915 4947 5228 5240 5253 5653 5940 6294 6391 6645 6705 6756 6880 6995\n",
    " 7075 7705 7867 7947 8134 8246 8380 8424 8734 9064 9125 9328 9659 9863\n",
    " 10008 10074 10089 10443 10751 10808 10910 11041 11057 11194 11330 11543\n",
    " 11594 11831 11902 12423 12424 13106 13155 13680 13897 13967 14203 14265\n",
    " 14309 14589 14881 15013 15131 15487 15492 15622 15720 15903 16271 16276\n",
    " 16664 16794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scores as a numpy array of float16\n",
    "np.save(\"scores.npy\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autointerpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to analyze sparse autoencoder features by retrieving top-k activating examples,\n",
    "obtaining top-k and bottom-k boosted logits, formatting prompts, and getting responses\n",
    "from an AI interpreter.\n",
    "\n",
    "Requirements:\n",
    "- torch\n",
    "- huggingface_hub\n",
    "- einops\n",
    "- numpy\n",
    "- yaml\n",
    "- transformer_lens\n",
    "- datasets\n",
    "- tqdm\n",
    "- openai\n",
    "- IPython\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from models import SparseAutoencoder  # Ensure this matches your model definition\n",
    "from huggingface_hub import hf_hub_download\n",
    "import einops\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import re\n",
    "import html\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# ----------------------------- Configuration -----------------------------\n",
    "\n",
    "# Parameters\n",
    "REPO_NAME = \"charlieoneill/sparse-coding\"  # Hugging Face repo name\n",
    "MODEL_FILENAME = \"sparse_autoencoder.pth\"  # Model file name in the repo\n",
    "INPUT_DIM = 768  # Example input dimension\n",
    "HIDDEN_DIM = 22 * INPUT_DIM  # Projection up parameter * input_dim\n",
    "SCORES_PATH = \"scores.npy\"  # Path to the saved scores\n",
    "FEATURE_INDICES = [  0,   8,  15,  25,  32,  37,  39,  43,  50,  51,  55,  61,  63,\n",
    "        65,  70,  74,  81,  93,  94,  97, 112, 117, 118, 119, 120, 122,\n",
    "       128, 130, 139, 150, 152, 153, 154, 156, 163, 172, 173, 177, 183,\n",
    "       191, 202, 206, 207, 215, 216, 218, 224, 229, 243, 260, 264, 273,\n",
    "       275, 284, 287, 293, 297, 298, 299, 301, 302, 306, 308, 309, 325,\n",
    "       330, 332, 335, 337, 338, 345, 347, 354, 356, 358, 370, 371, 373,\n",
    "       381, 384, 389, 402, 413, 417, 420, 421, 426, 436, 438, 449, 451,\n",
    "       453, 456, 462, 463, 464, 465, 467, 472, 481, 485, 488, 492, 509,\n",
    "       513, 517, 524, 527, 538, 541, 548, 552, 560, 563, 566, 568, 573,\n",
    "       575, 577, 579, 581, 582, 585, 587, 590, 594, 596, 603, 605, 621,\n",
    "       624, 626, 628, 634, 636, 638, 639, 650, 656, 657, 669, 676, 677,\n",
    "       687, 688, 693, 699, 709, 712, 715, 718, 728, 730, 738, 742, 743,\n",
    "       751, 766, 772, 774, 776, 778, 781, 782, 786, 787, 789, 794, 796,\n",
    "       797, 798, 802, 805, 808, 810, 821, 822, 823, 828, 829, 845, 851,\n",
    "       864, 873, 874, 878, 879, 881, 898, 907, 908, 917, 924, 926, 930,\n",
    "       931, 933, 936, 938, 953, 954, 975, 981, 983, 987, 988, 996]\n",
    "feature_indices = FEATURE_INDICES\n",
    "TOP_K = 10  # Number of top activating examples\n",
    "BOTTOM_K = 10  # Number of bottom boosted logits\n",
    "\n",
    "# OpenAI Configuration\n",
    "CONFIG_PATH = \"config.yaml\"  # Path to your config file containing API keys\n",
    "\n",
    "# ----------------------------- Helper Functions -----------------------------\n",
    "\n",
    "def load_sparse_autoencoder(repo_name: str, model_filename: str, input_dim: int, hidden_dim: int) -> SparseAutoencoder:\n",
    "    model_path = hf_hub_download(repo_id=repo_name, filename=model_filename)\n",
    "    sae = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "    sae.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    sae.eval()\n",
    "    return sae\n",
    "\n",
    "def load_transformer_model(model_name: str = 'gpt2-small') -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained(model_name)\n",
    "    return model.cpu()\n",
    "\n",
    "def load_scores(scores_path: str) -> np.ndarray:\n",
    "    scores = np.load(scores_path)\n",
    "    return scores\n",
    "\n",
    "def load_tokenized_data(max_length: int = 128, batch_size: int = 64, take_size: int = 102400) -> torch.Tensor:\n",
    "    def tokenize_and_concatenate(\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        streaming=False,\n",
    "        max_length=1024,\n",
    "        column_name=\"text\",\n",
    "        add_bos_token=True,\n",
    "    ):\n",
    "        for key in dataset.features:\n",
    "            if key != column_name:\n",
    "                dataset = dataset.remove_columns(key)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "        seq_len = max_length - 1 if add_bos_token else max_length\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            text = examples[column_name]\n",
    "            full_text = tokenizer.eos_token.join(text)\n",
    "            num_chunks = 20\n",
    "            chunk_length = (len(full_text) - 1) // num_chunks + 1\n",
    "            chunks = [\n",
    "                full_text[i * chunk_length : (i + 1) * chunk_length]\n",
    "                for i in range(num_chunks)\n",
    "            ]\n",
    "            tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\n",
    "                \"input_ids\"\n",
    "            ].flatten()\n",
    "            tokens = tokens[tokens != tokenizer.pad_token_id]\n",
    "            num_tokens = len(tokens)\n",
    "            num_batches = num_tokens // seq_len\n",
    "            tokens = tokens[: seq_len * num_batches]\n",
    "            tokens = einops.rearrange(\n",
    "                tokens, \"(batch seq) -> batch seq\", batch=num_batches, seq=seq_len\n",
    "            )\n",
    "            if add_bos_token:\n",
    "                prefix = np.full((num_batches, 1), tokenizer.bos_token_id)\n",
    "                tokens = np.concatenate([prefix, tokens], axis=1)\n",
    "            return {\"tokens\": tokens}\n",
    "\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function, batched=True, remove_columns=[column_name]\n",
    "        )\n",
    "        return tokenized_dataset\n",
    "\n",
    "    transformer_model = load_transformer_model()\n",
    "    dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "    dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "    tokenized_owt = tokenize_and_concatenate(dataset, transformer_model.tokenizer, max_length=max_length, streaming=True)\n",
    "    tokenized_owt = tokenized_owt.shuffle(42)\n",
    "    tokenized_owt = tokenized_owt.take(take_size)\n",
    "    owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "    owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "    return owt_tokens_torch\n",
    "\n",
    "def compute_scores(sae: SparseAutoencoder, transformer_model: HookedTransformer, owt_tokens_torch: torch.Tensor, layer: int, feature_indices: list, device: str = 'cpu') -> np.ndarray:\n",
    "    sae.eval()\n",
    "\n",
    "    # Compute scores\n",
    "    scores = []\n",
    "    batch_size = 64\n",
    "    for i in tqdm(range(0, owt_tokens_torch.shape[0], batch_size), desc=\"Computing scores\"):\n",
    "        with torch.no_grad():\n",
    "            _, cache = transformer_model.run_with_cache(\n",
    "                owt_tokens_torch[i : i + batch_size],\n",
    "                stop_at_layer=layer + 1,\n",
    "                names_filter=None,\n",
    "            )\n",
    "            X = cache[\"resid_pre\", layer].cpu()  # Shape: (batch, pos, d_model)\n",
    "            X = einops.rearrange(X, \"batch pos d_model -> (batch pos) d_model\")\n",
    "            del cache\n",
    "            cur_scores = sae.encoder(X)[:, feature_indices]\n",
    "            cur_scores_reshaped = einops.rearrange(cur_scores, \"(b pos) n -> b n pos\", pos=owt_tokens_torch.shape[1]).cpu().numpy().astype(np.float16)\n",
    "            scores.append(cur_scores_reshaped)\n",
    "\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    np.save(SCORES_PATH, scores)\n",
    "    return scores\n",
    "\n",
    "def get_top_k_indices(scores: np.ndarray, feature_index: int, k: int = TOP_K) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Get the indices of the examples where the feature activates the most\n",
    "    scores is shape (batch, feature, pos), so we index with feature_index\n",
    "    \"\"\"\n",
    "    feature_scores = scores[:, feature_index, :]\n",
    "    top_k_indices = feature_scores.argsort()[-k:][::-1]\n",
    "    return top_k_indices\n",
    "\n",
    "def get_topk_bottomk_logits(feature_index: int, sae: SparseAutoencoder, transformer_model: HookedTransformer, k: int = TOP_K) -> tuple:\n",
    "    feature_vector = sae.decoder.weight.data[:, feature_index]\n",
    "    W_U = transformer_model.W_U  # (d_model, vocab)\n",
    "    logits = einops.einsum(W_U, feature_vector, \"d_model vocab, d_model -> vocab\")\n",
    "    top_k_logits = logits.topk(k).indices\n",
    "    bottom_k_logits = logits.topk(k, largest=False).indices\n",
    "    top_k_tokens = [transformer_model.to_string(x.item()) for x in top_k_logits]\n",
    "    bottom_k_tokens = [transformer_model.to_string(x.item()) for x in bottom_k_logits]\n",
    "    return top_k_tokens, bottom_k_tokens\n",
    "\n",
    "def highlight_scores_in_html(token_strs: list, scores: list, seq_idx: int, max_color: str = \"#ff8c00\", zero_color: str = \"#ffffff\", show_score: bool = True) -> tuple:\n",
    "    if len(token_strs) != len(scores):\n",
    "        print(f\"Length mismatch between tokens and scores (len(tokens)={len(token_strs)}, len(scores)={len(scores)})\") \n",
    "        return \"\", \"\"\n",
    "    scores_min = min(scores)\n",
    "    scores_max = max(scores)\n",
    "    if scores_max - scores_min == 0:\n",
    "        scores_normalized = np.zeros_like(scores)\n",
    "    else:\n",
    "        scores_normalized = (np.array(scores) - scores_min) / (scores_max - scores_min)\n",
    "    max_color_vec = np.array(\n",
    "        [int(max_color[1:3], 16), int(max_color[3:5], 16), int(max_color[5:7], 16)]\n",
    "    )\n",
    "    zero_color_vec = np.array(\n",
    "        [int(zero_color[1:3], 16), int(zero_color[3:5], 16), int(zero_color[5:7], 16)]\n",
    "    )\n",
    "    color_vecs = np.einsum(\"i, j -> ij\", scores_normalized, max_color_vec) + np.einsum(\n",
    "        \"i, j -> ij\", 1 - scores_normalized, zero_color_vec\n",
    "    )\n",
    "    color_strs = [f\"#{int(x[0]):02x}{int(x[1]):02x}{int(x[2]):02x}\" for x in color_vecs]\n",
    "    if show_score:\n",
    "        tokens_html = \"\".join(\n",
    "            [\n",
    "                f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}<span class='feature_val'> ({scores[i]:.2f})</span></span>\"\"\"\n",
    "                for i, token_str in enumerate(token_strs)\n",
    "            ]\n",
    "        )\n",
    "        clean_text = \" | \".join(\n",
    "            [f\"{token_str} ({scores[i]:.2f})\" for i, token_str in enumerate(token_strs)]\n",
    "        )\n",
    "    else:\n",
    "        tokens_html = \"\".join(\n",
    "            [\n",
    "                f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}</span>\"\"\"\n",
    "                for i, token_str in enumerate(token_strs)\n",
    "            ]\n",
    "        )\n",
    "        clean_text = \" | \".join(token_strs)\n",
    "    head = \"\"\"\n",
    "    <style>\n",
    "        span.token {\n",
    "            font-family: monospace;\n",
    "            border-style: solid;\n",
    "            border-width: 1px;\n",
    "            border-color: #dddddd;\n",
    "        }\n",
    "        span.feature_val {\n",
    "            font-size: smaller;\n",
    "            color: #555555;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    return head + tokens_html, convert_clean_text(clean_text)\n",
    "\n",
    "def convert_clean_text(clean_text: str, k: int = 1, tokens_left: int = 30, tokens_right: int = 5) -> str:\n",
    "    # Split the clean text on the \"|\" separator\n",
    "    token_score_pairs = clean_text.split(\" | \")\n",
    "\n",
    "    # Remove the first token if present\n",
    "    if token_score_pairs:\n",
    "        token_score_pairs = token_score_pairs[1:]\n",
    "\n",
    "    # Initialize a list to hold tuples of (token, score)\n",
    "    tokens_with_scores = []\n",
    "\n",
    "    # Define regex to capture tokens with scores\n",
    "    token_score_pattern = re.compile(r\"^(.+?) \\((\\d+\\.\\d+)\\)$\")\n",
    "\n",
    "    for token_score in token_score_pairs:\n",
    "        match = token_score_pattern.match(token_score.strip())\n",
    "        if match:\n",
    "            token = match.group(1)\n",
    "            score = float(match.group(2))\n",
    "            tokens_with_scores.append((token, score))\n",
    "        else:\n",
    "            # Handle cases where score is zero or absent\n",
    "            token = token_score.split(' (')[0].strip()\n",
    "            tokens_with_scores.append((token, 0.0))\n",
    "\n",
    "    # Sort tokens by score in descending order\n",
    "    sorted_tokens = sorted(tokens_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top k tokens with non-zero scores\n",
    "    top_k_tokens = [token for token, score in sorted_tokens if score > 0][:k]\n",
    "\n",
    "    # Find all indices of top k tokens\n",
    "    top_k_indices = [i for i, (token, score) in enumerate(tokens_with_scores) if token in top_k_tokens and score >0]\n",
    "\n",
    "    # Define windows around each top token\n",
    "    windows = []\n",
    "    for idx in top_k_indices:\n",
    "        start = max(0, idx - tokens_left)\n",
    "        end = min(len(tokens_with_scores) - 1, idx + tokens_right)\n",
    "        windows.append((start, end))\n",
    "\n",
    "    # Merge overlapping windows\n",
    "    merged_windows = []\n",
    "    for window in sorted(windows, key=lambda x: x[0]):\n",
    "        if not merged_windows:\n",
    "            merged_windows.append(window)\n",
    "        else:\n",
    "            last_start, last_end = merged_windows[-1]\n",
    "            current_start, current_end = window\n",
    "            if current_start <= last_end + 1:\n",
    "                # Overlapping or adjacent windows, merge them\n",
    "                merged_windows[-1] = (last_start, max(last_end, current_end))\n",
    "            else:\n",
    "                merged_windows.append(window)\n",
    "\n",
    "    # Collect all unique indices within the merged windows\n",
    "    selected_indices = set()\n",
    "    for start, end in merged_windows:\n",
    "        selected_indices.update(range(start, end + 1))\n",
    "\n",
    "    # Create the converted tokens list with wrapping\n",
    "    converted_tokens = []\n",
    "    for i, (token, score) in enumerate(tokens_with_scores):\n",
    "        if i in selected_indices:\n",
    "            if token in top_k_tokens and score > 0:\n",
    "                token = f\"<<{token}>>\"\n",
    "            converted_tokens.append(token)\n",
    "        # Else, skip tokens outside the selected windows\n",
    "\n",
    "    # Join the converted tokens into a single string\n",
    "    converted_text = \" \".join(converted_tokens)\n",
    "    return converted_text\n",
    "\n",
    "def format_interpreter_prompt(clean_text: str, top_k_tokens: list, bottom_k_tokens: list) -> str:\n",
    "    system_prompt = \"\"\"### SYSTEM PROMPT ###\n",
    "\n",
    "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior.\n",
    "Guidelines:\n",
    "\n",
    "You will be given a list of text examples on which the neuron activates. The specific tokens which cause the neuron to activate will appear between delimiters like <<this>>. If a sequence of consecutive tokens all cause the neuron to activate, the entire sequence of tokens will be contained between delimiters <<just like this>>. The activation value of the example is listed after each example in parentheses.\n",
    "\n",
    "- Try to produce a concise final description. Simply describe the text features that activate the neuron, and what its role might be based on the tokens it predicts.\n",
    "- If either the text features or the predicted tokens are completely uninformative, you don't need to mention them.\n",
    "- The last line of your response must be the formatted explanation.\"\"\"\n",
    "\n",
    "    cot_prompt = \"\"\"\n",
    "(Part 1) Tokens that the neuron activates highly on in text\n",
    "\n",
    "Step 1: List a couple activating and contextual tokens you find interesting. Search for patterns in these tokens, if there are any. Don't list more than 5 tokens.\n",
    "Step 2: Write down general shared features of the text examples.\n",
    "\"\"\"\n",
    "\n",
    "    activations_section = \"\"\"\n",
    "(Part 1) Tokens that the neuron activates highly on in text\n",
    "\n",
    "Step 1: List a couple activating and contextual tokens you find interesting. Search for patterns in these tokens, if there are any.\n",
    "Step 2: Write down several general shared features of the text examples.\n",
    "Step 3: Take note of the activation values to understand which examples are most representative of the neuron.\n",
    "\"\"\"\n",
    "\n",
    "    logits_section = \"\"\"\n",
    "(Part 2) Tokens that the neuron boosts in the next token prediction\n",
    "\n",
    "You will also be shown a list called Top_logits. The logits promoted by the neuron shed light on how the neuron's activation influences the model's predictions or outputs. Look at this list of Top_logits and refine your hypotheses from part 1. It is possible that this list is more informative than the examples from part 1.\n",
    "\n",
    "Pay close attention to the words in this list and write down what they have in common. Then look at what they have in common, as well as patterns in the tokens you found in Part 1, to produce a single explanation for what features of text cause the neuron to activate. Propose your explanation in the following format:\n",
    "[EXPLANATION]: <your explanation>\n",
    "\"\"\"\n",
    "\n",
    "    # Define the Examples and Their Responses (can be customized or loaded from a file)\n",
    "    examples = \"\"\"\n",
    "### EXAMPLE STEP-BY-STEP WALKTHROUGH 1 ###\n",
    "Example 1:  and he was <<over the moon>> to find\n",
    "Example 2:  we'll be laughing <<till the cows come home>>! Pro\n",
    "Example 3:  thought Scotland was boring, but really there's more <<than meets the eye>>! I'd\n",
    "\n",
    "Top_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\n",
    "\n",
    "(Part 1)\n",
    "ACTIVATING TOKENS: \"over the moon\", \"than meets the eye\".\n",
    "PREVIOUS TOKENS: No interesting patterns.\n",
    "\n",
    "Step 1.\n",
    "The activating tokens are all parts of common idioms.\n",
    "The previous tokens have nothing in common.\n",
    "\n",
    "Step 2.\n",
    "- The examples contain common idioms.\n",
    "- In some examples, the activating tokens are followed by an exclamation mark.\n",
    "\n",
    "Let me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\n",
    "- Yes, I missed one: The text examples all convey positive sentiment.\n",
    "\n",
    "(Part 2)\n",
    "SIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\n",
    "- The top logits list contains words that are strongly associated with positive emotions.\n",
    "\n",
    "[EXPLANATION]: Common idioms in text conveying positive sentiment.\n",
    "\n",
    "### EXAMPLE STEP-BY-STEP WALKTHROUGH 2 ###\n",
    "\n",
    "Example 1:  a river is wide but the ocean is wid<<er>>. The ocean\n",
    "Example 2:  every year you get tall<<er>>,\" she\n",
    "Example 3:  the hole was small<<er>> but deep<<er>> than the\n",
    "\n",
    "Top_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\n",
    "\n",
    "(Part 1)\n",
    "ACTIVATING TOKENS: \"er\", \"er\", \"er\".\n",
    "PREVIOUS TOKENS: \"wid\", \"tall\", \"small\", \"deep\".\n",
    "\n",
    "Step 1.\n",
    "- The activating tokens are mostly \"er\".\n",
    "- The previous tokens are mostly adjectives, or parts of adjectives, describing size.\n",
    "- The neuron seems to activate on, or near, the tokens in comparative adjectives describing size.\n",
    "\n",
    "Step 2.\n",
    "- In each example, the activating token appeared at the end of a comparative adjective.\n",
    "- The comparative adjectives (\"wider\", \"tallish\", \"smaller\", \"deeper\") all describe size.\n",
    "\n",
    "Let me look again for patterns in the examples. Are there any links or hidden linguistic commonalities that I missed?\n",
    "- I can't see any.\n",
    "\n",
    "(Part 2)\n",
    "SIMILAR TOKENS: None\n",
    "- The top logits list contains unrelated nouns and adverbs.\n",
    "\n",
    "[EXPLANATION]: The token \"er\" at the end of a comparative adjective describing size.\n",
    "\n",
    "### EXAMPLE STEP-BY-STEP WALKTHROUGH 3 ###\n",
    "\n",
    "Example 1:  something happening inside my <<house>>\", he\n",
    "Example 2:  presumably was always contained in <<a box>>\", according\n",
    "Example 3:  people were coming into the <<smoking area>>\".\n",
    "\n",
    "However he\n",
    "Example 4:  Patrick: \"why are you getting in the << way?>>\" Later,\n",
    "\n",
    "Top_logits: [\"room\", \"end\", \"container\", \"space\", \"plane\"]\n",
    "\n",
    "(Part 1)\n",
    "ACTIVATING TOKENS: \"house\", \"a box\", \"smoking area\", \"way?\".\n",
    "PREVIOUS TOKENS: No interesting patterns.\n",
    "\n",
    "Step 1.\n",
    "- The activating tokens are all things that one can be in.\n",
    "- The previous tokens have nothing in common.\n",
    "\n",
    "Step 2.\n",
    "- The examples involve being inside something, sometimes figuratively.\n",
    "- The activating token is a thing which something else is inside of.\n",
    "\n",
    "Let me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\n",
    "- Yes, I missed one: The activating token is followed by a quotation mark, suggesting it occurs within speech.\n",
    "\n",
    "(Part 2)\n",
    "SIMILAR TOKENS: \"room\", \"container\", \"space\".\n",
    "- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\n",
    "\n",
    "[EXPLANATION]: Nouns preceding a quotation mark, representing distinct objects that contain something.\n",
    "\"\"\"\n",
    "\n",
    "    # Combine all predefined sections\n",
    "    predefined_prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "{cot_prompt}\n",
    "\n",
    "{logits_section}\n",
    "\n",
    "We will now provide three step-by-step examples of how you should approach this.\n",
    "\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "    # Prepare the current data sections\n",
    "    # Format the top_k_tokens and bottom_k_tokens as JSON-like lists\n",
    "    top_logits_str = \"[\" + \", \".join(f'\"{token}\"' for token in top_k_tokens) + \"]\"\n",
    "    bottom_logits_str = \"[\" + \", \".join(f'\"{token}\"' for token in bottom_k_tokens) + \"]\"\n",
    "\n",
    "    current_activations = f\"\"\"\n",
    "(Part 1) Tokens that the neuron activates highly on in text\n",
    "\n",
    "{clean_text}\n",
    "\"\"\"\n",
    "\n",
    "    current_logits = f\"\"\"\n",
    "(Part 2) Tokens that the neuron boosts in the next token prediction\n",
    "\n",
    "Top_logits: {top_logits_str}\n",
    "Bottom_logits: {bottom_logits_str}\n",
    "\"\"\"\n",
    "\n",
    "    # Combine all parts into the final prompt\n",
    "    full_prompt = f\"\"\"{predefined_prompt}\n",
    "\n",
    "### OUR NEURON WE NEED TO INTERPRET STEP-BY-STEP ###\n",
    "\n",
    "{current_activations}\n",
    "\n",
    "{current_logits}\n",
    "\n",
    "Walk through the steps to interpret this neuron.\n",
    "\"\"\"\n",
    "    return full_prompt\n",
    "\n",
    "def get_ai_response(formatted_prompt: str, config: dict) -> str:\n",
    "    azure_client = AzureOpenAI(\n",
    "        azure_endpoint=config[\"base_url\"],\n",
    "        api_key=config[\"azure_api_key\"],\n",
    "        api_version=config[\"api_version\"],\n",
    "    )\n",
    "    \n",
    "    # If prompt is a str\n",
    "    if type(formatted_prompt) == str:\n",
    "        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "    elif type(formatted_prompt) == list:\n",
    "        messages = formatted_prompt\n",
    "    response = azure_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ----------------------------- Main Processing Function -----------------------------\n",
    "\n",
    "def analyze_feature(feature_index: int, sae: SparseAutoencoder, transformer_model: HookedTransformer, \n",
    "                   owt_tokens_torch: torch.Tensor, scores: np.ndarray, top_k_indices, top_k_batch_indices, top_k_tokens, top_k_tokens_str, top_k_scores_per_seq,\n",
    "                   config: dict, \n",
    "                   top_k: int = TOP_K, bottom_k: int = BOTTOM_K) -> None:\n",
    "    \"\"\" \n",
    "    Returns formatted_prompt, analysis, interp_text, scoring_text, false_text\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Get top-k and bottom-k boosted logits\n",
    "    top_logits, bottom_logits = get_topk_bottomk_logits(feature_indices[feature_index], sae, transformer_model, k=top_k)\n",
    "    \n",
    "    # Highlight scores in HTML and prepare clean text\n",
    "    examples_html = []\n",
    "    examples_clean_text = []\n",
    "    examples_false_text = []\n",
    "\n",
    "    # False feature index\n",
    "    false_feature_idx = 0 if feature_index != 0 else 15\n",
    "\n",
    "    # Single-feature slice: (batch, seq_len)\n",
    "    single_feature_scores_false = scores[:, false_feature_idx, :]  \n",
    "\n",
    "    # Flatten, pick top k\n",
    "    flat_scores_false = single_feature_scores_false.flatten()\n",
    "    top_k_indices_false = flat_scores_false.argsort()[-top_k:][::-1]\n",
    "    top_k_batch_indices_false, top_k_seq_indices_false = np.unravel_index(\n",
    "        top_k_indices_false, single_feature_scores_false.shape\n",
    "    )\n",
    "    top_k_tokens_false = [owt_tokens_torch[b].tolist() for b in top_k_batch_indices_false]\n",
    "    top_k_tokens_str_false = [\n",
    "        [transformer_model.to_string(tok_id) for tok_id in seq]\n",
    "        for seq in top_k_tokens_false\n",
    "    ]\n",
    "    top_k_scores_per_seq_false = [scores[b] for b in top_k_batch_indices_false]\n",
    "\n",
    "    for i in range(top_k):\n",
    "        try:\n",
    "            example_html, clean_text = highlight_scores_in_html(\n",
    "                top_k_tokens_str[i],\n",
    "                top_k_scores_per_seq[i][feature_index],\n",
    "                seq_idx=i,\n",
    "                show_score=True\n",
    "            )\n",
    "            _, false_text = highlight_scores_in_html(\n",
    "                top_k_tokens_str_false[i],\n",
    "                top_k_scores_per_seq_false[i][false_feature_idx],\n",
    "                seq_idx=i,\n",
    "                show_score=True\n",
    "            )\n",
    "            examples_html.append(example_html)\n",
    "            if len(clean_text) > 10:\n",
    "                #print(f'Length of clean text: {len(clean_text)}')\n",
    "                examples_clean_text.append(clean_text)\n",
    "                examples_false_text.append(false_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    # If length of examples_clean_text is less than top_k, raise an exception\n",
    "    if len(examples_clean_text) < 4:\n",
    "        raise Exception(f\"Not enough activating examples (number of examples: {len(examples_clean_text)})\")\n",
    "\n",
    "    \n",
    "    # Combine clean texts\n",
    "    length_examples = len(examples_clean_text) // 2\n",
    "    examples_to_insert = [f\"Example {i+1}: {example}\" for i, example in enumerate(examples_clean_text)][:length_examples]\n",
    "    combined_clean_text = \"\\n\\n\".join(examples_to_insert).strip()\n",
    "\n",
    "    if len(combined_clean_text) < 100:\n",
    "        raise Exception(\"No activating examples for this one\")\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = format_interpreter_prompt(combined_clean_text, top_logits, bottom_logits)\n",
    "    \n",
    "    # Get the AI response\n",
    "    analysis = get_ai_response(formatted_prompt, config)\n",
    "\n",
    "    interp_text = examples_clean_text[:length_examples]\n",
    "    scoring_text = examples_clean_text[length_examples:length_examples*2]\n",
    "    false_text = examples_false_text[:length_examples]\n",
    "\n",
    "    return formatted_prompt, analysis, interp_text, scoring_text, false_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "Scores file not found at scores.npy. Computing scores...\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73252 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abce98552434562bdbe43164be265de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing scores:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73252 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function to execute the analysis for all defined feature indices.\n",
    "\"\"\"\n",
    "# Load configuration\n",
    "config = yaml.safe_load(open(CONFIG_PATH))\n",
    "\n",
    "# Load models\n",
    "sae = load_sparse_autoencoder(REPO_NAME, MODEL_FILENAME, INPUT_DIM, HIDDEN_DIM)\n",
    "transformer_model = load_transformer_model()\n",
    "\n",
    "# Load or compute scores\n",
    "try:\n",
    "    scores = load_scores(SCORES_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Scores file not found at {SCORES_PATH}. Computing scores...\")\n",
    "    owt_tokens_torch = load_tokenized_data()\n",
    "    # Assume layer and loss_fn are defined appropriately\n",
    "    layer = 9  # Example layer\n",
    "    device = 'cpu'\n",
    "    scores = compute_scores(sae, transformer_model, owt_tokens_torch, layer, FEATURE_INDICES, device=device)\n",
    "\n",
    "# Load tokenized data\n",
    "owt_tokens_torch = load_tokenized_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero entries: 25442071\n",
      "Number of features with non-zero activations: 207\n",
      "Mean number of examples an activating feature activates on: 2529.207729468599\n"
     ]
    }
   ],
   "source": [
    "# 1) Count number of non-zero entries\n",
    "nonzero_count = np.count_nonzero(scores)\n",
    "\n",
    "# 2) Number of features with any non-zero activation\n",
    "features_nonzero = np.sum((scores != 0).any(axis=(0, 2)))\n",
    "\n",
    "# 3) Mean number of examples an activating feature activates on\n",
    "#    For each feature, count how many examples are non-zero (anywhere along the last axis).\n",
    "examples_per_feature = np.sum((scores != 0).any(axis=2), axis=0)\n",
    "mean_examples = np.mean(examples_per_feature[examples_per_feature > 0])\n",
    "\n",
    "print(\"Number of non-zero entries:\", nonzero_count)\n",
    "print(\"Number of features with non-zero activations:\", features_nonzero)\n",
    "print(\"Mean number of examples an activating feature activates on:\", mean_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_score_prompt(interpretation, examples, false_examples):\n",
    "    DSCORER_SYSTEM_PROMPT = \"\"\"You are an intelligent and meticulous linguistics researcher.\n",
    "\n",
    "    You will be given a certain feature of text, such as \"male pronouns\" or \"text with negative sentiment\". You will be given a few examples of text that contain this feature. Portions of the sentence which strongly represent this feature are between tokens << and >>.\n",
    "\n",
    "    Some examples might be mislabeled. Your task is to determine if every single token within << and >> is correctly labeled. Consider that all provided examples could be correct, none of the examples could be correct, or a mix. An example is only correct if every marked token is representative of the feature\n",
    "\n",
    "    For each example in turn, return 1 if the sentence is correctly labeled or 0 if the tokens are mislabeled. You must return your response in a valid Python list. Do not return anything else besides a Python list.\n",
    "    \"\"\"\n",
    "\n",
    "    # https://www.neuronpedia.org/gpt2-small/6-res-jb/6048\n",
    "    DSCORER_EXAMPLE_ONE = \"\"\"Feature explanation: Words related to American football positions, specifically the tight end position.\n",
    "\n",
    "    Text examples:\n",
    "\n",
    "    Example 0:<|endoftext|>Getty ImagesĊĊPatriots<< tight end>> Rob Gronkowski had his bossâĢĻ\n",
    "    Example 1: posted<|endoftext|>You should know this<< about>> offensive line coaches: they are large, demanding<< men>>\n",
    "    Example 2: Media Day 2015ĊĊLSU<< defensive>> end Isaiah Washington (94) speaks<< to the>>\n",
    "    Example 3:<< running backs>>,\" he said. .. Defensive<< end>> Carroll Phillips is improving and his injury is\n",
    "    Example 4:<< line>>, with the left side âĢĶ namely<< tackle>> Byron Bell at<< tackle>> and<< guard>> Amini\n",
    "    \"\"\"\n",
    "\n",
    "    DSCORER_RESPONSE_ONE = \"[1,0,0,1,1]\"\n",
    "\n",
    "    # https://www.neuronpedia.org/gpt2-small/6-res-jb/9396\n",
    "    DSCORER_EXAMPLE_TWO = \"\"\"Feature explanation: The word \"guys\" in the phrase \"you guys\".\n",
    "\n",
    "    Text examples:\n",
    "\n",
    "    Example 0: if you are<< comfortable>> with it. You<< guys>> support me in many other ways already and\n",
    "    Example 1: birth control access<|endoftext|> but I assure you<< women>> in Kentucky aren't laughing as they struggle\n",
    "    Example 2:âĢĻs gig! I hope you guys<< LOVE>> her, and<< please>> be nice,\n",
    "    Example 3:American, told<< Hannity>> that âĢľyou<< guys>> are playing the race card.âĢĿ\n",
    "    Example 4:<< the>><|endoftext|>ľI want to<< remind>> you all that 10 days ago (director Massimil\n",
    "    \"\"\"\n",
    "\n",
    "    DSCORER_RESPONSE_TWO = \"[0,0,0,0,0]\"\n",
    "\n",
    "    # https://www.neuronpedia.org/gpt2-small/8-res-jb/12654\n",
    "    DSCORER_EXAMPLE_THREE = \"\"\"Feature explanation: \"of\" before words that start with a capital letter.\n",
    "\n",
    "    Text examples:\n",
    "\n",
    "    Example 0: climate, TomblinâĢĻs Chief<< of>> Staff Charlie Lorensen said.Ċ\n",
    "    Example 1: no wonderworking relics, no true Body and Blood<< of>> Christ, no true Baptism\n",
    "    Example 2:ĊĊDeborah Sathe, Head<< of>> Talent Development and Production at Film London,\n",
    "    Example 3:ĊĊIt has been devised by Director<< of>> Public Prosecutions (DPP)\n",
    "    Example 4: and fair investigation not even include the Director<< of>> Athletics? Â· Finally, we believe the\n",
    "    \"\"\"\n",
    "\n",
    "    DSCORER_RESPONSE_THREE = \"[1,1,1,1,1]\"\n",
    "    combined_examples = examples + false_examples\n",
    "    examples_to_insert = [f\"Example {i+1}: {example}\" for i, example in enumerate(combined_examples)]\n",
    "    combined_clean_text = \"\\n\\n\".join(examples_to_insert).strip()\n",
    "    \n",
    "\n",
    "    GENERATION_PROMPT = f\"\"\"Feature explanation: {interpretation}\n",
    "\n",
    "    Text examples:\n",
    "\n",
    "    {combined_clean_text}\n",
    "    \"\"\"\n",
    "\n",
    "    final_user_prompt = f\"\"\"\n",
    "\n",
    "    ### EXAMPLE 1 ###\n",
    "    {DSCORER_EXAMPLE_ONE}\n",
    "    {DSCORER_RESPONSE_ONE}\n",
    "\n",
    "    ### EXAMPLE 2 ###\n",
    "    {DSCORER_EXAMPLE_TWO}\n",
    "    {DSCORER_RESPONSE_TWO}\n",
    "\n",
    "    ### EXAMPLE 3 ###\n",
    "    {DSCORER_EXAMPLE_THREE}\n",
    "    {DSCORER_RESPONSE_THREE}\n",
    "\n",
    "    ### YOUR TURN ###\n",
    "    {GENERATION_PROMPT}\n",
    "    \"\"\"\n",
    "\n",
    "    return [{\"role\": \"system\", \"content\": DSCORER_SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": final_user_prompt}]\n",
    "\n",
    "\n",
    "# interpretation = \"Adjectives conveying positive attributes or strong characteristics.\"\n",
    "# prompt = format_score_prompt(interpretation=interpretation, examples=scoring_text, false_examples=false_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1_score(scoring_response, scoring_text, false_text):\n",
    "\n",
    "    # print(f\"Scoring response: {scoring_response}\")\n",
    "\n",
    "    # Get response from string\n",
    "    scoring_response = eval('['+scoring_response.split('[')[-1].split(']')[0]+']')\n",
    "    #print(f\"Scoring response: {scoring_response}\")\n",
    "\n",
    "    y_ones = np.ones(len(scoring_text))\n",
    "    y_zeros = np.zeros(len(false_text))\n",
    "    y_true = np.concatenate([y_ones, y_zeros])\n",
    "    y_pred = np.array(scoring_response)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return f1\n",
    "\n",
    "# calculate_f1_score(response, scoring_text, false_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for feature 1: 0.8285714285714285\n"
     ]
    }
   ],
   "source": [
    "f_idx = 1\n",
    "#print(f\"Analyzing feature {f_idx} (feature id = {feat_id})...\")\n",
    "# Single-feature slice: (batch, seq_len)\n",
    "single_feature_scores = scores[:, f_idx, :]  \n",
    "\n",
    "# Flatten, pick top k\n",
    "flat_scores = single_feature_scores.flatten()\n",
    "top_k_indices = flat_scores.argsort()[-k:][::-1]\n",
    "top_k_batch_indices, top_k_seq_indices = np.unravel_index(\n",
    "    top_k_indices, single_feature_scores.shape\n",
    ")\n",
    "\n",
    "# Convert to tokens\n",
    "top_k_tokens = [owt_tokens_torch[b].tolist() for b in top_k_batch_indices]\n",
    "top_k_tokens_str = [\n",
    "    [transformer_model.to_string(tok_id) for tok_id in seq]\n",
    "    for seq in top_k_tokens\n",
    "]\n",
    "\n",
    "# Collect entire (n_features, seq_len) so we can index [feature_index] → (seq_len,)\n",
    "top_k_scores_per_seq = [scores[b] for b in top_k_batch_indices]\n",
    "#   ^ scores[b].shape = (n_features, seq_len)\n",
    "#     so top_k_scores_per_seq[i][f_idx] is shape (seq_len,)\n",
    "\n",
    "# Now call analyze_feature on this single feature\n",
    "\n",
    "formatted_prompt, analysis, interp_text, scoring_text, false_text = analyze_feature(\n",
    "    feature_index=f_idx,\n",
    "    sae=sae,\n",
    "    transformer_model=transformer_model,\n",
    "    owt_tokens_torch=owt_tokens_torch,\n",
    "    scores=scores,  # can still pass the full scores if needed\n",
    "    top_k_indices=top_k_indices,\n",
    "    top_k_batch_indices=top_k_batch_indices,\n",
    "    top_k_tokens=top_k_tokens,\n",
    "    top_k_tokens_str=top_k_tokens_str,\n",
    "    top_k_scores_per_seq=top_k_scores_per_seq,\n",
    "    config=config,\n",
    "    top_k=10\n",
    ")\n",
    "#print(f\"Feature {f_idx}: {len(interp_text)} examples for interpretation, {len(scoring_text)} for scoring, {len(false_text)} for false\")\n",
    "# Assert the number of non-empty strings in each list is the same\n",
    "interp_length = len([x for x in interp_text if len(x) > 1])\n",
    "\n",
    "\n",
    "interpretation = analysis.split(\"[EXPLANATION]: \")[-1]\n",
    "scoring_prompt = format_score_prompt(interpretation=interpretation, examples=scoring_text, false_examples=false_text)\n",
    "# for p in scoring_prompt:\n",
    "#     print(p['content'])\n",
    "# print()\n",
    "scoring_response = get_ai_response(scoring_prompt, config)\n",
    "f1 = calculate_f1_score(scoring_response, scoring_text, false_text)\n",
    "\n",
    "print(f\"F1 score for feature {f_idx}: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8f6a99c7d04c6ab7021c3601226874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for feature 0: 0.23076923076923078\n",
      "F1 score for feature 1: 0.625\n",
      "F1 score for feature 2: 0.696969696969697\n",
      "F1 score for feature 3: 0.4949494949494949\n",
      "Feature 4 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 5: 0.4857142857142857\n",
      "Feature 6 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 7: 0.3333333333333333\n",
      "F1 score for feature 8: 0.6703296703296704\n",
      "F1 score for feature 9: 0.3333333333333333\n",
      "F1 score for feature 10: 1.0\n",
      "F1 score for feature 11: 0.7333333333333334\n",
      "F1 score for feature 12: 0.7333333333333334\n",
      "Feature 13 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 14: 0.873015873015873\n",
      "F1 score for feature 15: 0.898989898989899\n",
      "F1 score for feature 16: 0.7916666666666667\n",
      "F1 score for feature 17: 0.898989898989899\n",
      "F1 score for feature 18: 0.4949494949494949\n",
      "F1 score for feature 19: 0.696969696969697\n",
      "F1 score for feature 20: 0.898989898989899\n",
      "F1 score for feature 21: 0.6666666666666666\n",
      "Feature 22 failed: Not enough activating examples (number of examples: 1)\n",
      "Feature 23 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 24: 0.5833333333333333\n",
      "F1 score for feature 25: 0.375\n",
      "F1 score for feature 26: 0.8285714285714285\n",
      "F1 score for feature 27: 1.0\n",
      "Feature 28 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 29: 0.6703296703296704\n",
      "F1 score for feature 30: 0.898989898989899\n",
      "F1 score for feature 31: 0.625\n",
      "F1 score for feature 32: 1.0\n",
      "Feature 33 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 34: 1.0\n",
      "F1 score for feature 35: 0.5833333333333333\n",
      "Feature 36 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 37: 0.7333333333333334\n",
      "F1 score for feature 38: 0.6703296703296704\n",
      "F1 score for feature 39: 0.898989898989899\n",
      "F1 score for feature 40: 0.5\n",
      "F1 score for feature 41: 0.6703296703296704\n",
      "F1 score for feature 42: 0.4\n",
      "F1 score for feature 43: 0.8\n",
      "F1 score for feature 44: 0.898989898989899\n",
      "F1 score for feature 45: 1.0\n",
      "F1 score for feature 46: 0.3333333333333333\n",
      "F1 score for feature 47: 0.4857142857142857\n",
      "Feature 48 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 49: 0.696969696969697\n",
      "F1 score for feature 50: 0.3333333333333333\n",
      "F1 score for feature 51: 0.5\n",
      "F1 score for feature 52: 1.0\n",
      "F1 score for feature 53: 1.0\n",
      "F1 score for feature 54: 0.7333333333333334\n",
      "F1 score for feature 55: 0.5\n",
      "Feature 56 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 57: 0.5\n",
      "F1 score for feature 58: 0.696969696969697\n",
      "F1 score for feature 59: 0.898989898989899\n",
      "F1 score for feature 60: 0.3333333333333333\n",
      "F1 score for feature 61: 0.4949494949494949\n",
      "Feature 62 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 63: 0.25\n",
      "F1 score for feature 64: 0.3333333333333333\n",
      "Feature 65 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 66: 0.6703296703296704\n",
      "F1 score for feature 67: 0.3333333333333333\n",
      "F1 score for feature 68: 0.3333333333333333\n",
      "Feature 69 failed: Not enough activating examples (number of examples: 2)\n",
      "Feature 70 failed: Not enough activating examples (number of examples: 1)\n",
      "Feature 71 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 72: 0.7916666666666667\n",
      "Feature 73 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 74: 0.898989898989899\n",
      "F1 score for feature 75: 0.23076923076923078\n",
      "Feature 76 failed: Not enough activating examples (number of examples: 3)\n",
      "Feature 77 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 78: 0.7333333333333334\n",
      "F1 score for feature 79: 0.898989898989899\n",
      "F1 score for feature 80: 0.696969696969697\n",
      "F1 score for feature 81: 0.45054945054945056\n",
      "F1 score for feature 82: 0.696969696969697\n",
      "F1 score for feature 83: 0.4666666666666667\n",
      "F1 score for feature 84: 0.8285714285714285\n",
      "F1 score for feature 85: 0.3333333333333333\n",
      "F1 score for feature 86: 0.7916666666666667\n",
      "F1 score for feature 87: 0.4949494949494949\n",
      "F1 score for feature 88: 0.3333333333333333\n",
      "F1 score for feature 89: 0.7333333333333334\n",
      "F1 score for feature 90: 0.4949494949494949\n",
      "F1 score for feature 91: 0.6190476190476191\n",
      "F1 score for feature 92: 0.7333333333333334\n",
      "F1 score for feature 93: 0.7333333333333334\n",
      "Feature 94 failed: Not enough activating examples (number of examples: 3)\n",
      "Feature 95 failed: Not enough activating examples (number of examples: 1)\n",
      "Feature 96 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 97: 0.3333333333333333\n",
      "Feature 98 failed: Not enough activating examples (number of examples: 2)\n",
      "Feature 99 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 100: 0.7916666666666667\n",
      "F1 score for feature 101: 0.4\n",
      "Feature 102 failed: Not enough activating examples (number of examples: 2)\n",
      "Feature 103 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 104: 0.2727272727272727\n",
      "F1 score for feature 105: 0.5833333333333333\n",
      "Feature 106 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 107: 0.6666666666666666\n",
      "F1 score for feature 108: 0.3333333333333333\n",
      "Feature 109 failed: Not enough activating examples (number of examples: 2)\n",
      "Feature 110 failed: Not enough activating examples (number of examples: 2)\n",
      "Feature 111 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 112: 0.3333333333333333\n",
      "F1 score for feature 113: 0.36507936507936506\n",
      "F1 score for feature 114: 0.4949494949494949\n",
      "Feature 115 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 116: 0.3333333333333333\n",
      "F1 score for feature 117: 1.0\n",
      "F1 score for feature 118: 0.7916666666666667\n",
      "F1 score for feature 119: 0.29292929292929293\n",
      "F1 score for feature 120: 0.7333333333333334\n",
      "F1 score for feature 121: 0.7333333333333334\n",
      "Feature 122 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 123: 0.2857142857142857\n",
      "F1 score for feature 124: 0.09090909090909091\n",
      "F1 score for feature 125: 0.873015873015873\n",
      "F1 score for feature 126: 0.5833333333333333\n",
      "Feature 127 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 128: 0.7916666666666667\n",
      "Feature 129 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 130: 0.2727272727272727\n",
      "F1 score for feature 131: 0.7333333333333334\n",
      "F1 score for feature 132: 0.3333333333333333\n",
      "F1 score for feature 133: 0.8\n",
      "F1 score for feature 134: 0.7916666666666667\n",
      "F1 score for feature 135: 0.7916666666666667\n",
      "F1 score for feature 136: 0.6190476190476191\n",
      "F1 score for feature 137: 0.2\n",
      "F1 score for feature 138: 1.0\n",
      "Feature 139 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 140: 0.3333333333333333\n",
      "F1 score for feature 141: 0.3333333333333333\n",
      "F1 score for feature 142: 0.6703296703296704\n",
      "F1 score for feature 143: 0.2\n",
      "F1 score for feature 144: 0.29292929292929293\n",
      "Feature 145 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 146: 0.7916666666666667\n",
      "F1 score for feature 147: 0.3333333333333333\n",
      "F1 score for feature 148: 0.8285714285714285\n",
      "F1 score for feature 149: 0.625\n",
      "F1 score for feature 150: 1.0\n",
      "F1 score for feature 151: 0.7333333333333334\n",
      "F1 score for feature 152: 0.898989898989899\n",
      "F1 score for feature 153: 0.6190476190476191\n",
      "F1 score for feature 154: 0.5\n",
      "Feature 155 failed: Not enough activating examples (number of examples: 1)\n",
      "Feature 156 failed: Not enough activating examples (number of examples: 2)\n",
      "Feature 157 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 158: 0.8\n",
      "F1 score for feature 159: 1.0\n",
      "F1 score for feature 160: 0.898989898989899\n",
      "Feature 161 failed: Not enough activating examples (number of examples: 1)\n",
      "F1 score for feature 162: 0.5\n",
      "F1 score for feature 163: 0.4\n",
      "Feature 164 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 165: 0.5833333333333333\n",
      "F1 score for feature 166: 0.898989898989899\n",
      "Feature 167 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 168: 0.4\n",
      "F1 score for feature 169: 0.7333333333333334\n",
      "Feature 170 failed: Not enough activating examples (number of examples: 3)\n",
      "F1 score for feature 171: 1.0\n",
      "F1 score for feature 172: 0.5238095238095238\n",
      "F1 score for feature 173: 0.7916666666666667\n",
      "F1 score for feature 174: 0.7916666666666667\n",
      "Feature 175 failed: unexpected character after line continuation character (<string>, line 1)\n",
      "F1 score for feature 176: 0.4949494949494949\n",
      "F1 score for feature 177: 0.4857142857142857\n",
      "F1 score for feature 178: 0.3333333333333333\n",
      "F1 score for feature 179: 0.25\n",
      "F1 score for feature 180: 0.625\n",
      "F1 score for feature 181: 0.09090909090909091\n",
      "F1 score for feature 182: 0.7333333333333334\n",
      "Feature 183 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 184: 0.898989898989899\n",
      "F1 score for feature 185: 0.3333333333333333\n",
      "F1 score for feature 186: 1.0\n",
      "Feature 187 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 188: 0.5238095238095238\n",
      "Feature 189 failed: Found input variables with inconsistent numbers of samples: [10, 9]\n",
      "Feature 190 failed: Not enough activating examples (number of examples: 2)\n",
      "Feature 191 failed: Not enough activating examples (number of examples: 2)\n",
      "F1 score for feature 192: 0.625\n",
      "F1 score for feature 193: 0.696969696969697\n",
      "F1 score for feature 194: 0.873015873015873\n",
      "F1 score for feature 195: 0.5833333333333333\n",
      "F1 score for feature 196: 0.5833333333333333\n",
      "F1 score for feature 197: 0.625\n",
      "F1 score for feature 198: 0.6\n",
      "F1 score for feature 199: 0.3333333333333333\n",
      "F1 score for feature 200: 0.5833333333333333\n",
      "F1 score for feature 201: 0.36507936507936506\n",
      "F1 score for feature 202: 0.6\n",
      "F1 score for feature 203: 1.0\n",
      "F1 score for feature 204: 0.873015873015873\n",
      "F1 score for feature 205: 0.873015873015873\n",
      "Feature 206 failed: Found input variables with inconsistent numbers of samples: [10, 9]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "interp_results = []\n",
    "\n",
    "for f_idx, feat_id in tqdm(enumerate(feature_indices), total=len(feature_indices)):\n",
    "    #print(f\"Analyzing feature {f_idx} (feature id = {feat_id})...\")\n",
    "    # Single-feature slice: (batch, seq_len)\n",
    "    single_feature_scores = scores[:, f_idx, :]  \n",
    "\n",
    "    # Flatten, pick top k\n",
    "    flat_scores = single_feature_scores.flatten()\n",
    "    top_k_indices = flat_scores.argsort()[-k:][::-1]\n",
    "    top_k_batch_indices, top_k_seq_indices = np.unravel_index(\n",
    "        top_k_indices, single_feature_scores.shape\n",
    "    )\n",
    "\n",
    "    # Convert to tokens\n",
    "    top_k_tokens = [owt_tokens_torch[b].tolist() for b in top_k_batch_indices]\n",
    "    top_k_tokens_str = [\n",
    "        [transformer_model.to_string(tok_id) for tok_id in seq]\n",
    "        for seq in top_k_tokens\n",
    "    ]\n",
    "\n",
    "    # Collect entire (n_features, seq_len) so we can index [feature_index] → (seq_len,)\n",
    "    top_k_scores_per_seq = [scores[b] for b in top_k_batch_indices]\n",
    "    #   ^ scores[b].shape = (n_features, seq_len)\n",
    "    #     so top_k_scores_per_seq[i][f_idx] is shape (seq_len,)\n",
    "\n",
    "    # Now call analyze_feature on this single feature\n",
    "    try:\n",
    "        formatted_prompt, analysis, interp_text, scoring_text, false_text = analyze_feature(\n",
    "            feature_index=f_idx,\n",
    "            sae=sae,\n",
    "            transformer_model=transformer_model,\n",
    "            owt_tokens_torch=owt_tokens_torch,\n",
    "            scores=scores,  # can still pass the full scores if needed\n",
    "            top_k_indices=top_k_indices,\n",
    "            top_k_batch_indices=top_k_batch_indices,\n",
    "            top_k_tokens=top_k_tokens,\n",
    "            top_k_tokens_str=top_k_tokens_str,\n",
    "            top_k_scores_per_seq=top_k_scores_per_seq,\n",
    "            config=config,\n",
    "            top_k=k\n",
    "        )\n",
    "        #print(f\"Feature {f_idx}: {len(interp_text)} examples for interpretation, {len(scoring_text)} for scoring, {len(false_text)} for false\")\n",
    "        # Assert the number of non-empty strings in each list is the same\n",
    "        interp_length = len([x for x in interp_text if len(x) > 1])\n",
    "\n",
    "\n",
    "        interpretation = analysis.split(\"[EXPLANATION]: \")[-1]\n",
    "        scoring_prompt = format_score_prompt(interpretation=interpretation, examples=scoring_text, false_examples=false_text)\n",
    "        # for p in scoring_prompt:\n",
    "        #     print(p['content'])\n",
    "        # print()\n",
    "        scoring_response = get_ai_response(scoring_prompt, config)\n",
    "        f1 = calculate_f1_score(scoring_response, scoring_text, false_text)\n",
    "\n",
    "        results_dict = {\n",
    "            \"feature_index\": feat_id,\n",
    "            \"analysis\": analysis,\n",
    "            \"interpretation\": interpretation,\n",
    "            \"f1_score\": f1\n",
    "        }\n",
    "        interp_results.append(results_dict)\n",
    "        print(f\"F1 score for feature {f_idx}: {f1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Feature {f_idx} failed: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23076923076923078,\n",
       " 0.625,\n",
       " 0.696969696969697,\n",
       " 0.4949494949494949,\n",
       " 0.4857142857142857,\n",
       " 0.3333333333333333,\n",
       " 0.6703296703296704,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.7333333333333334,\n",
       " 0.7333333333333334,\n",
       " 0.873015873015873,\n",
       " 0.898989898989899,\n",
       " 0.7916666666666667,\n",
       " 0.898989898989899,\n",
       " 0.4949494949494949,\n",
       " 0.696969696969697,\n",
       " 0.898989898989899,\n",
       " 0.6666666666666666,\n",
       " 0.5833333333333333,\n",
       " 0.375,\n",
       " 0.8285714285714285,\n",
       " 1.0,\n",
       " 0.6703296703296704,\n",
       " 0.898989898989899,\n",
       " 0.625,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5833333333333333,\n",
       " 0.7333333333333334,\n",
       " 0.6703296703296704,\n",
       " 0.898989898989899,\n",
       " 0.5,\n",
       " 0.6703296703296704,\n",
       " 0.4,\n",
       " 0.8,\n",
       " 0.898989898989899,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.4857142857142857,\n",
       " 0.696969696969697,\n",
       " 0.3333333333333333,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.7333333333333334,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.696969696969697,\n",
       " 0.898989898989899,\n",
       " 0.3333333333333333,\n",
       " 0.4949494949494949,\n",
       " 0.25,\n",
       " 0.3333333333333333,\n",
       " 0.6703296703296704,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.7916666666666667,\n",
       " 0.898989898989899,\n",
       " 0.23076923076923078,\n",
       " 0.7333333333333334,\n",
       " 0.898989898989899,\n",
       " 0.696969696969697,\n",
       " 0.45054945054945056,\n",
       " 0.696969696969697,\n",
       " 0.4666666666666667,\n",
       " 0.8285714285714285,\n",
       " 0.3333333333333333,\n",
       " 0.7916666666666667,\n",
       " 0.4949494949494949,\n",
       " 0.3333333333333333,\n",
       " 0.7333333333333334,\n",
       " 0.4949494949494949,\n",
       " 0.6190476190476191,\n",
       " 0.7333333333333334,\n",
       " 0.7333333333333334,\n",
       " 0.3333333333333333,\n",
       " 0.7916666666666667,\n",
       " 0.4,\n",
       " 0.2727272727272727,\n",
       " 0.5833333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.36507936507936506,\n",
       " 0.4949494949494949,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.7916666666666667,\n",
       " 0.29292929292929293,\n",
       " 0.7333333333333334,\n",
       " 0.7333333333333334,\n",
       " 0.2857142857142857,\n",
       " 0.09090909090909091,\n",
       " 0.873015873015873,\n",
       " 0.5833333333333333,\n",
       " 0.7916666666666667,\n",
       " 0.2727272727272727,\n",
       " 0.7333333333333334,\n",
       " 0.3333333333333333,\n",
       " 0.8,\n",
       " 0.7916666666666667,\n",
       " 0.7916666666666667,\n",
       " 0.6190476190476191,\n",
       " 0.2,\n",
       " 1.0,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 0.6703296703296704,\n",
       " 0.2,\n",
       " 0.29292929292929293,\n",
       " 0.7916666666666667,\n",
       " 0.3333333333333333,\n",
       " 0.8285714285714285,\n",
       " 0.625,\n",
       " 1.0,\n",
       " 0.7333333333333334,\n",
       " 0.898989898989899,\n",
       " 0.6190476190476191,\n",
       " 0.5,\n",
       " 0.8,\n",
       " 1.0,\n",
       " 0.898989898989899,\n",
       " 0.5,\n",
       " 0.4,\n",
       " 0.5833333333333333,\n",
       " 0.898989898989899,\n",
       " 0.4,\n",
       " 0.7333333333333334,\n",
       " 1.0,\n",
       " 0.5238095238095238,\n",
       " 0.7916666666666667,\n",
       " 0.7916666666666667,\n",
       " 0.4949494949494949,\n",
       " 0.4857142857142857,\n",
       " 0.3333333333333333,\n",
       " 0.25,\n",
       " 0.625,\n",
       " 0.09090909090909091,\n",
       " 0.7333333333333334,\n",
       " 0.898989898989899,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.5238095238095238,\n",
       " 0.625,\n",
       " 0.696969696969697,\n",
       " 0.873015873015873,\n",
       " 0.5833333333333333,\n",
       " 0.5833333333333333,\n",
       " 0.625,\n",
       " 0.6,\n",
       " 0.3333333333333333,\n",
       " 0.5833333333333333,\n",
       " 0.36507936507936506,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.873015873015873,\n",
       " 0.873015873015873]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the F1 scores from the results list\n",
    "f1_scores = [r[\"f1_score\"] for r in interp_results]\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "x=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "nbinsx": 15,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "histogram",
         "x": [
          0.23076923076923078,
          0.625,
          0.696969696969697,
          0.4949494949494949,
          0.4857142857142857,
          0.3333333333333333,
          0.6703296703296704,
          0.3333333333333333,
          1,
          0.7333333333333334,
          0.7333333333333334,
          0.873015873015873,
          0.898989898989899,
          0.7916666666666667,
          0.898989898989899,
          0.4949494949494949,
          0.696969696969697,
          0.898989898989899,
          0.6666666666666666,
          0.5833333333333333,
          0.375,
          0.8285714285714285,
          1,
          0.6703296703296704,
          0.898989898989899,
          0.625,
          1,
          1,
          0.5833333333333333,
          0.7333333333333334,
          0.6703296703296704,
          0.898989898989899,
          0.5,
          0.6703296703296704,
          0.4,
          0.8,
          0.898989898989899,
          1,
          0.3333333333333333,
          0.4857142857142857,
          0.696969696969697,
          0.3333333333333333,
          0.5,
          1,
          1,
          0.7333333333333334,
          0.5,
          0.5,
          0.696969696969697,
          0.898989898989899,
          0.3333333333333333,
          0.4949494949494949,
          0.25,
          0.3333333333333333,
          0.6703296703296704,
          0.3333333333333333,
          0.3333333333333333,
          0.7916666666666667,
          0.898989898989899,
          0.23076923076923078,
          0.7333333333333334,
          0.898989898989899,
          0.696969696969697,
          0.45054945054945056,
          0.696969696969697,
          0.4666666666666667,
          0.8285714285714285,
          0.3333333333333333,
          0.7916666666666667,
          0.4949494949494949,
          0.3333333333333333,
          0.7333333333333334,
          0.4949494949494949,
          0.6190476190476191,
          0.7333333333333334,
          0.7333333333333334,
          0.3333333333333333,
          0.7916666666666667,
          0.4,
          0.2727272727272727,
          0.5833333333333333,
          0.6666666666666666,
          0.3333333333333333,
          0.3333333333333333,
          0.36507936507936506,
          0.4949494949494949,
          0.3333333333333333,
          1,
          0.7916666666666667,
          0.29292929292929293,
          0.7333333333333334,
          0.7333333333333334,
          0.2857142857142857,
          0.09090909090909091,
          0.873015873015873,
          0.5833333333333333,
          0.7916666666666667,
          0.2727272727272727,
          0.7333333333333334,
          0.3333333333333333,
          0.8,
          0.7916666666666667,
          0.7916666666666667,
          0.6190476190476191,
          0.2,
          1,
          0.3333333333333333,
          0.3333333333333333,
          0.6703296703296704,
          0.2,
          0.29292929292929293,
          0.7916666666666667,
          0.3333333333333333,
          0.8285714285714285,
          0.625,
          1,
          0.7333333333333334,
          0.898989898989899,
          0.6190476190476191,
          0.5,
          0.8,
          1,
          0.898989898989899,
          0.5,
          0.4,
          0.5833333333333333,
          0.898989898989899,
          0.4,
          0.7333333333333334,
          1,
          0.5238095238095238,
          0.7916666666666667,
          0.7916666666666667,
          0.4949494949494949,
          0.4857142857142857,
          0.3333333333333333,
          0.25,
          0.625,
          0.09090909090909091,
          0.7333333333333334,
          0.898989898989899,
          0.3333333333333333,
          1,
          0.5238095238095238,
          0.625,
          0.696969696969697,
          0.873015873015873,
          0.5833333333333333,
          0.5833333333333333,
          0.625,
          0.6,
          0.3333333333333333,
          0.5833333333333333,
          0.36507936507936506,
          0.6,
          1,
          0.873015873015873,
          0.873015873015873
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "Mean F1: 0.62",
          "x": 0.6213464453970784,
          "xanchor": "left",
          "xref": "x",
          "y": 1,
          "yanchor": "top",
          "yref": "y domain"
         }
        ],
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "shapes": [
         {
          "line": {
           "color": "red",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0.6213464453970784,
          "x1": 0.6213464453970784,
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Histogram of F1 Scores"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Histogram of F1 scores\n",
    "n_bins = 15\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "fig = px.histogram(x=f1_scores, nbins=n_bins, title=\"Histogram of F1 Scores\", width=600)\n",
    "# Red vertical line for the mean\n",
    "fig.add_vline(x=mean_f1_score, line_dash=\"dash\", line_color=\"red\", annotation_text=f\"Mean F1: {mean_f1_score:.2f}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results dict as json\n",
    "import json\n",
    "\n",
    "with open('interp_results_sae.json', 'w') as f:\n",
    "    json.dump(interp_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to analyze MLP features by retrieving top-k activating examples,\n",
    "obtaining top-k and bottom-k boosted logits, formatting prompts, and getting responses\n",
    "from an AI interpreter.\n",
    "\n",
    "Requirements:\n",
    "- torch\n",
    "- huggingface_hub\n",
    "- einops\n",
    "- numpy\n",
    "- yaml\n",
    "- transformer_lens\n",
    "- datasets\n",
    "- tqdm\n",
    "- openai\n",
    "- IPython\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import einops\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import re\n",
    "import html\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# ----------------------------- Configuration -----------------------------\n",
    "\n",
    "# Parameters\n",
    "REPO_NAME = \"charlieoneill/sparse-coding\"  # Hugging Face repo name\n",
    "MODEL_FILENAME = \"mlp_model.pth\"  # MLP model file name in the repo\n",
    "INPUT_DIM = 768  # Example input dimension\n",
    "HIDDEN_DIM = 16896 // 4  # Adjust this based on your MLP configuration\n",
    "OUTPUT_DIM = 16896\n",
    "SCORES_PATH = \"mlp_scores.npy\"  # Path to the saved scores\n",
    "FEATURE_INDICES = [\n",
    "    13, 87, 152, 172, 240, 404, 514, 554, 562, 607, 752, 821, 854, 1091,\n",
    "    1130, 1153, 1276, 1463, 1498, 1609, 1829, 2060, 2252, 2260, 2395, 2456, \n",
    "    2480, 2548, 2566, 2830, 2841, 2912, 2994, 3229, 3233, 3407, 3474, 3594, \n",
    "    3758, 3764, 3768, 3797, 3832, 4140, 4455, 4643, 4777, 4874, 4875, 4943, \n",
    "    5305, 5389, 5466, 5604, 5675, 5693, 5789, 6091, 6243, 6300, 6303, 6314, \n",
    "    6432, 6745, 6818, 7010, 7025, 7211, 7248, 7371, 7804, 8052, 8472, 8479, \n",
    "    8480, 8481, 8694, 8744, 8765, 9098, 9146, 9358, 9383, 9441, 9693, 9715, \n",
    "    9741, 9944, 10147, 10192, 10308, 10420, 10508, 10605, 10611, 10837, \n",
    "    10974, 10994, 11163, 11209, 11234, 11673, 11743, 11932, 12084, 12186, \n",
    "    12318, 12325, 12515, 12691, 12771, 12780, 12815, 12847, 13095, 13100, \n",
    "    13111, 13120, 13272, 13314, 13325, 13372, 13561, 13849, 14167, 14237, \n",
    "    14280, 14289, 14359, 14460, 15428, 15464, 15485, 15553, 15656, 15736, \n",
    "    16112, 16724, 16805, 16892\n",
    "]\n",
    "feature_indices = FEATURE_INDICES\n",
    "TOP_K = 10  # Number of top activating examples\n",
    "BOTTOM_K = 10  # Number of bottom boosted logits\n",
    "\n",
    "# OpenAI Configuration\n",
    "CONFIG_PATH = \"config.yaml\"  # Path to your config file containing API keys\n",
    "\n",
    "# Define the MLP class (make sure this matches your uploaded model architecture)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, M, N, h, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(M, h, bias=use_bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h, N, bias=use_bias),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Linear(N, M, bias=use_bias)\n",
    "\n",
    "    def forward(self, X, norm_D=True):\n",
    "        if norm_D:\n",
    "            self.decoder.weight.data /= torch.linalg.norm(self.decoder.weight.data, dim=0, keepdim=True)\n",
    "        S_ = self.encoder(X)\n",
    "        X_ = torch.matmul(S_, self.decoder.weight.T)\n",
    "        if self.decoder.bias is not None:\n",
    "            X_ += self.decoder.bias\n",
    "        return S_, X_\n",
    "\n",
    "# ----------------------------- Helper Functions -----------------------------\n",
    "\n",
    "def load_mlp(repo_name: str, model_filename: str, input_dim: int, hidden_dim: int, output_dim: int) -> torch.nn.Module:\n",
    "    model_path = hf_hub_download(repo_id=repo_name, filename=model_filename)\n",
    "    model = MLP(M=input_dim, N=output_dim, h=hidden_dim).to(torch.device('cpu'))\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_transformer_model(model_name: str = 'gpt2-small') -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained(model_name)\n",
    "    return model.cpu()\n",
    "\n",
    "def load_scores(scores_path: str) -> np.ndarray:\n",
    "    scores = np.load(scores_path)\n",
    "    return scores\n",
    "\n",
    "def load_tokenized_data(max_length: int = 128, batch_size: int = 64, take_size: int = 102400) -> torch.Tensor:\n",
    "    def tokenize_and_concatenate(dataset, tokenizer, streaming=False, max_length=1024, column_name=\"text\", add_bos_token=True):\n",
    "        for key in dataset.features:\n",
    "            if key != column_name:\n",
    "                dataset = dataset.remove_columns(key)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "        seq_len = max_length - 1 if add_bos_token else max_length\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            text = examples[column_name]\n",
    "            full_text = tokenizer.eos_token.join(text)\n",
    "            num_chunks = 20\n",
    "            chunk_length = (len(full_text) - 1) // num_chunks + 1\n",
    "            chunks = [full_text[i * chunk_length: (i + 1) * chunk_length] for i in range(num_chunks)]\n",
    "            tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\"input_ids\"].flatten()\n",
    "            tokens = tokens[tokens != tokenizer.pad_token_id]\n",
    "            num_tokens = len(tokens)\n",
    "            num_batches = num_tokens // seq_len\n",
    "            tokens = tokens[: seq_len * num_batches]\n",
    "            tokens = einops.rearrange(tokens, \"(batch seq) -> batch seq\", batch=num_batches, seq=seq_len)\n",
    "            if add_bos_token:\n",
    "                prefix = np.full((num_batches, 1), tokenizer.bos_token_id)\n",
    "                tokens = np.concatenate([prefix, tokens], axis=1)\n",
    "            return {\"tokens\": tokens}\n",
    "\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[column_name])\n",
    "        return tokenized_dataset\n",
    "\n",
    "    transformer_model = load_transformer_model()\n",
    "    dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "    dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "    tokenized_owt = tokenize_and_concatenate(dataset, transformer_model.tokenizer, max_length=max_length, streaming=True)\n",
    "    tokenized_owt = tokenized_owt.shuffle(42)\n",
    "    tokenized_owt = tokenized_owt.take(take_size)\n",
    "    owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "    owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "    return owt_tokens_torch\n",
    "\n",
    "def compute_scores(mlp: torch.nn.Module, transformer_model: HookedTransformer, owt_tokens_torch: torch.Tensor, layer: int, feature_indices: list, device: str = 'cpu') -> np.ndarray:\n",
    "    mlp.to(device)\n",
    "\n",
    "    # Load transformer model activations and compute scores\n",
    "    scores = []\n",
    "    batch_size = 64\n",
    "    for i in tqdm(range(0, owt_tokens_torch.shape[0], batch_size), desc=\"Computing scores\"):\n",
    "        with torch.no_grad():\n",
    "            _, cache = transformer_model.run_with_cache(owt_tokens_torch[i: i + batch_size], stop_at_layer=layer + 1, names_filter=None)\n",
    "            X = cache[\"resid_pre\", layer].cpu()\n",
    "            X = einops.rearrange(X, \"batch pos d_model -> (batch pos) d_model\")\n",
    "            del cache\n",
    "            S_, X_ = mlp(X)\n",
    "            cur_scores = mlp.encoder(X)[:, feature_indices]\n",
    "            cur_scores_reshaped = einops.rearrange(cur_scores, \"(b pos) n -> b n pos\", pos=owt_tokens_torch.shape[1]).cpu().numpy().astype(np.float16)\n",
    "            scores.append(cur_scores_reshaped)\n",
    "\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    np.save(SCORES_PATH, scores)\n",
    "    return scores\n",
    "\n",
    "def get_topk_bottomk_logits(feature_index: int, mlp: torch.nn.Module, transformer_model: HookedTransformer, k: int = TOP_K) -> tuple:\n",
    "    feature_vector = mlp.decoder.weight.data[:, feature_index]\n",
    "    W_U = transformer_model.W_U\n",
    "    logits = einops.einsum(W_U, feature_vector, \"d_model vocab, d_model -> vocab\")\n",
    "    top_k_logits = logits.topk(k).indices\n",
    "    bottom_k_logits = logits.topk(k, largest=False).indices\n",
    "    top_k_tokens = [transformer_model.to_string(x.item()) for x in top_k_logits]\n",
    "    bottom_k_tokens = [transformer_model.to_string(x.item()) for x in bottom_k_logits]\n",
    "    return top_k_tokens, bottom_k_tokens\n",
    "\n",
    "def analyze_feature(feature_index: int, mlp: torch.nn.Module, transformer_model: HookedTransformer, owt_tokens_torch: torch.Tensor, scores: np.ndarray, top_k_indices, top_k_batch_indices, top_k_tokens, top_k_tokens_str, top_k_scores_per_seq, config: dict, top_k: int = TOP_K, bottom_k: int = BOTTOM_K) -> None:\n",
    "\n",
    "    # Get top-k and bottom-k boosted logits\n",
    "    top_logits, bottom_logits = get_topk_bottomk_logits(feature_indices[feature_index], mlp, transformer_model, k=top_k)\n",
    "    \n",
    "    # Highlight scores in HTML and prepare clean text\n",
    "    examples_html = []\n",
    "    examples_clean_text = []\n",
    "    examples_false_text = []\n",
    "    false_feature_index = 1 if feature_index != 1 else 14\n",
    "    for i in range(top_k * 2):\n",
    "        try:\n",
    "            example_html, clean_text = highlight_scores_in_html(\n",
    "                top_k_tokens_str[i],\n",
    "                top_k_scores_per_seq[i][feature_index],\n",
    "                seq_idx=i,\n",
    "                show_score=True\n",
    "            )\n",
    "            _, false_text = highlight_scores_in_html(\n",
    "                top_k_tokens_str[i],\n",
    "                top_k_scores_per_seq[i][false_feature_index],\n",
    "                seq_idx=i,\n",
    "                show_score=True\n",
    "            )\n",
    "            examples_html.append(example_html)\n",
    "            examples_clean_text.append(clean_text)\n",
    "            examples_false_text.append(false_text)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if len(examples_clean_text) == 0:\n",
    "        raise Exception(\"No activating examples for this one\")\n",
    "\n",
    "    length_examples = len(examples_clean_text) // 2\n",
    "    examples_to_insert = [f\"Example {i + 1}: {example}\" for i, example in enumerate(examples_clean_text)][:length_examples]\n",
    "    combined_clean_text = \"\\n\\n\".join(examples_to_insert).strip()\n",
    "\n",
    "    if len(combined_clean_text) < 100:\n",
    "        raise Exception(\"No activating examples for this one\")\n",
    "\n",
    "    formatted_prompt = format_interpreter_prompt(combined_clean_text, top_logits, bottom_logits)\n",
    "    \n",
    "    response = get_ai_response(formatted_prompt, config)\n",
    "\n",
    "    return formatted_prompt, response, examples_clean_text[:length_examples], examples_clean_text[length_examples:], examples_false_text[:length_examples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (73252 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load configuration\n",
    "config = yaml.safe_load(open(CONFIG_PATH))\n",
    "\n",
    "# Load models\n",
    "mlp = load_mlp(REPO_NAME, MODEL_FILENAME, INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "# Load or compute scores\n",
    "try:\n",
    "    scores = load_scores(SCORES_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Scores file not found at {SCORES_PATH}. Computing scores...\")\n",
    "    owt_tokens_torch = load_tokenized_data()\n",
    "    layer = 9  # Example layer\n",
    "    device = 'cpu'\n",
    "    scores = compute_scores(mlp, load_transformer_model(), owt_tokens_torch, layer, FEATURE_INDICES, device=device)\n",
    "\n",
    "# Load tokenized data\n",
    "owt_tokens_torch = load_tokenized_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "transformer_model = load_transformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate top examples\n",
    "flat_scores = scores.flatten()\n",
    "top_k_indices = flat_scores.argsort()[-TOP_K:][::-1]\n",
    "top_k_scores = flat_scores[top_k_indices]\n",
    "top_k_batch_indices, _, top_k_seq_indices = np.unravel_index(top_k_indices, scores.shape)\n",
    "top_k_tokens = [owt_tokens_torch[batch_idx].tolist() for batch_idx in top_k_batch_indices]\n",
    "top_k_tokens_str = [[transformer_model.to_string(x) for x in token_seq] for token_seq in top_k_tokens]\n",
    "top_k_scores_per_seq = [scores[batch_idx].tolist() for batch_idx in top_k_batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Feature Index: 0 (original feature: 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/sparse-inference/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "### SYSTEM PROMPT ###\n",
      "\n",
      "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior.\n",
      "Guidelines:\n",
      "\n",
      "You will be given a list of text examples on which the neuron activates. The specific tokens which cause the neuron to activate will appear between delimiters like <<this>>. If a sequence of consecutive tokens all cause the neuron to activate, the entire sequence of tokens will be contained between delimiters <<just like this>>. The activation value of the example is listed after each example in parentheses.\n",
      "\n",
      "- Try to produce a concise final description. Simply describe the text features that activate the neuron, and what its role might be based on the tokens it predicts.\n",
      "- If either the text features or the predicted tokens are completely uninformative, you don't need to mention them.\n",
      "- The last line of your response must be the formatted explanation.\n",
      "\n",
      "\n",
      "(Part 1) Tokens that the neuron activates highly on in text\n",
      "\n",
      "Step 1: List a couple activating and contextual tokens you find interesting. Search for patterns in these tokens, if there are any. Don't list more than 5 tokens.\n",
      "Step 2: Write down general shared features of the text examples.\n",
      "\n",
      "\n",
      "\n",
      "(Part 2) Tokens that the neuron boosts in the next token prediction\n",
      "\n",
      "You will also be shown a list called Top_logits. The logits promoted by the neuron shed light on how the neuron's activation influences the model's predictions or outputs. Look at this list of Top_logits and refine your hypotheses from part 1. It is possible that this list is more informative than the examples from part 1.\n",
      "\n",
      "Pay close attention to the words in this list and write down what they have in common. Then look at what they have in common, as well as patterns in the tokens you found in Part 1, to produce a single explanation for what features of text cause the neuron to activate. Propose your explanation in the following format:\n",
      "[EXPLANATION]: <your explanation>\n",
      "\n",
      "\n",
      "We will now provide three step-by-step examples of how you should approach this.\n",
      "\n",
      "\n",
      "### EXAMPLE STEP-BY-STEP WALKTHROUGH 1 ###\n",
      "Example 1:  and he was <<over the moon>> to find\n",
      "Example 2:  we'll be laughing <<till the cows come home>>! Pro\n",
      "Example 3:  thought Scotland was boring, but really there's more <<than meets the eye>>! I'd\n",
      "\n",
      "Top_logits: [\"elated\", \"joyful\", \"story\", \"thrilled\", \"spider\"]\n",
      "\n",
      "(Part 1)\n",
      "ACTIVATING TOKENS: \"over the moon\", \"than meets the eye\".\n",
      "PREVIOUS TOKENS: No interesting patterns.\n",
      "\n",
      "Step 1.\n",
      "The activating tokens are all parts of common idioms.\n",
      "The previous tokens have nothing in common.\n",
      "\n",
      "Step 2.\n",
      "- The examples contain common idioms.\n",
      "- In some examples, the activating tokens are followed by an exclamation mark.\n",
      "\n",
      "Let me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\n",
      "- Yes, I missed one: The text examples all convey positive sentiment.\n",
      "\n",
      "(Part 2)\n",
      "SIMILAR TOKENS: \"elated\", \"joyful\", \"thrilled\".\n",
      "- The top logits list contains words that are strongly associated with positive emotions.\n",
      "\n",
      "[EXPLANATION]: Common idioms in text conveying positive sentiment.\n",
      "\n",
      "### EXAMPLE STEP-BY-STEP WALKTHROUGH 2 ###\n",
      "\n",
      "Example 1:  a river is wide but the ocean is wid<<er>>. The ocean\n",
      "Example 2:  every year you get tall<<er>>,\" she\n",
      "Example 3:  the hole was small<<er>> but deep<<er>> than the\n",
      "\n",
      "Top_logits: [\"apple\", \"running\", \"book\", \"wider\", \"quickly\"]\n",
      "\n",
      "(Part 1)\n",
      "ACTIVATING TOKENS: \"er\", \"er\", \"er\".\n",
      "PREVIOUS TOKENS: \"wid\", \"tall\", \"small\", \"deep\".\n",
      "\n",
      "Step 1.\n",
      "- The activating tokens are mostly \"er\".\n",
      "- The previous tokens are mostly adjectives, or parts of adjectives, describing size.\n",
      "- The neuron seems to activate on, or near, the tokens in comparative adjectives describing size.\n",
      "\n",
      "Step 2.\n",
      "- In each example, the activating token appeared at the end of a comparative adjective.\n",
      "- The comparative adjectives (\"wider\", \"tallish\", \"smaller\", \"deeper\") all describe size.\n",
      "\n",
      "Let me look again for patterns in the examples. Are there any links or hidden linguistic commonalities that I missed?\n",
      "- I can't see any.\n",
      "\n",
      "(Part 2)\n",
      "SIMILAR TOKENS: None\n",
      "- The top logits list contains unrelated nouns and adverbs.\n",
      "\n",
      "[EXPLANATION]: The token \"er\" at the end of a comparative adjective describing size.\n",
      "\n",
      "### EXAMPLE STEP-BY-STEP WALKTHROUGH 3 ###\n",
      "\n",
      "Example 1:  something happening inside my <<house>>\", he\n",
      "Example 2:  presumably was always contained in <<a box>>\", according\n",
      "Example 3:  people were coming into the <<smoking area>>\".\n",
      "\n",
      "However he\n",
      "Example 4:  Patrick: \"why are you getting in the << way?>>\" Later,\n",
      "\n",
      "Top_logits: [\"room\", \"end\", \"container\", \"space\", \"plane\"]\n",
      "\n",
      "(Part 1)\n",
      "ACTIVATING TOKENS: \"house\", \"a box\", \"smoking area\", \"way?\".\n",
      "PREVIOUS TOKENS: No interesting patterns.\n",
      "\n",
      "Step 1.\n",
      "- The activating tokens are all things that one can be in.\n",
      "- The previous tokens have nothing in common.\n",
      "\n",
      "Step 2.\n",
      "- The examples involve being inside something, sometimes figuratively.\n",
      "- The activating token is a thing which something else is inside of.\n",
      "\n",
      "Let me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\n",
      "- Yes, I missed one: The activating token is followed by a quotation mark, suggesting it occurs within speech.\n",
      "\n",
      "(Part 2)\n",
      "SIMILAR TOKENS: \"room\", \"container\", \"space\".\n",
      "- The top logits list suggests a focus on nouns representing physical or metaphorical spaces.\n",
      "\n",
      "[EXPLANATION]: Nouns preceding a quotation mark, representing distinct objects that contain something.\n",
      "\n",
      "\n",
      "\n",
      "### OUR NEURON WE NEED TO INTERPRET STEP-BY-STEP ###\n",
      "\n",
      "\n",
      "(Part 1) Tokens that the neuron activates highly on in text\n",
      "\n",
      "Example 1:   E leanor , played by trans actress Zack ary Dru cker   E leanor leads the support group that Maur a joins when she first comes out <<.>>   G itt el el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and transitioned against the wishes of her mother <<.>> G itt el was arrested by the Nazis for being trans and eventually died in the Holocaust <<.>>   May a\n",
      "\n",
      "Example 2: � � s a bit of a � � duty � � to let some hidden talents come through , and do it without too much bureaucracy and hard times <<they>> might have encountered otherwise trying\n",
      "\n",
      "Example 3: most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and <<LSD>> .   Obama said\n",
      "\n",
      "Example 4: to be a liberal idea . You know , progressives are supposed to be against war , but where are <<they>> now with Obama ? �\n",
      "\n",
      "Example 5: from your side at the chim ing of four And is down in the par lor at work on another . Your break f asts are spoiled , And your <<dinners>> half - bo iled ,\n",
      "\n",
      "\n",
      "\n",
      "(Part 2) Tokens that the neuron boosts in the next token prediction\n",
      "\n",
      "Top_logits: [\" versa\", \" thereafter\", \" afterwards\", \" thereof\", \" respectively\", \" latter\", \" afterward\", \" ones\", \" substituted\", \" later\"]\n",
      "Bottom_logits: [\" Canaver\", \"hene\", \"Sanders\", \"ratom\", \"iannopoulos\", \"monton\", \" enthusi\", \" GOODMAN\", \" glim\", \"annabin\"]\n",
      "\n",
      "\n",
      "Walk through the steps to interpret this neuron.\n",
      "\n",
      "### Analysis\n",
      "\n",
      "(Part 1) Tokens that the neuron activates highly on in text\n",
      "\n",
      "Step 1:\n",
      "- ACTIVATING TOKENS: \".\", \"they\", \"LSD\", \"they\", \"dinners\".\n",
      "- PREVIOUS TOKENS: No clear pattern stands out immediately.\n",
      "\n",
      "Step 2:\n",
      "- General shared features of the text examples:\n",
      "  - The token \".\" appears at the end of sentences.\n",
      "  - \"they\" could refer to a pronoun, suggesting continuation of discussion about a subject.\n",
      "  - \"LSD\" and \"dinners\" are nouns but differ in type (one is a drug, the other a meal).\n",
      "\n",
      "Looking for additional patterns:\n",
      "- The activation of \"they\" suggests linking or continuation.\n",
      "- The activation of \".\" suggests sentence completions or transitions.\n",
      "\n",
      "(Part 2) Tokens that the neuron boosts in the next token prediction\n",
      "\n",
      "SIMILAR TOKENS: \n",
      "- \"versa\", \"thereafter\", \"afterwards\", \"thereof\", \"respectively\", \"latter\", \"afterward\", \"ones\", \"substituted\", \"later\".\n",
      "\n",
      "Common features:\n",
      "- Many of these tokens indicate sequences, transitions, or temporal relationships (e.g., \"thereafter\", \"afterwards\", \"later\").\n",
      "- Other tokens involve equivalence or reference (e.g., \"versa\", \"thereof\", \"respectively\").\n",
      "\n",
      "**Refine Hypothesis:**\n",
      "Combining both insights, it seems that the neuron activates on tokens that are crucial for indicating transitions or references within a text. This is supported by the activating patterns in sentences (either ending or continuing thoughts) and the promoted logits that are related to temporal transitions or equivalence.\n",
      "\n",
      "### Final Explanation\n",
      "\n",
      "[EXPLANATION]: The neuron activates on tokens that signal sentence completions, continuity or transitions in the text, and boosts predictions for subsequent tokens that indicate sequences, temporal relations, or equivalencies.\n"
     ]
    }
   ],
   "source": [
    "feature_index = 0\n",
    "print(f\"Analyzing Feature Index: {feature_index} (original feature: {feature_indices[feature_index]})\")\n",
    "formatted_prompt, analysis, interp_text, scoring_text, false_text = analyze_feature(feature_index, mlp, load_transformer_model(), owt_tokens_torch, scores, top_k_indices, top_k_batch_indices, top_k_tokens, top_k_tokens_str, top_k_scores_per_seq, config)\n",
    "\n",
    "print(formatted_prompt)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad8da9a442c470bb6537c6550c94414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_index': 13, 'analysis': '### PART 1: Activating Tokens Analysis\\n\\nStep 1: List a couple of activating and contextual tokens you find interesting. Search for patterns in these tokens, if there are any.\\n\\nACTIVATING TOKENS: \".\", \"they\", \"LSD\", \"they\", \"dinners\".\\n\\nStep 2: Write down general shared features of the text examples.\\n\\n- The activating tokens include punctuation (periods) and common pronouns (\"they\").\\n- Some tokens represent specific entities or are part of a list (\"LSD\", \"dinners\").\\n- Contextually, the examples seem diverse: from proper nouns and titles (\"Eleanor\") to drugs and meals.\\n\\n### PART 2: Top_logits Analysis\\n\\nTop_logits: [\"versa\", \"thereafter\", \"afterwards\", \"thereof\", \"respectively\", \"latter\", \"afterward\", \"ones\", \"substituted\", \"later\"]\\n\\nWhat they have in common:\\n- The promoted tokens are mostly conjunctions, adverbs, or pronouns that relate to time, sequence, or reference to previous subjects (e.g., \"thereafter\", \"later\", \"respectively\").\\n\\n### Conclusion\\nThe neuron appears to activate on punctuation marks (especially periods) and pronouns while boosting terms that imply sequence or reference to previously mentioned subjects. This suggests the neuron is responsible for understanding sentence boundaries and continuity in text, ensuring coherence especially when transitioning from one portion of text to another.\\n\\n[EXPLANATION]: This neuron activates on punctuation marks (periods) and pronouns, boosting terms that imply sequence or reference to previous subjects to ensure textual coherence and sentence transitions.', 'interpretation': 'This neuron activates on punctuation marks (periods) and pronouns, boosting terms that imply sequence or reference to previous subjects to ensure textual coherence and sentence transitions.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 87, 'analysis': '### (Part 1) Analyzing Tokens that Activate the Neuron\\n\\n#### Step 1: Identifying Activating and Previous Tokens\\n\\nACTIVATING TOKENS: \\n1. \"suicide\"\\n2. \"decision\"\\n3. \"arrests\"\\n4. \"war\"\\n5. \"Analysis\"\\n\\nPREVIOUS TOKENS: \\n- No clear patterns.\\n\\n#### Step 2: General Features of the Text Examples\\n1. The neuron activates on nouns.\\n2. These nouns are often associated with serious or weighty topics (e.g., \"suicide,\" \"arrests,\" \"war\").\\n3. These activating tokens are often central to the subject matter of their respective sentences.\\n\\nDid I miss any patterns in the text examples? Are there more linguistic similarities?\\n- The activating tokens seem to focus on the core subjects or key points of their respective sentences.\\n\\n### (Part 2) Analyzing Top_Logits\\n\\n#### Top Logits\\n- Similar Tokens: \" spree\", \"eering\", \" undertaken\", \" process\", \" techniques\", \" mechanism\", \" technique\", \"ees\", \" conducted\", \"able\"\\n\\n#### Common Characteristics Among Top Logits\\n1. Many tokens are related to processes or actions (e.g., \"undertaken,\" \"conducted\").\\n2. There are also nouns related to methods or systems (e.g., \"technique,\" \"mechanism\").\\n3. The neuron seems to promote words that relate to actions performed or systems in place, often in a context where a serious issue or subject is being discussed.\\n\\nConsidering patterns in both the activating tokens and top logits:\\n- The activating tokens are serious nouns often serving as the subjects in sentences.\\n- The top logits indicate further actions or mechanisms connected to these serious subjects.\\n\\n[EXPLANATION]: Nouns related to serious subjects or issues, often serving as the core subject matter in their sentences and promoting tokens related to actions or processes associated with these serious topics.', 'interpretation': 'Nouns related to serious subjects or issues, often serving as the core subject matter in their sentences and promoting tokens related to actions or processes associated with these serious topics.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 152, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: Marines and hangs out with her trans friends .   E leanor , played by trans actress Zack ary Dru cker   E leanor leads the support group <<that>> Maur a joins when she\\n\\nExample 2: , psychedelic dance music . That � � s what it is . Also I feel that it � � s a bit of a � � duty � � <<to>> let some hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying <<to>> push it forward . Running\\n\\nExample 3: ecstasy and LSD .   Obama said his focus on reforming laws that punish drug users , noting the racial disparity in drug arrests .   \" We <<should>> not be locking up kids\\n\\nExample 4: <<to>> be a liberal idea . You know , progressives are supposed <<to>> be against war , but who recently launched The Ron Paul Institute , a new think - tank � � for peace and prosperity . � �   The Ron Paul Channel is set <<to>> launch officially in the first\\n\\nExample 5: And is down in the par lor at work on another . Your break f asts are spoiled , And your dinners half - bo iled , And your efforts <<to>> get a square supper are fo iled By the crazy - qu ilt man ia that fi end ishly r aves , And <<to>> which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the current study we analyse crazy qu ilts using spatial statistics , comparing them <<to>> ` normal � � regular qu ilts . Qu ilts in general are subject <<to>> a number of constraints that would be difficult <<to>> capture in standard random\\n\\nStep 1.\\n- ACTIVATING TOKENS: \"that\", \"to\", \"should\".\\n- PREVIOUS TOKENS: Various punctuation marks and common words with no clear pattern.\\n\\nStep 2.\\n- The activating tokens are all common functional words, mainly conjunctions (\"that\", \"should\") and prepositions (\"to\").\\n- The context around these tokens is varied, spanning different topics and structures.\\n- There doesn\\'t seem to be a clear pattern in the surrounding context.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No noticeable additional patterns. The tokens \"that\", \"to\", and \"should\" appear commonly in various grammatical structures.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\" emulate\", \" establish\", \" avoid\", \" make\", \" explore\", \" fulfill\", \" satisfy\", \" recreate\", \" pursue\", \" defend\"]\\n\\nStep 1.\\n- SIMILAR TOKENS: \"emulate\", \"establish\", \"avoid\", \"make\", \"explore\", \"fulfill\", \"satisfy\", \"recreate\", \"pursue\", \"defend\".\\n- These tokens are all verbs, often suggesting actions or objectives.\\n\\nStep 2.\\n- Commonality in activated tokens and boosted logits: The neuron seems to activate on functional words (conjunctions, prepositions) that connect clauses or phrases, often leading to verbs that describe actions or objectives.\\n- The top logits list contains verbs suggesting purpose or actions that follow the functional words.\\n\\n[EXPLANATION]: This neuron activates on functional words such as conjunctions and prepositions that connect clauses or phrases, and it boosts verbs that describe subsequent actions or objectives.', 'interpretation': 'This neuron activates on functional words such as conjunctions and prepositions that connect clauses or phrases, and it boosts verbs that describe subsequent actions or objectives.', 'f1_score': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 172, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: ff erm ans who lived in the famous H irsch feld Institute in Berlin and transitioned against the wishes of her mother . G itt el was arrested by the <<Nazis>> for being trans and eventually\\n\\nExample 2: a bit of a � � duty � � to let some hidden talents come through , and do it without too much bureaucracy and hard times they might have <<encountered>> otherwise trying to push it\\nExample 3: jail time when some of the folks who are writing those laws have probably done the same thing ,\" he said . In August , the Obama administration <<announced>> it would not stop Washington\\n\\nExample 4: war , but where are they now with Obama ? � � The channel is the most recent project for a busy post - ret irement Paul who <<recently>> launched The Ron Paul Institute\\n\\nExample 5: other ? She crept from your side at the chim ing of four <<And>> is down in the par\\n\\n\\nStep 1:\\nACTIVATING TOKENS: \"Nazis\", \"encountered\", \"announced\", \"recently\", \"And\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2:\\n- The examples involve a mix of past events and reporting or formal announcements.\\n- Some examples mention significant events or notable occurrences.\\n- The activating tokens do not necessarily follow a specific punctuation pattern.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The activating tokens often relate to historical or contextual circumstances.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"originally\", \"fateful\", \"unsuccessfully\", \"1978\", \"initially\", \"1938\", \"inception\", \"1929\", \"1973\", \"1972\"]\\n\\n- The top logits list contains specific years and terms that indicate the beginning or origin of something.\\n\\n[EXPLANATION]: Tokens related to historical events, contextual circumstances, or formal developments, often linked to specific time periods or notable occurrences.', 'interpretation': 'Tokens related to historical events, contextual circumstances, or formal developments, often linked to specific time periods or notable occurrences.', 'f1_score': 0.4}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 240, 'analysis': '### ANALYSIS ###\\n\\n(Part 1)\\n\\nACTIVATING TOKENS: \"first\", \"one\", \"dangerous\", \"first\", \"another\".\\nPREVIOUS TOKENS:\\n- \"first\": Appears in Example 1 and Example 4.\\n- \"one\": Appears in Example 2.\\n- \"dangerous\": Appears in Example 3.\\n- \"another\": Appears in Example 5.\\n\\nStep 1:\\n- The activating tokens include ordinal numbers (\"first\"), cardinal numbers (\"one\"), adjectives (\"dangerous\"), and demonstrative pronouns (\"another\").\\n- There seems to be a slight emphasis on tokens that may indicate ranking or ordering (\"first\", \"one\").\\n\\nStep 2:\\n- Examples tend to involve contexts with emphasis on sequence or classification.\\n- \"first\" and \"one\" suggest elements of ordering.\\n- \"dangerous\" and \"another\" suggest comparison or distinction from other elements.\\n\\nLet\\'s check for any hidden linguistic similarities:\\n- There’s a nuanced trend of categorizing or distinguishing in the examples: \"first\" in lining up events, \"dangerous\" in classifying severity, \"one\" in describing a singular instance, \"another\" to refer to an additional instance.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"installment\", \"baseman\", \"generation\", \"iteration\", \"incarnation\", \"batch\", \"edition\", \"tier\", \"hurdle\", \"chapter\".\\n- Tokens indicate sequences, versions, groupings, and classifications.\\n\\n[EXPLANATION]: The neuron activates on tokens that reflect ordering, ranking, or classification, often indicating a sequence or a distinct part within a whole.', 'interpretation': 'The neuron activates on tokens that reflect ordering, ranking, or classification, often indicating a sequence or a distinct part within a whole.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 404, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nACTIVATING TOKENS: \"<<cker>>\", \"<<atic>>\", \"<<…>>\", \"<<ecstasy>>\", \"<<prosperity>>\".\\nPREVIOUS TOKENS: \"Zack ary Dru\", \"arm\", \"heroin ,\", \"peace and\".\\n\\nStep 1.\\n- The activating tokens show a mix of suffixes and complete words.\\n- The previous tokens don\\'t show a clear pattern.\\n\\nStep 2.\\n- The examples contain a mix of text that includes proper nouns (names) and generalized terms.\\n- The activating tokens sometimes occur in fragments of words (suffixes like \"atic\", \"cker\") or in complete words (like \"ecstasy\", \"prosperity\").\\n- The examples don\\'t present a clear sequence pattern in context; they cover various topics from media, drugs, to organizations.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nSIMILAR TOKENS: \")\", \"*)\", \")]\", \")/\".\\n- The top logits list focuses primarily on different closing parentheses symbols.\\n- The bottom logits list consists of various unrelated words, which tells us less about the neuron\\'s specific activations.\\n\\n[EXPLANATION]: This neuron activates on suffixes and specific words within diverse contexts, particularly boosting the model\\'s expectation of closing parentheses in following text.', 'interpretation': \"This neuron activates on suffixes and specific words within diverse contexts, particularly boosting the model's expectation of closing parentheses in following text.\", 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 514, 'analysis': '### PART 1 ###\\n\\nStep 1: List interesting activating and contextual tokens.\\n- ACTIVATING TOKENS: <<fe>>, <<to>>, <<.>>, <<run>>, <<of>>.\\n- PREVIOUS TOKENS: \"Pfeffermans\", \"push\", \"said\", \"Boot\".\\n\\nStep 2: General shared features of the text examples.\\n- The activating tokens are common, short sequences of characters frequently appearing in words.\\n- The activating tokens can be parts of words or standalone punctuation marks.\\n- The contexts around these tokens do not show clear patterns in terms of syntax or semantics.\\n- The use of punctuation (period) appears once, which could be indicative but not consistent.\\n\\n### PART 2 ###\\n\\nStep 1: Analyze the promoted tokens in the Top_logits.\\n- SIMILAR TOKENS: \"\\xa0\\xa0\\xa0\", \"\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\", \"\\xa0\\xa0\\xa0\\xa0\", \"\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\", \"\\xa0\\xa0\", \" \", \"Rail\", \"Bus\", \"posted\", \"????????\"\\n- These tokens involve whitespace characters extensively, which is highly uncommon in practical language contexts.\\n\\nStep 2: Refine the hypothesis using both the tokens and the logits.\\n- The activating tokens are mostly common characters or short sequences that appear in varied contexts.\\n- The list of boosted tokens does not provide useful semantic connections but rather hints at the neuron\\'s potential function.\\n- Combining both observations, it seems the neuron may be playing a role in handling formatting or segmentation in the text rather than focusing on specific lexical items or their meanings.\\n\\n### FINAL EXPLANATION ###\\n\\n[EXPLANATION]: Tokens represent short, common character sequences and punctuation, likely involved in handling formatting or text segmentation.', 'interpretation': 'Tokens represent short, common character sequences and punctuation, likely involved in handling formatting or text segmentation.', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 554, 'analysis': '### STEP-BY-STEP ANALYSIS ###\\n\\n\\n#### Part 1: Tokens that the neuron activates highly on in text\\n\\n**Examples:**\\n\\n1. .   G itt el , played by trans actress Har i N ef   G itt el is an ancestor of the P fe ff erm ans <<who>> lived in the famous H (Activation Value)\\n\\n2. ` ` s a bit of a ` ` duty ` ` to let some hidden talents come through , and do it without too much bureaucracy and hard times <<they>> might have encountered otherwise trying (Activation Value)\\n\\n3. racial disparity in drug arrests .   \" We should not be locking up kids or individual users for long stretches of jail time when some of the folks <<who>> are writing those laws have (Activation Value)\\n\\n4. to be a liberal idea . You know , progressives <<are>> supposed to be against war , but where <<are>> they now with Obama ? (Activation Value)\\n\\n5. other ? She crept from your side at the chim ing of four And is down in the par lor at work on another . Your break f asts <<are>> spoiled , And your dinners half - bo iled , And your efforts to get a square supper <<are>> fo iled By the crazy - qu ilt man ia that fi end ishly r aves , And to which all the women <<are>> absolute slaves [ … ]. of Patterns In the current study we analyse crazy qu ilts using spatial statistics , comparing them to ` normal ` ` regular qu ilts . Qu ilts in general <<are>> subject to a number of (Activation Value)\\n\\n**Step 1: Activating and Contextual Tokens**\\n\\nACTIVATING TOKENS: \"who\", \"they\", \"are\".\\nPREVIOUS TOKENS: No consistent patterns.\\n\\n**Step 2: General Shared Features of Text Examples**\\n\\n- The activating tokens (\"who\", \"they\", \"are\") are common pronouns or forms of the verb \"to be\".\\n- The examples suggest dependency on a group or collective action.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The use of pronouns in contexts suggesting group actions or relationships.\\n\\n#### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"themselves\", \"collectively\", \"selves\", \"respectively\", \"alike\", \"converge\", \"clustered\", \"jointly\", \"respective\", \"are\"]\\n\\n**Bottom_logits:** [\"someone\", \"laughs\", \"ventory\", \"availability\", \"ishable\", \"something\", \"washer\", \"Starts\", \"Goes\", \"ヴァ\"]\\n\\n**Similarity and Patterns Analysis:**\\n\\n- The top logits suggest a focus on collective or group-oriented actions and relationships (e.g., \"collectively\", \"jointly\", \"clustered\").\\n- The presence of pronouns and forms of the verb \"to be\" further underlines this collective action.\\n\\n**Conclusion:** \\n\\n[EXPLANATION]: Pronouns or forms of the verb \"to be\" in contexts suggesting group actions or relationships.', 'interpretation': 'Pronouns or forms of the verb \"to be\" in contexts suggesting group actions or relationships.', 'f1_score': 0.5833333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 607, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nStep 1: Activating and contextual tokens\\n\\nACTIVATING TOKENS: \"of\", \"forward\", \"writing\", \"a\", \"get\".\\n\\nStep 2: General shared features of the text examples\\n\\n- Activating tokens seem to be common function words (\"of\", \"a\") and verbs/other common words in various grammatical contexts.\\n- The activation appears within diverse sentence structures and contexts.\\n- There is no strong evidence from these examples alone to pinpoint a clear shared feature.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"lawfully\", \"oneself\", \"predetermined\", \"otherwise\", \"desired\", \"safely\", \"efficiently\", \"wearer\", \"objects\", \"etc\"]\\n\\n- The promoted tokens are diverse but mostly slightly formal or precise words and terms.\\n- They can fit various contexts but seem to relate to clear synthetic or technical expressions.\\n\\nBased on both parts, the neuron seems to play a role in adding specific, somewhat formal options for continuation in a wide range of contexts.\\n\\n[EXPLANATION]: The neuron activates on common function words and ordinary verbs, promoting a precise or formal continuation.', 'interpretation': 'The neuron activates on common function words and ordinary verbs, promoting a precise or formal continuation.', 'f1_score': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 752, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: N ef   G itt el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and transitioned <<against>> the wishes of her mother (activation value)\\n\\nExample 2: is . Also I feel that it’s a bit of a \\'duty\\' to let some hidden talents come through , and do it <<without>> too much bureaucracy and hard (activation value)\\n\\nExample 3: currently classified <<by>> the Drug Enforcement Administration as (activation value)\\n\\nExample 4: to be a liberal idea . You know , progressives are supposed to be against war , but where are they now <<with>> Obama ? (activation value)\\n\\nExample 5: work on another . Your break f asts are spoiled , And your dinners half - bo iled , And your efforts to get a square supper are fo iled <<By>> the crazy - quilt (activation value)\\n\\n\\nStep 1.\\nACTIVATING TOKENS: \"against\", \"without\", \"by\", \"with\".\\n- The activating tokens are prepositions.\\n\\nPREVIOUS TOKENS: No consistent patterns noted within the preceding context. \\n\\nStep 2.\\n- The prepositions are integral to the structure of the sentences.\\n- They serve to connect different elements in the sentences.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- The tokens seem to have a grammatical role as prepositions connecting different parts of speech or clauses.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\" having\", \" being\", \" agreeing\", \" providing\", \" completing\", \" acknowledging\", \" ensuring\", \" knowing\", \" adding\", \" sacrificing\"]\\n\\nStep 1.\\nSIMILAR TOKENS: \" having\", \" being\", \" agreeing\", \" providing\".\\n- The top logits list contains words ending with \"ing\" which typically are gerunds or present participles.\\n\\nStep 2.\\n- The neuron boosts tokens that are often verbs in their gerund or present participle form.\\n- The prepositions (activating tokens) usually precede gerunds or present participles, which is consistent with the boosted logits.\\n\\n[EXPLANATION]: Prepositions that commonly precede gerunds or present participles.', 'interpretation': 'Prepositions that commonly precede gerunds or present participles.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 821, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS:\\n1. \"lived\"\\n2. \"the\"\\n3. \"of\"\\n4. \"of\"\\n5. \"half\"\\n\\nStep 1:\\n- The activating tokens are mostly common words: \"lived\", \"the\", \"of\", \"of\", \"half\".\\n- The previous tokens vary widely and do not show any specific pattern.\\n\\nStep 2:\\n- The examples involve general language and expressions, but there is no clear, strong pattern emerging from just the activating tokens.\\n- The activating tokens appear in different types of sentences and contexts.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- I can\\'t see any clear linguistic pattern or theme.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"decade\", \"week\", \"month\", \"weeks\", \"months\", \"year\", \"YEAR\", \"years\", \"decades\", \"fortnight\".\\n- The top logits list contains terms related to time periods.\\n\\n[EXPLANATION]: The neuron activates on common words (\"lived\", \"the\", \"of\", \"half\") that serve as connectors or fillers in sentences and boosts predictions related to time periods.', 'interpretation': 'The neuron activates on common words (\"lived\", \"the\", \"of\", \"half\") that serve as connectors or fillers in sentences and boosts predictions related to time periods.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 854, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: Berlin and transitioned against the wishes of her mother . G itt el was arrested by the Nazis for being trans and eventually died in the Holocaust .   <<May>> a\\n\\nExample 2: and very bad business decision . I have no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above .   <<Does>> your approach and set list\\n\\nExample 3: currently classified by the <<Drug>> Enforcement Administration as a Schedule\\n\\nExample 4: to be a liberal idea . You know , progressives are supposed to be against war , but where are they now with Obama ? � �   <<The>> channel is the most recent project for a busy post - ret irement Paul who recently launched <<The>> Ron Paul Institute , a\\n\\nExample 5: the crazy - qu ilt man ia that fi end ishly r aves , And to which all the women are absolute slaves [ … ]. Sp atial Analysis of <<Patterns>> In the current study we\\n\\nStep 1.\\nACTIVATING TOKENS: \"May\", \"Does\", \"Drug\", \"The\", \"Patterns\".\\nPrevious Tokens: No interesting patterns.\\n\\nStep 2.\\n- The examples contain a mix of verbs, proper nouns, and general nouns.\\n- The activating tokens do not form a clear single category of words.\\n- The text examples do not show clear thematic patterns.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\" Fund\", \" Matters\", \" Society\", \" Room\", \" Works\", \" Names\", \" Connection\", \" Profession\", \" Values\", \" Records\"]\\nBottom_logits: [\" toget\", \" disg\", \" reper\", \" sidel\", \"etheless\", \" inclined\", \" customary\", \" mechanically\", \" pse\", \" captcha\"]\\n\\n- The top logits list includes common nouns or entities that would often follow formal or structured statements or names.\\n- The promoted tokens suggest an association with formal documents, professional titles, or topics relating to institutional or societal structures.\\n\\n[EXPLANATION]: This neuron activates on tokens that are often parts of or precede formal or structured statements, titles, or names, particularly in professional or institutional contexts.', 'interpretation': 'This neuron activates on tokens that are often parts of or precede formal or structured statements, titles, or names, particularly in professional or institutional contexts.', 'f1_score': 0.6}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 1130, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: Berlin and transitioned against the wishes of her mother . G itt el was arrested by the Nazis for being trans and eventually died in the Holocaust .   <<May>> a\\n\\nExample 2: s not heavily genre bound . It � � s just good , electronic , psychedelic dance music . That � � s what it is . Also I <<feel>> that it � � s\\n\\nExample 3: users for long stretches of jail time when some of the folks who are writing those laws have probably done the same thing ,\" he said .   In <<August>> , the Obama administration announced\\n\\nExample 4: a new think - tank � � for peace and prosperity . � �   The Ron Paul Channel is set to launch officially in the first part of <<August>> .   Follow Gabe\\n\\nExample 5: qu ilt man ia that fi end ishly r aves , And to which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the <<current>> study we analyse crazy qu\\n\\nACTIVATING TOKENS: \"May\", \"August\", \"current\".\\nPREVIOUS TOKENS: \"feel\".\\n\\nStep 1.\\n- The activating tokens \"May\" and \"August\" are months of the year.\\n- \"current\" might relate to a time frame.\\n- \"feel\" is unrelated to time or periods.\\n\\nStep 2.\\n- The examples contain references to specific months or a sense of timing.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"afternoon\", \"evening\", \"morning\", \"th\", \"night\", \"onwards\", \"2017\", \"edition\", \"mornings\", \"2015\"]\\n\\nSIMILAR TOKENS: \"afternoon\", \"evening\", \"morning\", \"night\", \"onwards\", \"2017\", \"2015\".\\n- The top logits suggest a focus on different times of the day, specific years, or periods.\\n\\n[EXPLANATION]: This neuron activates on tokens indicating specific months, time of day, or periods in time.', 'interpretation': 'This neuron activates on tokens indicating specific months, time of day, or periods in time.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 1153, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \"cker\", \"Also\", \"August\", \"know\", \"study\".\\nPREVIOUS TOKENS: \"Dru\", \"Obama administration announced\", \"liberal idea\", \"patterns\".\\n\\nStep 1.\\n- The activating tokens include parts of proper nouns, conjunctions, and months.\\n- The previous tokens do not show any clear specific pattern.\\n\\nStep 2.\\n- The examples contain a mixture of proper nouns, conjunctions, and temporal references.\\n- There seems to be a variety of contexts without a clear common theme.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- The activating tokens may occur at the end of clauses or sentences.\\n- The examples often bring up specific entities or concrete subjects.\\n\\n(Part 2)\\nSIMILAR TOKENS: \",\", \",.\", \",,\", \".,\", \"!,\", \",...\", \" however\", \",-\", \"*,\", \",[\".\\n\\n- The top logits list primarily contains comma variations and a few conjunctions or punctuation marks following a comma.\\n\\n[EXPLANATION]: Tokens that appear frequently at the end of clauses, including proper nouns, conjunctions, and temporal references.', 'interpretation': 'Tokens that appear frequently at the end of clauses, including proper nouns, conjunctions, and temporal references.', 'f1_score': 0.4}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 1276, 'analysis': '### STEP-BY-STEP INTERPRETATION ###\\n\\n#### PART 1: Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs out with her trans friends .   E <<leanor>> , played by trans actress Zack ary Dru cker   E <<leanor>> leads the support group that\\n\\nExample 2: some hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward . Running a label <<like>> this in the digital age\\n\\nExample 3: not be locking up kids or individual users for long stretches of jail time when some of the folks who are writing those laws have probably done the same thing <<,\">> he said .\\n\\nExample 4: � � for peace and prosperity . � �   The Ron Paul Channel is set to launch officially in the first part of August .   Follow <<Gabe>> on Twitter Spring Boot :\\n\\nExample 5: fasts are spoiled , And your dinners half - boiled , And your efforts to get a square supper are foiled By the crazy - quilt <<man>> ia that fiendishly\\n\\n**Step 1: Identify Activating and Contextual Tokens**\\n\\nACTIVATING TOKENS:\\n1. \"Eleanor\"\\n2. \"like\"\\n3. \",\"\\n4. \"Gabe\"\\n5. \"man\"\\n\\n**Step 2: Write Down General Shared Features**\\n\\n- The activating tokens include proper nouns (\"Eleanor\", \"Gabe\"), a common noun (\"man\"), a punctuation mark (\",\"), and a preposition (\"like\").\\n- The text usually gives context to identities or quotations, often involving introductions of names or characters.\\n- The terminology includes proper nouns which are typically first names and quoted speech indications.\\n  \\n#### PART 2: Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\" Allen\", \" Smith\", \" Lee\", \" Reed\", \" Miller\", \" Mc\", \" McC\", \" Alexander\", \" Wilson\", \" Lewis\"]\\n\\n**Identifying Patterns in Top_logits**\\n\\n- The top logits are all common surnames, with some names prefixed by \"Mc\" which could start surnames of Scottish origin.\\n- This collection further suggests the neuron\\'s involvement with personal names.\\n\\n### Conclusion\\n\\n**Drawing Connections between Parts 1 and 2:**\\n\\nBased on the activating tokens, which frequently introduce or involve names or quoted speech, and the top logits, which are common surnames, it becomes clear that this neuron is activated by situations or contexts involving proper names or the lead-up to introducing names.\\n\\n[EXPLANATION]: The neuron activates on tokens that are associated with introducing or referencing personal names, particularly within quoted speech or character identification contexts.', 'interpretation': 'The neuron activates on tokens that are associated with introducing or referencing personal names, particularly within quoted speech or character identification contexts.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 1498, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: with her trans friends .   E leanor , played by trans actress Zack ary Dru cker   E leanor leads the support group that Maur a joins <<when>> she first comes out .\\n\\nExample 2: some hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward . Running a label <<like>> this in the digital age\\n\\nExample 3: currently classified by the Drug Enforcement Administration <<as>> a Schedule 1 substance ,\\n\\nExample 4: August .   Follow Gabe on Twitter Spring Boot : REST + TD D from scratch   B run o Kre bs Bl ocked Un block Follow Following <<Mar>> 2 , 2016  \\n\\nExample 5: And your dinners half - bo iled , And your efforts to get a square supper are fo iled By the crazy - qu ilt man ia that fi end <<ishly>> r aves , And to\\n\\nStep 1.\\nACTIVATING TOKENS: \"when\", \"like\", \"as\", \"Mar\", \"ishly\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- Some tokens are connectors (\"when\", \"like\", \"as\") while others are parts of words/dates (\"Mar\", \"ishly\").\\n- The examples do not have obvious shared features other than the two categories mentioned.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- The activating tokens seem to be functional or part of more complex words/phrases.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"well\", \"opposed\", \"evidenced\", \"soon\", \"pires\", \"usual\", \"pects\", \"far\", \"follows\".\\n- The top logits suggest a variety of contexts, including functional words, suffixes or roots that may complete more complex tokens.\\n\\n[EXPLANATION]: Activation occurs on functional connectors or parts of words/phrases that often complete more complex expressions.', 'interpretation': 'Activation occurs on functional connectors or parts of words/phrases that often complete more complex expressions.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 1609, 'analysis': '### INTERPRETATION STEPS ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich, full life where <<she>> volunteers at a suicide hotline her trans friends. Eleanor, played by trans actress Zackary Drucker. Eleanor leads the support group that Maura joins when <<she>> first comes out.\\n\\nExample 2: “ It’s a bit of a “ duty “ to let some hidden talents come through, and do it without too much bureaucracy and hard times <<they>> might have encountered otherwise trying\\n\\nExample 3: , ecstasy and LSD. Obama said his focus on reforming laws that punish drug users, noting the racial disparity in drug arrests. “ <<We>> should not be locking up\\n\\nExample 4: to be a liberal idea. <<You>> know, progressives are supposed\\n\\nExample 5: mania that fiendishly raves, And to which all the women are absolute slaves [ … ]. Spatial Analysis of Patterns In the current study <<we>> analyse crazy quilts using\\n\\n#### Step-by-Step Process ####\\n\\nStep 1\\nACTIVATING TOKENS: \"she\", \"they\", \"we\", \"you\".\\nPREVIOUS TOKENS: No notable common patterns.\\n\\n- The activating tokens are all pronouns.\\n- The previous tokens do not reveal a strong pattern.\\n\\nStep 2\\nShared features of text examples:\\n\\n- The activating tokens are all pronouns (both singular and plural).\\n- The context varies widely around these pronouns, indicating the neuron responds strongly to different types of subject pronouns.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"\\'re\", \"\\'ve\", \"\\'ll\", \" want\", \" know\", \"\\'d\", \" guessed\", \" wanna\", \" need\", \" think\"]\\nBottom_logits: [\"Canaver\", \"��\", \"Glob\", \"srfAttach\", \"Contents\", \"forms\", \"Palest\", \"unspecified\", \"Thomson\", \"ipal\"]\\n\\n#### Step-by-Step Process ####\\n\\nSIMILAR TOKENS: \"\\'re\", \"\\'ve\", \"\\'ll\", \" want\", \" know\", \"\\'d\".\\n\\n- The top logits are mostly contractions of auxiliary verbs and some common verbs.\\n- These tokens are likely to follow pronouns in typical English sentence structures.\\n\\n### FINAL DESCRIPTION ###\\n\\n[EXPLANATION]: The neuron activates on subject pronouns, likely due to their high probability of being followed by auxiliary verbs or commonly used verbs.', 'interpretation': 'The neuron activates on subject pronouns, likely due to their high probability of being followed by auxiliary verbs or commonly used verbs.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 1829, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy <<Marines>> and hangs out with her\\n\\nExample 2: s not heavily genre bound . It � � s just good , electronic , <<psychedelic>> dance music . That �\\n\\nExample 3: \" the most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , <<ecstasy>> and LSD .  \\n\\nExample 4: for peace and prosperity . � �   The Ron Paul Channel is set to launch officially in the first part of August .   Follow Gabe on <<Twitter>> Spring Boot : REST +\\n\\nExample 5: the chim ing of four And is down in the par lor at work on another . Your break f asts are spoiled , And your dinners half - bo <<iled>> , And your efforts to get a square supper are fo <<iled>> By the crazy - qu\\n\\nStep 1.\\nACTIVATING TOKENS: \"Marines\", \"psychedelic\", \"ecstasy\", \"Twitter\", \"iled\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The activating tokens include proper nouns, specific terms, and parts of words.\\n- The examples often pertain to topics involving drugs, social media, or descriptive terms.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: Several examples pertain to youth or modern culture references (e.g., \"Twitter\", \"ecstasy\", \"psychedelic\").\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\" etc\", \" and\", \" &\", \" or\", \" AND\", \" et\", \",\", \"&\", \",...\", \"etc\"]\\n\\nSIMILAR TOKENS: \"etc\", \"and\", \"or\".\\n- The top logits focus heavily on conjunctions and symbols representing addition or continuation.\\n\\n[EXPLANATION]: Tokens related to youth or modern culture references, and occasionally, parts of specific terms, especially when the context anticipates a list or continuation.', 'interpretation': 'Tokens related to youth or modern culture references, and occasionally, parts of specific terms, especially when the context anticipates a list or continuation.', 'f1_score': 0.5238095238095238}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 2252, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \"May\", \"might\", \"would\", \"to\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are modal verbs (\"might\", \"would\") and an infinitive marker \"to\".\\n- The activating token \"May\" is possibly functioning as a modal verb if it is not the name of a month.\\n\\nStep 2.\\n- Each activating token either conveys possibility/likelihood (modals) or initiates an infinitive (to).\\n- Contexts include discussions of hypothetical or non-assertive scenarios.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No other obvious patterns in the surrounding text.\\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \"likely\", \"inevitably\", \"entail\", \"eventually\", \"require\", \"surely\", \"probably\".\\n- The top logits list contains words that convey likelihood, necessity, or inference.\\n\\n[EXPLANATION]: The neuron activates on modal verbs and the infinitive marker \"to\", indicating scenarios that are hypothetical, possible, or stating intentions or necessities.', 'interpretation': 'The neuron activates on modal verbs and the infinitive marker \"to\", indicating scenarios that are hypothetical, possible, or stating intentions or necessities.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 2260, 'analysis': '### Step-by-Step Walkthrough ###\\n\\n#### Part 1 ####\\n**Tokens that the neuron activates highly on in text:**\\n\\n**Example 1:**\\n- Text: a rich , full life <<where>> she volunteers at a suicide\\n- Activation: Medium\\n\\n**Example 2:**\\n- Text: s not heavily genre bound . It � � s just good , electronic , psychedelic dance music . That � � s what it is . Also I feel <<that>> it � � s a\\n- Activation: Medium\\n\\n**Example 3:**\\n- Text: with a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD .   Obama <<said>> his focus on reforming laws\\n- Activation: High\\n\\n**Example 4:**\\n- Text: to be a liberal idea . You know , progressives are supposed to be against war , <<but>> where are they now with\\n- Activation: Medium\\n\\n**Example 5:**\\n- Text: other ? She crept from your side at the chim ing of four <<And>> is down in the par lor at work on another . Your break f asts are spoiled , <<And>> your dinners half - bo iled , <<And>> your efforts to get a square supper are fo iled By the crazy - qu ilt man ia that fi end ishly r aves , <<And>> to which all the women\\n- Activation: Medium\\n\\n**Step 1: Interesting tokens**\\n- Activating tokens: \"where\", \"that\", \"said\", \"but\", \"And\"\\n- Previous tokens: No remarkable patterns, the text appears to be general narrative sequences.\\n\\n**Step 2: Shared features of the text examples**\\n- These examples are all parts of sentences.\\n- The neuron activates on conjunctions (\"but\", \"And\"), relative pronouns (\"where\", \"that\"), and verbs (\"said\").\\n\\n**Summary of Part 1:**\\n- The neuron seems to activate on conjunctions, relative pronouns, and indicative speech verbs within narrative contexts.\\n\\n#### Part 2 ####\\n**Tokens that the neuron boosts in the next token prediction:**\\n\\nTop_logits: [\"there\", \"nevertheless\", \"fortunately\", \"alas\", \"luckily\", \"it\", \"nonetheless\", \"there\", \"unfortunately\", \"unlike\"]\\n\\n**Analyzing the logits:**\\n- The top logits suggest a focus on contrastive conjunctions and discourse markers (\"nevertheless\", \"nonetheless\", \"unfortunately\", \"unlike\"), as well as pronouns and adverbs indicating continuation or contrast (\"it\", \"there\", \"fortunately\").\\n\\n**Refining Hypothesis:**\\n- The top logits of the neuron align with tokens typically used to introduce contrasts, elaborations, or emphasize parts of a narrative.\\n\\n### Final Explanation ###\\n[EXPLANATION]: The neuron activates on conjunctions, relative pronouns, and indicative speech verbs within sentences, suggesting its role in marking points of contrast, continuation, or emphasis in narrative text.', 'interpretation': 'The neuron activates on conjunctions, relative pronouns, and indicative speech verbs within sentences, suggesting its role in marking points of contrast, continuation, or emphasis in narrative text.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 2480, 'analysis': '### Analysis of the Neuron (Step-by-Step)\\n\\n**(Part 1)** Tokens that the neuron activates highly on in text:\\n\\n**Example 1:** a rich, full <<life>> where she volunteers at a (31.42)\\n**Example 2:** through, and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward. Running a label like this in the <<digital>> age is a pain in (30.95)\\n**Example 3:** as a Schedule 1 substance, which the DEA considers \"the most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or <<physical>> dependence.\" Other Schedule 1 (30.66)\\n**Example 4:** channel is the most recent project for a busy post-retirement Paul who recently launched The Ron Paul Institute, a new think-tank for <<peace>> and prosperity. (30.49)\\n**Example 5:** r aves, And to which all the women are absolute slaves [ … ]. Spatial Analysis of Patterns In the current study we analyse crazy quilts using <<spatial>> statistics, comparing them to (30.38)\\n\\n**Step 1.** \\n*Activating Tokens:* \"life\", \"digital\", \"physical\", \"peace\", \"spatial\". \\n*Previous Tokens:* No clear pattern.\\n\\n**Step 2.**\\n- The activating tokens are generally nouns or adjectives with significant conceptual or abstract meanings.\\n- Many activating tokens relate to broad themes or contexts (e.g., life, digital age, physical dependence, peace, spatial statistics).\\n\\n**Review of patterns:**\\n- The text examples often involve discussions of broader societal, technological, or scientific topics.\\n- The activating tokens tend to be key thematic words in a sentence.\\n\\n**(Part 2)** Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"considerations\", \"relations\", \"affairs\", \"literacy\", \"isation\", \"izations\", \"norms\", \"ization\", \"istically\", \"interactions\"]\\n**Bottom_logits:** [\"hirt\", \"urion\", \"Hundred\", \"tower\", \"xual\", \"wered\", \"lain\", \"20439\", \"ño\", \"urat\"]\\n\\n**Analysis:**\\n- Top logits include abstract nouns and suffixes forming abstract nouns or adjectives (e.g., \"considerations\", \"relations\", \"affairs\", \"literacy\", \"izations\", \"istically\").\\n- These logits indicate a focus on themes related to societal constructs, scientific or academic discourse, and complex interactions or processes.\\n\\n**Integrated Explanation:**\\nCombining insights from the text examples and the logits, the neuron appears to activate on key thematic nouns and adjectives related to societal, academic, and scientific discourse. \\n\\n**[EXPLANATION]:** Key thematic nouns and adjectives in societal, academic, or scientific contexts.', 'interpretation': '### Analysis of the Neuron (Step-by-Step)\\n\\n**(Part 1)** Tokens that the neuron activates highly on in text:\\n\\n**Example 1:** a rich, full <<life>> where she volunteers at a (31.42)\\n**Example 2:** through, and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward. Running a label like this in the <<digital>> age is a pain in (30.95)\\n**Example 3:** as a Schedule 1 substance, which the DEA considers \"the most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or <<physical>> dependence.\" Other Schedule 1 (30.66)\\n**Example 4:** channel is the most recent project for a busy post-retirement Paul who recently launched The Ron Paul Institute, a new think-tank for <<peace>> and prosperity. (30.49)\\n**Example 5:** r aves, And to which all the women are absolute slaves [ … ]. Spatial Analysis of Patterns In the current study we analyse crazy quilts using <<spatial>> statistics, comparing them to (30.38)\\n\\n**Step 1.** \\n*Activating Tokens:* \"life\", \"digital\", \"physical\", \"peace\", \"spatial\". \\n*Previous Tokens:* No clear pattern.\\n\\n**Step 2.**\\n- The activating tokens are generally nouns or adjectives with significant conceptual or abstract meanings.\\n- Many activating tokens relate to broad themes or contexts (e.g., life, digital age, physical dependence, peace, spatial statistics).\\n\\n**Review of patterns:**\\n- The text examples often involve discussions of broader societal, technological, or scientific topics.\\n- The activating tokens tend to be key thematic words in a sentence.\\n\\n**(Part 2)** Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"considerations\", \"relations\", \"affairs\", \"literacy\", \"isation\", \"izations\", \"norms\", \"ization\", \"istically\", \"interactions\"]\\n**Bottom_logits:** [\"hirt\", \"urion\", \"Hundred\", \"tower\", \"xual\", \"wered\", \"lain\", \"20439\", \"ño\", \"urat\"]\\n\\n**Analysis:**\\n- Top logits include abstract nouns and suffixes forming abstract nouns or adjectives (e.g., \"considerations\", \"relations\", \"affairs\", \"literacy\", \"izations\", \"istically\").\\n- These logits indicate a focus on themes related to societal constructs, scientific or academic discourse, and complex interactions or processes.\\n\\n**Integrated Explanation:**\\nCombining insights from the text examples and the logits, the neuron appears to activate on key thematic nouns and adjectives related to societal, academic, and scientific discourse. \\n\\n**[EXPLANATION]:** Key thematic nouns and adjectives in societal, academic, or scientific contexts.', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 2548, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs out with her trans friends .   E leanor , played by trans <<actress>> Zack ary Dru cker   E leanor leads the support group that Maur a joins when she first comes out .   G itt el , played by trans <<actress>> Har i N ef \\n\\nExample 2: hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward . Running a label like <<this>> in the digital age is\\n\\nExample 3: abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD .   Obama said his focus on reforming <<laws>> that punish drug users ,\\n\\nStep 1\\nACTIVATING TOKENS: \"actress\", \"this\", \"laws\"\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2\\n- The example tokens \"actress\", \"this\", and \"laws\" seem unrelated in their immediate context.\\n- The surrounding text doesn\\'t appear to have any shared features.\\n- Tokens occur in various topics ranging from entertainment to policy and technology.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"rookie\", \"Rookie\", \"playoff\", \"preseason\", \"teammate\", \"postseason\", \"quarterback\", \"teammates\", \"rookies\", \"Defensive\"]\\n\\nStep 1\\nSIMILAR TOKENS: \"rookie\", \" Rookie\", \"playoff\", \"preseason\", \"teammate\", \"postseason\", \"quarterback\", \"teammates\", \"rookies\", \"Defensive\".\\n- The top logits list contains sports-related words, particularly American football terminology.\\n\\nFinal Conclusion\\n- The neuron appears to concern very specific contexts and tokens.\\n- The activation examples don\\'t directly relate to the logits which focus on sports terms.\\n- However, considering the likely sports-related nature of the logits and inspecting the activation tokens in a more general sense, let\\'s connect these possibly to the fact that the prediction could be driven by nouns describing entities or principles.\\n\\n[EXPLANATION]: The neuron activates on general nouns that can relate to entities or principles in various contexts, affecting predictions towards sports terminology.', 'interpretation': 'The neuron activates on general nouns that can relate to entities or principles in various contexts, affecting predictions towards sports terminology.', 'f1_score': 0.29292929292929293}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 2566, 'analysis': '## Part 1: Tokens that the neuron activates highly on in text\\n\\n### Step 1: List a couple activating and contextual tokens you find interesting.\\n\\n**ACTIVATING TOKENS:**\\n1. \"G\"\\n2. \"�\"\\n3. \"/\"\\n\\n**PREVIOUS TOKENS:**\\nNo discernable pattern from the previous tokens.\\n\\n### Step 2: Write down general shared features of the text examples.\\n\\n- The activating tokens include special characters (\"�\", \"/\") and a singular capital letter (\"G\").\\n- There are multiple instances of the activating token \"�\".\\n- The overall context of the text examples does not seem to have common linguistic patterns or themes.\\n\\n### General Shared Features:\\n1. Activation on special characters and specific individual capital letters.\\n2. No clear themes or patterns in terms of topic or semantics in the context of the activating tokens.\\n\\n## Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n### SIMILAR TOKENS:\\n- \"s\", \"t\", \"ship\", \"tal\", \"ski\", \"own\", \"sure\", \"sent\", \"mir\".\\n\\n### ANALYSIS:\\n- The top logits promoted by this neuron suggest the neuron influences predictions towards tokens that are parts of words or suffixes, particularly those that seem to appear at the beginning or within words. \\n- The list contains suffixes or fragments that might be common in words across various contexts.\\n\\n### FINAL EXPLANATION:\\n\\n[EXPLANATION]: The neuron activates on special characters and individual capital letters, influencing predictions towards common word fragments and suffixes.', 'interpretation': 'The neuron activates on special characters and individual capital letters, influencing predictions towards common word fragments and suffixes.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 2830, 'analysis': '### PART 1: ANALYZING TEXT EXAMPLES THAT ACTIVATE THE NEURON ###\\n\\n#### Step 1: List of Activating Tokens ####\\n1. \"full\"\\n2. \"hard\"\\n3. \"high\"\\n4. \"post\"\\n5. \"half\"\\n\\n#### Step 2: Shared Features of the Text Examples ####\\n- The activating tokens are common adjectives and adverbs.\\n- The sentences discuss qualities or states in a descriptive manner.\\n- The activating tokens appear in the context of describing situations or attributes related to intensity or completeness (\"full life,\" \"hard times,\" \"high potential,\" \"busy post-retirement,\" \"half-boiled\").\\n\\n### PART 2: TOKENS BOOSTED IN NEXT TOKEN PREDICTION ###\\n\\n#### Top Logits: ####\\n- \"-\", \"water\", \"‑\" (non-breaking hyphen), \"‐\" (hyphen), \"lining\", \"lights\", \"lander\", \"bound\", \"brid\", \"cut\"\\n\\n### Step 3: Refining Hypotheses Based on Top Logits ###\\n\\n#### Similar Tokens: ####\\n- Many tokens are related to connections or separations, such as \"-\" (hyphen), \"‑\" (non-breaking hyphen), \"‐\" (hyphen).\\n- Some tokens (e.g., \"lining\", \"lights\", \"cut\") imply aspects related to boundaries, edges, and completeness.\\n  \\n### Step 4: Final Analysis of Patterns and Hypothesis ####\\nThe text examples feature adjectives and adverbs emphasizing qualities of completeness, difficulty, or intensity. The neuron boosts tokens involving separation and connection, such as hyphens and words implying boundaries or transitions.\\n\\n[EXPLANATION]: The neuron activates on adjectives and adverbs describing qualities of intensity, difficulty, or completeness and boosts tokens related to connections or separations, such as hyphens and boundary-related words.', 'interpretation': 'The neuron activates on adjectives and adverbs describing qualities of intensity, difficulty, or completeness and boosts tokens related to connections or separations, such as hyphens and boundary-related words.', 'f1_score': 0.45054945054945056}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 2841, 'analysis': '(Part 1) \\n\\nStep 1:\\nACTIVATING TOKENS: \"wishes\", \"digital\", \"Obama\", \"Paul\", \"crazy\"\\nPREVIOUS TOKENS: \"the\", \"in the\", \"administration\", \"Institute\", \"quilt\"\\n\\nStep 2:\\n- The activating tokens vary widely. \\n- There is no clear pattern in the specific words themselves.\\n- The contexts span various topics: historical figures, digital age, politics, and metaphors.\\n\\nConsidering linguistic features:\\n- Sometimes the activating tokens are nouns or proper nouns.\\n- The activating tokens can sometimes convey strong sentiments or significant concepts within their context.\\n\\nAdditional observation:\\n- The neuron activates in contexts that discuss impactful subjects or attributes characters with strong descriptors.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"portion\", \"iest\", \"osphere\", \"liest\", \"itself\", \"aspect\", \"hierarchy\", \"process\", \"continuum\", \"era\".\\n- The top logits suggest a mix of suffixes and abstract nouns.\\n- Some of the tokens seem to suggest a focus on properties, qualities, or abstract measures.\\n\\nCombining observations from Part 1 and Part 2:\\n- The activating tokens often denote meaningful or impactful subjects.\\n- The top logits suggest a leaning towards suffixes that typically extend properties or denote abstract concepts. \\n\\n[EXPLANATION]: Tokens that denote impactful subjects or significant descriptors within their context, often conveying strong sentiments or abstract qualities.', 'interpretation': 'Tokens that denote impactful subjects or significant descriptors within their context, often conveying strong sentiments or abstract qualities.', 'f1_score': 0.375}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 2912, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"rich\", \"bad\", \"dangerous\", \"busy\", \"difficult\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are all adjectives.\\n- The previous tokens have nothing in common.\\n\\nStep 2.\\n- The examples contain various adjectives describing different states or qualities.\\n- The adjectives do not seem to follow a common theme in sentiment or descriptive focus.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Upon closer inspection, the adjectives describing various qualities are used in evaluative or descriptive contexts.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"enough\", \"ly\", \"ness\", \"nesses\", \"hearted\", \"ened\".\\n- The top logits list contains suffixes or parts of suffixes that transform adjectives or adverbs (e.g., \"ly\", \"ness\", \"ened\").\\n\\n[EXPLANATION]: Adjectives used in evaluative or descriptive contexts.', 'interpretation': 'Adjectives used in evaluative or descriptive contexts.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 2994, 'analysis': '### STEP-BY-STEP WALKTHROUGH ###\\n\\n(Part 1)\\n\\nACTIVATING TOKENS: \"friends\", \"otherwise\", \"dependence\", \"prosperity\", \"slaves\".\\n\\nStep 1:\\n- The activating tokens appear to be a mix of nouns and adverbs.\\n\\nStep 2:\\n- The examples are diverse in content and context.\\n- Contextually, the activating tokens appear at the ends or close to the ends of sentences or clauses.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The sentences or clauses often seem complete or about to conclude when the activating token appears. \\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \".\", \".[\", \".</\", \".(\", \"*.\", \".\\'\\'.\",\" .\\'\\' \", \".*\", \".\\'\", \".\".\\n\\n- The top logits list primarily consists of punctuation marks, especially full stops (periods) and variants including brackets and asterisks.\\n\\n[EXPLANATION]: Tokens appearing at or near the end of complete sentences or clauses, often followed by punctuation marks indicating a conclusion.', 'interpretation': 'Tokens appearing at or near the end of complete sentences or clauses, often followed by punctuation marks indicating a conclusion.', 'f1_score': 0.4}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 3229, 'analysis': '### Analysis ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nACTIVATING TOKENS: \"Eleanor\", \"�\", \"Schedule\", \"�\", \"`\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens include \"Eleanor,\" which is a name, \"Schedule,\" which refers to a classification term, and several instances of \"�\" and \"`\", which are special characters or quotation marks.\\n\\nStep 2.\\n- The examples contain various special characters: the em-dash \"�\" and backtick \"`\".\\n- The token \"Eleanor\" is a proper noun (a name).\\n- \"Schedule\" is followed by a number or detailed specification indicating classification.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The neuron seems to activate on special characters and names or terms indicating a classification or specific role.\\n\\n### Refined Analysis ###\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nSIMILAR TOKENS: \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\".\\n- The top logits list is dominated by the special character \"�\".\\n\\n### Conclusion ###\\n\\n[EXPLANATION]: The neuron activates on specific names, roles, and various special characters or symbols in the text.', 'interpretation': 'The neuron activates on specific names, roles, and various special characters or symbols in the text.', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 3233, 'analysis': '### Part 1: Tokens that the neuron activates highly on in text\\n\\n**Example Analysis:**\\n\\n1. *Example 1:* \"Eleanor leads the support group that Maur a <<joins>> when she first comes out\"\\n   - **Activation Token:** joins\\n  \\n2. *Example 2:* \"That’s what <<it>> is. Also I feel that it’s a bit of a ‘duty’ to let some hidden talents come through, and do <<it>> without too much bureaucracy and\"\\n   - **Activation Tokens:** it, it\\n  \\n3. *Example 3:* \"folks who are writing those laws have probably done the same thing,\" <<he>> said.\\n   - **Activation Token:** he\\n  \\n4. *Example 4:* \"progressives are supposed to be against war, but where are <<they>> now with Obama?\"\\n   - **Activation Token:** they\\n  \\n5. *Example 5:* \"comparing them <<to>> ` normal’ quilts. Quilts in general are subject to a number of constraints that would be difficult <<to>> capture in standard random\"\\n   - **Activation Tokens:** to, to\\n\\n**Activating Tokens:** \"joins\", \"it\", \"he\", \"they\", \"to\"\\n- The activating tokens consist of pronouns and a preposition.\\n  \\n**Shared Features:**\\n- The tokens often appear in contexts needing follow-up information or actions.\\n- They commonly serve as connecting elements within sentences.\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top Logistic Analysis:**\\n- **Promoted Tokens:** \"happening\", \"transpired\", \"happ\", \"happen\", \"happened\", \"entails\", \"wrought\", \"happens\", \"Happ\", \"entail\"\\n- **Common Features:** These tokens are all verbs in their various conjugations, indicating actions or events.\\n\\n### Final Explanation\\n\\n**[EXPLANATION]:** The neuron activates on pronouns and prepositions that serve as connecting elements within a sentence, specifically those leading to verbs describing actions or events.', 'interpretation': '### Part 1: Tokens that the neuron activates highly on in text\\n\\n**Example Analysis:**\\n\\n1. *Example 1:* \"Eleanor leads the support group that Maur a <<joins>> when she first comes out\"\\n   - **Activation Token:** joins\\n  \\n2. *Example 2:* \"That’s what <<it>> is. Also I feel that it’s a bit of a ‘duty’ to let some hidden talents come through, and do <<it>> without too much bureaucracy and\"\\n   - **Activation Tokens:** it, it\\n  \\n3. *Example 3:* \"folks who are writing those laws have probably done the same thing,\" <<he>> said.\\n   - **Activation Token:** he\\n  \\n4. *Example 4:* \"progressives are supposed to be against war, but where are <<they>> now with Obama?\"\\n   - **Activation Token:** they\\n  \\n5. *Example 5:* \"comparing them <<to>> ` normal’ quilts. Quilts in general are subject to a number of constraints that would be difficult <<to>> capture in standard random\"\\n   - **Activation Tokens:** to, to\\n\\n**Activating Tokens:** \"joins\", \"it\", \"he\", \"they\", \"to\"\\n- The activating tokens consist of pronouns and a preposition.\\n  \\n**Shared Features:**\\n- The tokens often appear in contexts needing follow-up information or actions.\\n- They commonly serve as connecting elements within sentences.\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top Logistic Analysis:**\\n- **Promoted Tokens:** \"happening\", \"transpired\", \"happ\", \"happen\", \"happened\", \"entails\", \"wrought\", \"happens\", \"Happ\", \"entail\"\\n- **Common Features:** These tokens are all verbs in their various conjugations, indicating actions or events.\\n\\n### Final Explanation\\n\\n**[EXPLANATION]:** The neuron activates on pronouns and prepositions that serve as connecting elements within a sentence, specifically those leading to verbs describing actions or events.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 3407, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \",\", \"s\", \"classified\", \"Follow\", \"Analysis\".\\nPREVIOUS TOKENS: \"suicide\", \"currently\", \"tank\", \"unblock\", \"spatial\".\\n\\nStep 1.\\n- The activating tokens are a mix of punctuation, individual letters, and words.\\n- The previous tokens do not immediately show a clear pattern.\\n\\nStep 2.\\n- Examples show the neuron activating on varied contexts, including punctuation, abbreviations, and specific terminology.\\n- The activating tokens appear in varied settings (e.g., suicide hotline, spatial analysis, following on Twitter).\\n\\nLet me think carefully. Are there patterns or linguistic commonalities that I might have missed?\\n- A significant portion of the examples contains proper nouns, titles, and terminologies that might appear in specialized or formal contexts.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"©\", \"Updated\", \"Courtesy\", \"Comments\", \"VIDEOS\".\\n- The top logits list includes tokens related to articles, media, or web content, often appearing in formal publications or editorial content.\\n\\nCombining the observations:\\n- The neuron activates on tokens that are associated with structured, often formal or specialized contexts.\\n- The activating tokens and boosted logits both suggest an affinity for proper formatting, metadata, or content within articles and publications.\\n\\n[EXPLANATION]: Tokens associated with formal or specialized contexts, such as punctuation, abbreviations, or terminologies in articles and publications.', 'interpretation': 'Tokens associated with formal or specialized contexts, such as punctuation, abbreviations, or terminologies in articles and publications.', 'f1_score': 0.6}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 3474, 'analysis': '## Step-by-Step Walkthrough\\n\\n### Part 1: Tokens that the neuron activates highly on in text\\n\\nExample 1: `a rich , full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs out with her <<trans>> friends . E leanor , played by <<trans>> actress Zack ary Dru cker E leanor leads the support group that Maur a joins when she first comes out . G itt el , played by <<trans>> actress Har i N ef`\\n\\nExample 2: `s not heavily genre bound . It � � s just good , electronic , psychedelic dance music . <<That>> � � s what it`\\n\\nExample 3: `We should not be locking up kids or individual users for long stretches of jail time when some of the folks who are writing those laws have probably done the <<same>> thing ,\" he said .`\\n\\nExample 4: `Obama ? � � The channel is the most recent project for a busy post - ret irement Paul who recently launched The Ron Paul Institute , a <<new>> think - tank � �`\\n\\nExample 5: `qu ilt man ia that fi end ishly r aves , And to which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the <<current>> study we analyse crazy qu`\\n\\n---\\n### Part 1 Analysis\\n\\n**Step 1**: \\n\\n**ACTIVATING TOKENS**: \"trans\", \"That\", \"same\", \"new\", \"current\".\\n\\n**Step 2**: \\n1. Tokens include adjectives and pronouns (e.g., \"That\").\\n2. These tokens often introduce or refer to specific categories or concepts.\\n3. They can be used to highlight a particular state or condition (e.g., \"trans\" referring to transgender identity).\\n4. Contextually, these tokens are often followed or preceded by information that specifies or defines something.\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits**: [\"particular\", \"kind\", \"phenomenon\", \"trope\", \"article\", \"tactic\", \"scenario\", \"type\", \"arrangement\", \"week\"]\\n\\n**Bottom_logits**: [\"icons\", \"emis\", \"asures\", \"anches\", \"aws\", \"��\", \"ographies\", \"emale\", \"pots\", \"aughtered\"]\\n\\n### Analysis of Top_logits:\\n\\n- The boosted tokens include words that define or categorize specific entities or concepts (e.g., \"particular\", \"kind\", \"type\", \"phenomenon\").\\n- These tokens are related to specifying, categorizing, or discussing particular types of things.\\n\\n### Part 2 Analysis\\n\\n- The activating tokens often introduce a specific category or concept.\\n- The neuron seems to promote the next token prediction to be something that specifies or elaborates on the mentioned category or concept.\\n\\n### Final Explanation\\n\\n[EXPLANATION]: The neuron activates on adjectives and pronouns that introduce or refer to specific categories or concepts, often prompting the model to predict words that further specify or categorize the mentioned entity or idea.', 'interpretation': 'The neuron activates on adjectives and pronouns that introduce or refer to specific categories or concepts, often prompting the model to predict words that further specify or categorize the mentioned entity or idea.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 3594, 'analysis': '### PART 1\\n\\n**Step 1:** Identifying key patterns in the activating tokens:\\n- \"Berlin\"\\n- \"indoor\"\\n- \"Washington\"\\n- \"liberal\"\\n- \"women\"\\n\\n**Step 2:** Identifying general shared features of the text examples:\\n- Example 1: The location \"Berlin\".\\n- Example 2: \"Indoor\" referenced in a general context.\\n- Example 3: The location \"Washington\".\\n- Example 4: Ideological term \"liberal\".\\n- Example 5: Group term \"women\".\\n\\nThe common feature here appears to be references to locations, ideologies, or specific groups of people.\\n\\n### PART 2\\n\\n**Step 3:** Examining the Top_logits:\\n- \"Airlines\"\\n- \"Airways\"\\n- \"natives\"\\n- \"Springs\"\\n- \"State\"\\n- \"winters\"\\n- \"City\"\\n- \"apolis\"\\n- \"native\"\\n- \"Police\"\\n\\n**Step 4:** Identifying commonalities in Top_logits:\\n- Many of the top logits refer to locations (cities, states) or groups of people (natives, police).\\n\\n### FINAL EXPLANATION\\n\\n[EXPLANATION]: The neuron activates on tokens referring to locations, ideologies, or groups of people.', 'interpretation': 'The neuron activates on tokens referring to locations, ideologies, or groups of people.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 3758, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: , sleeps with sexy Marines and hangs out with her trans friends .   E leanor , played by trans actress Zack ary Dru cker   E leanor <<leads>> the support group that Maur\\n\\nExample 2: psychedelic dance music . That � � s what it is . Also I feel that it � � s a bit of a � � duty � � to <<let>> some hidden talents come through\\n\\nExample 3: in drug arrests .   \" We should not be locking up kids or individual users for long stretches of jail time when some of the folks who are <<writing>> those laws have probably done\\n\\nExample 4: , but where are they now with Obama ? � �   The channel is the most recent project for a busy post - ret irement Paul who recently <<launched>> The Ron Paul Institute ,\\n\\nExample 5: qu ilts using spatial statistics , comparing them to ` normal � � regular qu ilts . Qu ilts in general are subject to a number of constraints that would <<be>> difficult to capture in standard\\n\\n\\nStep 1.\\nACTIVATING TOKENS: \"leads\", \"let\", \"writing\", \"launched\", \"be\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The activating tokens are all verbs.\\n- The activating tokens appear in contexts where some sort of action, initiative, or change is described.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\" sure\", \"hift\", \" sense\", \" adjustments\", \" decisions\", \" excuses\", \" headlines\", \" noises\", \" mistakes\", \" distinctions\"]\\nBottom_logits: [\"anwhile\", \"CLOSE\", \"halla\", \"thia\", \" Niet\", \" WATCHED\", \"bour\", \" destro\", \" withd\", \"assis\"]\\n\\n- The top logits include nouns related to mental or cognitive activities and changes (e.g. \"adjustments\", \"decisions\", \"distinctions\").\\n- The promoted tokens suggest that these activities often result from actions or ongoing processes described by the verbs in the activating tokens.\\n\\n[EXPLANATION]: Verbs describing actions, processes, or changes that often result in mental or cognitive activities.', 'interpretation': 'Verbs describing actions, processes, or changes that often result in mental or cognitive activities.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Found input variables with inconsistent numbers of samples: [10, 11]\n",
      "{'feature_index': 3768, 'analysis': '### Step-by-Step Walkthrough ###\\n\\n#### Part 1 ####\\n\\n**Example Analysis:**\\n1. **Example 1:**\\n   - Highly Activating Token: <<an>>\\n\\n2. **Example 2:**\\n   - Highly Activating Tokens: <<a>>, <<a>>\\n\\n3. **Example 3:**\\n   - Highly Activating Tokens: <<the>>, <<the>>, <<the>>\\n\\n4. **Example 4:**\\n   - Highly Activating Token: <<the>>\\n\\n5. **Example 5:**\\n   - Highly Activating Token: <<absolute>>\\n\\n**Activating Tokens:** \"an\", \"a\", \"the\", \"absolute\".\\n**Previous Tokens:** Sentences often contain various terms and phrases but don\\'t show an obvious commonality before activating tokens.\\n\\n**Shared Features:**\\n- All activating tokens are common articles (\"an\", \"a\", \"the\") or an adjective (\"absolute\").\\n- These tokens serve as determiners, helping denote specificity, quantity, or definiteness within sentences.\\n- These tokens are part of larger grammatical structures, playing a critical role in sentence formation.\\n\\n#### Part 2 ####\\n\\n**Top Logits:** \\n- [\"cornerstone\", \"centerpiece\", \"catalyst\", \"deterrent\", \"staple\", \"motiv\", \"focal\", \"gateway\", \"highlight\", \"fodder\"]\\n\\n**Commonalities in Top Logits:**\\n- The top logits list words that are generally nouns and indicate central or essential aspects of topics.\\n- Many of these words signal importance or prominence, often used in contexts discussing significant elements or focal points.\\n\\n**Inference:**\\n- The neuron activates on tokens that are essential for sentence construction (i.e., articles and determiners that contribute to the framework of a sentence).\\n- The neuron\\'s role involves determining the critical structural components of the sentence, ensuring that the essential elements within the sentence are highlighted or focused upon.\\n\\n**Final Explanation:**\\n\\n[EXPLANATION]: The neuron activates on essential grammatical components such as articles and determiners that support the structural framework of a sentence, indicating importance or central aspects in the context.', 'interpretation': 'The neuron activates on essential grammatical components such as articles and determiners that support the structural framework of a sentence, indicating importance or central aspects in the context.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Found input variables with inconsistent numbers of samples: [10, 11]\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 4140, 'analysis': '(Part 1) \\n\\nACTIVATING TOKENS: \"el\", \"age\", \"laws\", \"Channel\", \"ilts\".\\nPREVIOUS TOKENS: \"Gitt\", \"digital\", \"punish\", \"Paul\", \"crazy qu\".\\n\\nStep 1.\\n- The activating tokens include parts of words and complete short words.\\n- The previous tokens consist of parts of the preceding words; some are names or contain specific references.\\n\\nStep 2.\\n- The examples include contextually significant words or parts of words.\\n- The activating tokens often appear as suffixes or standalone short words in a context where they finalize or highlight an idea, entity, or concept.\\n\\nLet me look again for patterns in the examples. Are there any links or hidden linguistic commonalities that I missed?\\n- The activating tokens seem to be critical for conveying the meaning of the phrase or sentence they are part of.\\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \" is\", \" has\", \" seems\", \" tends\", \" cannot\", \" will\", \" may\", \" relies\", \" seemed\", \" differs\".\\n- The top logits list contains auxiliary verbs and modal verbs that often follow nouns or adjectives to form complete meanings.\\n\\n[EXPLANATION]: Tokens that act as suffixes or standalone short words, usually finalizing an idea or concept.', 'interpretation': 'Tokens that act as suffixes or standalone short words, usually finalizing an idea or concept.', 'f1_score': 0.5238095238095238}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 4777, 'analysis': '### STEP-BY-STEP WALKTHROUGH ###\\n\\n#### (Part 1) Tokens that the neuron activates highly on in text ####\\n\\nExample 1: sexy Marines and hangs out with her trans friends . E leanor , played by trans actress Zack ary Dru cker E leanor leads the support <<group>> that Maur a joins when\\n\\nExample 2: let some hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward. Running a <<label>> like this in the digital\\n\\nExample 3: currently classified by the Drug Enforcement Administration as a Schedule 1 substance , which the DEA considers \" the most dangerous <<class>> of drugs with a high\\n\\nExample 4: � The channel is the most recent project for a busy post - ret irement Paul who recently launched The Ron Paul Institute , a new think - <<tank>> � � for peace and\\n\\nExample 5: ilt man ia that fi end ishly r aves , And to which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the current <<study>> we analyse crazy qu ilts\\n\\n#### ACTIVATING TOKENS: \"group\", \"label\", \"class\", \"tank\", \"study\".\\n\\n#### Step 1. ####\\n- The activating tokens are all nouns.\\n- These nouns are often associated with organizational or categorical entities.\\n\\n#### Step 2. ####\\n- The examples involve nouns that represent some type of organized or classified entity.\\n- The activating tokens often describe an entity that plays a role in structuring information or activity (e.g., \"group\", \"label\", \"class\", \"tank\", \"study\").\\n\\n#### Let me think carefully. ####\\n- Did I miss any patterns? Not really, the nouns all have a clear commonality in their organizational implication.\\n\\n\\n#### (Part 2) Tokens that the neuron boosts in the next token prediction ####\\n\\nTop_logits: [\" consisting\", \" comprising\", \"mable\", \"matic\", \" wherein\", \"ifier\", \" involving\", \" featuring\", \" whose\", \"maker\"]\\n\\n#### SIMILAR TOKENS: \" consisting\", \" comprising\", \" involving\", \" featuring\", \" whose\".\\n- The top logits list features words that often introduce or describe parts of a whole or participatory entities within a larger system.\\n\\n#### [EXPLANATION]: Nouns representing organized or categorized entities involved in structuring activities or information.', 'interpretation': 'Nouns representing organized or categorized entities involved in structuring activities or information.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 4875, 'analysis': '(Part 1) \\n\\nACTIVATING TOKENS: \"ans\", \"s\", \"considers\", \"bs\", \"fi\".\\nPREVIOUS TOKENS: The activating tokens appear in various contexts and transactional phrases with no clear pattern.\\n\\nStep 1.\\n- The activating tokens are primarily \"ans\", \"s\", \"considers\", \"bs\", and \"fi\".\\n- The previous tokens do not exhibit a recognizable pattern.\\n  \\nStep 2.\\n- The text examples do not immediately suggest a single shared feature.\\n- The tokens that activate the neuron appear to be parts of words or suffixes.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: Some examples contain incomplete tokens (e.g., \"ans\", \"s\"), often appearing at word boundaries or mid-word in compound words.\\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \"terday\", \"atisf\", \"outhern\", \"lightly\", \"ustainable\", \"olution\", \"nyder\", \"avior\", \"omew\", \"niper\".\\n- The top logits list contains fragments that are parts or beginnings of words with varying morphemes.\\n\\n[EXPLANATION]: The neuron activates on partial word tokens, particularly those that could form parts of larger words or that are bound by word boundaries or mid-word contexts.', 'interpretation': 'The neuron activates on partial word tokens, particularly those that could form parts of larger words or that are bound by word boundaries or mid-word contexts.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 4943, 'analysis': '### Analysis and Interpretation of the Neuron\\n\\n#### (Part 1) Tokens that the neuron activates highly on in text\\n\\n**Examples:**\\n\\nExample 1: G itt el <<is>> an ancestor of the P (7.5)\\n\\nExample 2: music . That � � s what it <<is>> . (7.4)\\n\\nExample 3: have probably <<are>> writing those laws (7.3)\\n\\nExample 4: launched The Ron Paul Institute , a new think - tank � � for peace and prosperity . � �   The Ron Paul Channel <<is>> set to launch officially in : REST + TD D from scratch (7.3)\\n\\nExample 5: study we analyse crazy qu ilts using spatial statistics , comparing them to ` normal � � regular qu ilts . Qu ilts in general <<are>> subject to a number of (7.3)\\n\\n**Step 1: Analyzing Activating Tokens and Context**\\n- **Activating Tokens**: \"is\", \"is\", \"are\", \"is\", \"are\".\\n- **Previous Tokens**: No strong common patterns.\\n\\n**Step 2: General Shared Features**\\n- The neuron activates strongly on the tokens \"is\" and \"are\".\\n- Activating examples often feature statements of being, description, or existential claims (e.g., \"is an ancestor\", \"is what it\", \"are writing\", \"is set to launch\", \"are subject to\").\\n- These tokens attribute states of being, presence, or assertions within the context.\\n\\n#### (Part 2) Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"certainly\", \"definitely\", \"not\", \"unlikely\", \"hardly\", \"supposed\", \"probably\", \"also\", \"undoubtedly\", \"still\"]\\n\\n**Common Features in Top_logits:**\\n- These tokens are primarily modal adverbs or auxiliary verbs, indicating degrees of certainty, possibility, or necessity (e.g., \"certainly\", \"definitely\", \"hardly\", \"probably\").\\n- They often provide conclusive or definitive assertions (e.g., \"certainly\", \"definitely\", \"undoubtedly\"), or suggest speculation/negation (e.g., \"not\", \"unlikely\", \"hardly\").\\n\\n**Final Synthesis:**\\n- The neuron activates on the existential and affirmative verbs \"is\" and \"are\".\\n- The promoted tokens (\"certainly\", \"definitely\", \"not\", etc.) suggest that the neuron\\'s role includes asserting confidence, doubt, or probability in statements following an existence or state assertion.\\n\\n**[EXPLANATION]: Existential and affirmative verbs (\"is\", \"are\") in contexts asserting being, presence or state, often followed by tokens indicating degree of certainty or doubt.**', 'interpretation': 'Existential and affirmative verbs (\"is\", \"are\") in contexts asserting being, presence or state, often followed by tokens indicating degree of certainty or doubt.**', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 5305, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs out with <<her>> trans friends . G itt el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and transitioned against the wishes of <<her>> mother . G itt el\\n\\nExample 2: very bad business decision . I have no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above . Does <<your>> approach and set list differ\\n\\nExample 3: a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD . Obama said <<his>> focus on reforming laws that\\n\\nExample 4: the first part of August . Follow Gabe on Twitter Spring Boot : REST + TD D from scratch B run o Kre bs Bl ocked <<Un>> block Follow Following Mar 2\\n\\nExample 5: other ? She crept from your side at the chim ing of four And is down in the par lor at work on another . <<Your>> break f asts are spoiled\\n\\nStep 1.\\nACTIVATING TOKENS: \"her\", \"your\", \"his\", \"Un\".\\nPREVIOUS TOKENS: No interesting common patterns.\\n\\nStep 2.\\n- The activating tokens include possessive pronouns (\"her\", \"your\", \"his\").\\n- One activating token is the prefix \"Un\".\\n- The sentences do not show significant contextual consistency beyond the presence of possessive pronouns and one rare case of a prefix.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nSIMILAR TOKENS: \"own\", \"favourite\", \"namesake\", \"favorite\", \"intention\", \"daughter\".\\n- The promoted tokens are often related to possession, personal attributes, or relationships.\\n\\n[EXPLANATION]: The neuron activates on possessive pronouns and sometimes on prefixes like \"Un\", and it boosts tokens related to possession or personal attributes.', 'interpretation': 'The neuron activates on possessive pronouns and sometimes on prefixes like \"Un\", and it boosts tokens related to possession or personal attributes.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 5389, 'analysis': '### Interpretation Walkthrough ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: in the famous H irsch feld Institute in Berlin and transitioned against the wishes of her mother . G itt el was arrested by the Nazis for being trans and <<eventually>> died in the Holocaust .\\n\\nExample 2: duty � � to let some hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it <<forward>> . Running a label like\\n\\nExample 3: jail time when some of the folks who are writing those laws have probably done the same thing ,\" he said .   In August , the Obama administration <<announced>> it would not stop Washington\\n\\nExample 4: recently launched The Ron Paul Institute , a new think - tank � � for peace and prosperity . � �   The Ron Paul Channel is set to <<launch>> officially in the first part\\n\\nExample 5: other ? She crept from your side at the chim ing of four And is down in the par lor at work <<on>> another . Your break f\\n\\nStep 1: List a couple activating and contextual tokens\\n- ACTIVATING TOKENS: \"eventually\", \"forward\", \"announced\", \"launch\", \"on\".\\n- PREVIOUS TOKENS: None.\\n\\nStep 2: General shared features of the text examples\\n- The activating tokens are often verbs or adverbs related to future actions or events.\\n- Some activating tokens are used in the context of sequential or forward-looking progress.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The activating tokens are often associated with forward motion or anticipation of an event.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"tomorrow\", \"2019\", \"2021\", \"2020\", \"next\", \"soon\", \"morrow\", \"someday\", \"eventual\"]\\n\\nStep 2: Similar tokens in the Top_logits\\n- The top logits list contains many words that are related to a future time or event.\\n- These words indicate an anticipation or forward-looking context.\\n\\nBased on these patterns, the final explanation is:\\n\\n[EXPLANATION]: Verbs and adverbs related to future actions or events, indicating forward motion or anticipation.', 'interpretation': 'Verbs and adverbs related to future actions or events, indicating forward motion or anticipation.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 5466, 'analysis': '(Part 1)\\n\\n**ACTIVATING TOKENS:** \"died\", \"feel\", \"said\", \"know\", \"raves\".\\n\\n**PREVIOUS TOKENS:** None relevant.\\n\\n### Step 1 ###\\n- The activating tokens are mostly common verbs: \"died\", \"feel\", \"said\", \"know\", \"raves\".\\n- The previous tokens do not show a discernible pattern.\\n\\n### Step 2 ###\\n- The examples contain common verbs with emotional or sensory connotations (e.g., \"feel\", \"said\", \"knows\", \"died\").\\n- The examples involve reporting or expressing statements and feelings.\\n\\nDid I miss anything?\\n- There\\'s no explicit emphasis on any particular type of emotion (positive or negative).\\n\\n### Summary from Part 1 ###\\nThe neuron activates on common verbs, particularly those expressing emotions, sensations, and statements.\\n\\n(Part 2)\\n\\n### SIMILAR TOKENS ###\\n- The top logits include conversational or connecting terms: \"that\", \"how\", \"why\", \"about\".\\n- They also suggest clarifying or further developing a point in the narrative.\\n\\n**Connection between Part 1 and Part 2:**\\n- The neuron seems to encourage continuation or clarification in speech or narrative, suggested by the promoted tokens (\"that\", \"how\", \"why\").\\n\\n### Final Explanation ###\\n[EXPLANATION]: The neuron activates on common verbs that express emotions, sensations, and statements, typically in a conversational or narrative context, promoting connectors or clarifiers in the text.', 'interpretation': 'The neuron activates on common verbs that express emotions, sensations, and statements, typically in a conversational or narrative context, promoting connectors or clarifiers in the text.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 5604, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \"first\", \"just\", \"currently\", \"recently\", \"ishly\".\\nPREVIOUS TOKENS: None particularly stand out.\\n\\nStep 1.\\n- The activating tokens are all adverbs or adverbial phrases.\\n- The adverbs indicate time (\"first\", \"currently\", \"recently\") or manner (\"just\", \"ishly\").\\n\\nStep 2.\\n- The activating tokens appear to be adverbs.\\n- Many of them relate to time (specifically present or recent time) or manner.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- It seems the commonality lies in the adverbs, specifically those indicating time or manner.\\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \"referred\", \"alluded\", \"hinted\", \"ifiable\", \"mentioned\", \"employed\", \"surpassed\", \"housed\", \"addressed\", \"endorsed\".\\n\\n- The top logits list includes many verbs, some of which are related to communication or action in the past tense (\"referred\", \"alluded\", \"hinted\", \"mentioned\").\\n- These tokens suggest an emphasis on actions or states being described, often in a way linked to time or manner.\\n\\n[EXPLANATION]: Activation is triggered by adverbs indicating time or manner, influencing the prediction of verbs, often related to communication or action.', 'interpretation': 'Activation is triggered by adverbs indicating time or manner, influencing the prediction of verbs, often related to communication or action.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 5675, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \"G\", \"much\", \"most\", \"other\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- Activating tokens seem diverse and don\\'t show a strong pattern at first glance.\\n- However, the activations \"G\" seem significant as it appears multiple times with strong context around it.\\n\\nStep 2.\\n- On closer inspection, \"G\" appears at the beginning of a proper noun at the start of sentences.\\n- The words like \"much\", \"most\", \"other\" are all superlative or comparative adjectives that modify the intensity or degree of something.\\n- The context preceding \"much\", \"most\" deals with extremity and comparison.\\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \"than\", \"emphasis\", \"importantly\", \"attention\", \"scrutiny\".\\n- The top logits list suggests a focus on terms that emphasize a comparison or highlight significance.\\n\\n[EXPLANATION]: This neuron activates on tokens that mark the beginning of proper nouns (with special emphasis on \"G\") and superlative/comparative adjectives indicating degree or intensity.', 'interpretation': 'This neuron activates on tokens that mark the beginning of proper nouns (with special emphasis on \"G\") and superlative/comparative adjectives indicating degree or intensity.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 5693, 'analysis': '### Step-by-Step Walkthrough\\n\\n#### (Part 1) Tokens that the neuron activates highly on in text\\nExample 1: G itt el , played by trans actress Har i N ef G itt el is an ancestor of the P fe ff erm ans who <<lived>> in the famous H irsch\\nExample 2: s not heavily genre bound . It ’ ’ s just good , electronic , psychedelic dance music . That ’ ’ s what it is . <<Also>> I feel that it \\nExample 3: the DEA considers \" the most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs <<include>> heroin , ecstasy and LSD\\nExample 4: to be a liberal idea . You know , progressives are supposed to be <<against>> war , but where are\\nExample 5: By the crazy - qu ilt man ia that fi end ishly r aves , And to which all the women are absolute slaves [ … ]. Sp atial Analysis <<of>> Patterns In the current study we analyse crazy qu ilts using spatial statistics , comparing them to ` normal ’ ’ regular qu ilts . Qu ilts in general are subject to a number <<of>> constraints that would be difficult\\n\\n- ACTIVATING TOKENS: \"lived\", \"also\", \"include\", \"against\", \"of\".\\n- PREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are common prepositions, conjunctions, or verbs.\\n- The previous tokens don\\'t seem to share any noticeable common traits.\\n\\nStep 2.\\n- Examples include common words that connect phrases or lists.\\n- Activating tokens are often part of everyday vocabulary in general contexts.\\n\\nLet me look again for patterns in the examples. Are there any links or hidden linguistic commonalities that I missed?\\n- There\\'s another pattern: These tokens are often found in explanatory or descriptive content.\\n\\n#### (Part 2) Tokens that the neuron boosts in the next token prediction\\nTop_logits: [\" isot\", \" molecular\", \" biologically\", \" nucle\", \" calcium\", \" protein\", \" nitrogen\", \" chemically\", \" organisms\", \" enzymes\"]\\nBottom_logits: [\"Politics\", \" lawy\", \"soDeliveryDate\", \"gallery\", \"ticket\", \"Justice\", \"Street\", \"justice\", \"Security\", \"jobs\"]\\n\\n- SIMILAR TOKENS: \"isot\", \"molecular\", \"biologically\", \"nucle\", \"calcium\", \"protein\", \"nitrogen\", \"chemically\", \"organisms\", \"enzymes\".\\n  - The top logits list contains scientific terms, particularly those related to biology and chemistry.\\n  - Bottom logits suggest non-scientific terms, particularly those related to social sciences, arts, and civics.\\n\\n#### Conclusion\\n[EXPLANATION]: The neuron activates on common words used in explanatory or scientific contexts, and boosts tokens related to scientific terminology or biological and chemical entities.', 'interpretation': 'The neuron activates on common words used in explanatory or scientific contexts, and boosts tokens related to scientific terminology or biological and chemical entities.', 'f1_score': 0.375}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 5789, 'analysis': '## Interpretation Process\\n\\n### Part 1: Tokens that the neuron activates highly on in text\\n\\n#### Step 1: List of activating and contextual tokens\\n1. Activating token: \"P\"\\n2. Activating token: \"k\"\\n3. Activating tokens: \"on\"\\n\\n#### Step 2: General shared features of the text examples\\n1. Example 1: The activation occurred on the first letter of a surname (\"Pfeffermans\").\\n2. Example 2: The activation occurred on the first letter of the word \"karmatic\".\\n3. Example 3: The activation occurred on the preposition \"on\".\\n4. Example 4: The activation occurred on the preposition \"on\".\\n5. Example 5: The activation occurred on the preposition \"on\".\\n\\n- The examples include the initial letters of words (including capitalized and special words like surnames).\\n- The activating tokens include the common preposition \"on\".\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, the neuron seems to activate on a mix of prepositions (\"on\") and initials of words.\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n#### Similar tokens promoted by the neuron\\n1. \"behalf\"\\n2. \"occasion\"\\n3. \"site\"\\n4. \"shore\"\\n5. \"occasions\"\\n\\n#### Analysis of the promoted tokens\\n- The promoted tokens are primarily nouns, with an occasional preposition-like usage (e.g., \"behalf\").\\n- They imply a reference to specific locations or occasions, suggesting a contextual setting.\\n\\n#### Refined Hypotheses:\\n- The neuron\\'s activation is related primarily to either prepositions or the initials of significant nouns or words.\\n\\n[EXPLANATION]: The neuron activates on prepositions and initial letters of significant nouns, suggesting it plays a role in identifying and predicting contextual or locational settings in text.', 'interpretation': 'The neuron activates on prepositions and initial letters of significant nouns, suggesting it plays a role in identifying and predicting contextual or locational settings in text.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 6091, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"being\", \"heavily\", \"be\", \"be\", \"be\".\\nPREVIOUS TOKENS: No clear pattern in previous tokens.\\n\\nStep 1.\\n- The activating tokens frequently include variations of the verb \"be\" and the adverb \"heavily\".\\n- The tokens \"be\" and its variations appear multiple times.\\n\\nStep 2.\\n- The text examples where the neuron activates often involve forms of the verb \"be\".\\n- Some examples also involve verbs related to state or condition, such as \"being\".\\n- The context around the activating tokens varies significantly in theme and topic.\\n\\nLet me look again for patterns in the examples. Are there any more linguistic commonalities?\\n- The verb \"be\" seems central to sentences discussing states or conditions.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"replaced\", \"taken\", \"evaluated\", \"examined\", \"judged\", \"handled\", \"reviewed\", \"investigated\", \"tested\", \"implemented\".\\n- The top logits list contains verbs in the past participle form, often used in passive constructions.\\n\\nLet me consider both the patterns in the examples and the top logits list. What features of text cause the neuron to activate?\\n- The neuron activates on forms of the verb \"be\", which are commonly used in passive voice constructions.\\n\\n[EXPLANATION]: The neuron activates on forms of the verb \"be\", often used in passive voice constructions discussing states or conditions.', 'interpretation': 'The neuron activates on forms of the verb \"be\", often used in passive voice constructions discussing states or conditions.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 6300, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: Maur a joins when she first comes out .   G itt el , played by trans actress Har i N ef   G itt el is an <<ancestor>> of the P fe ff\\n\\nExample 2: . That � � s what it is . Also I feel that it � � s a bit of a � � duty � � to let some hidden <<talents>> come through , and do\\n\\nExample 3: the racial disparity in drug arrests .   \" We should not be locking up kids or individual users for long stretches of jail time when some of the <<folks>> who are writing those laws\\n\\nExample 4: to be a liberal idea . You know , <<progressives>> are supposed to be against\\n\\nExample 5: f asts are spoiled , And your dinners half - bo iled , And your efforts to get a square supper are fo iled By the crazy - qu ilt <<man>> ia that fi end ishly\\n\\nStep 1.\\nACTIVATING TOKENS: \"ancestor\", \"talents\", \"folks\", \"progressives\", \"man\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The activating tokens are mostly nouns.\\n- The activating tokens appear to describe people or qualities/attributes of people.\\n\\n(Part 2)\\nTop_logits: [\" who\", \" whom\", \"who\", \"hood\", \" whose\", \" beware\", \"folk\", \" Who\", \" wanting\", \"\\'s\"]\\n\\nSIMILAR TOKENS: \"who\", \"whom\", \"who\", \"whose\", \"who\".\\n- Many of the top logits are pronouns or words related to people.\\n\\n[EXPLANATION]: Nouns that describe people or attributes/qualities of people.', 'interpretation': 'Nouns that describe people or attributes/qualities of people.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 6303, 'analysis': '### Part 1\\n\\n#### Step 1: List a couple activating and contextual tokens you find interesting.\\n\\nACTIVATING TOKENS: \"was\", \"feel\", \"be\", \"are\".\\nPREVIOUS TOKENS: No interesting patterns in the previous tokens identified.\\n\\n#### Step 2: Write down general shared features of the text examples.\\n\\n- The activating tokens \"was\", \"be\", and \"are\" are all forms of the verb \"to be\".\\n- The token \"feel\" acts as a linking verb.\\n- These verbs are used to describe states of being, feelings, or occurrences.\\n\\n### Part 2\\n\\n#### Similar Tokens: \\n\\nTop_logits: [\"aware\", \"willing\", \"afraid\", \"able\", \"interested\", \"enjoying\", \"unable\", \"obsessed\", \"thankful\", \"addicted\"]\\n\\n- The top logits include adjectives that describe a person\\'s emotional or physical state or their abilities.\\n\\n### [EXPLANATION]\\n\\n[EXPLANATION]: The neuron activates on linking verbs that describe states of being, feelings, or occurrences, and it boosts predictions of adjectives that express emotional or physical states or abilities.', 'interpretation': 'The neuron activates on linking verbs that describe states of being, feelings, or occurrences, and it boosts predictions of adjectives that express emotional or physical states or abilities.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 6432, 'analysis': '## Part 1: Analyzing Activating Tokens\\n\\n### Step 1: Observing Interesting Activating and Contextual Tokens\\n\\n**ACTIVATING TOKENS:**\\n1. \"sexy\"\\n2. \"some\"\\n3. \"Other\"\\n4. \"are\"\\n5. \"of\"\\n\\nThere are no clear previous token patterns across these examples.\\n\\n### Step 2: Identifying Shared Features of Text Examples\\n\\nThe tokens that activate the neuron are common words, but each word appears in a different context. The words themselves do not provide explicit patterns.\\n\\nHowever, let\\'s look deeper:\\n1. \"sexy\" is an adjective.\\n2. \"some\" and \"Other\" are determiners.\\n3. \"are\" is a verb.\\n4. \"of\" is a preposition.\\n\\nThe activating tokens seem to be commonly used words that can appear in varied contexts.\\n\\n## Part 2: Analyzing Predicted Tokens\\n\\n### Observing and Analyzing Top Logits\\n\\n**TOP_LOGITS:**\\n- \"instances\"\\n- \"types\"\\n- \"examples\"\\n- \"occasions\"\\n- \"pieces\"\\n- \"cases\"\\n- \"categories\"\\n- \"kinds\"\\n- \"factors\"\\n- \"locations\"\\n\\nThese top logits mainly represent nouns that categorize or group other nouns.\\n\\n### Deriving Final Explanation\\n\\nCombining the observations, we see the neuron activates on common words (\"sexy\", \"some\", \"Other\", \"are\", \"of\") and then boosts predictions of nouns that denote types, collections, or categories (\"instances\", \"types\", \"examples\", etc.).\\n\\n[EXPLANATION]: Commonly used non-specialized words preceding nouns that categorize or group other nouns.', 'interpretation': 'Commonly used non-specialized words preceding nouns that categorize or group other nouns.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "Found input variables with inconsistent numbers of samples: [10, 11]\n",
      "{'feature_index': 7010, 'analysis': '### Step-By-Step Analysis ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: Marines and hangs out with her trans friends .   E leanor , played by trans actress Zack ary Dru cker   E leanor leads the support group <<that>> Maur a joins when she\\n\\nExample 2: s not heavily genre bound . It ’ ’ s just good , electronic , psychedelic dance music . That ’ ’ s <<what>> it is . Also I\\n\\nExample 3: and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD .   Obama said his focus on reforming laws <<that>> punish drug users , noting\\n\\nExample 4: against war , but where are they now with Obama ? ’ ’   The channel is the most recent project for a busy post - ret irement Paul <<who>> recently launched The Ron Paul\\n\\nExample 5: are spoiled , And your dinners half - bo iled , And your efforts to get a square supper are fo iled By the crazy - qu ilt man ia <<that>> fi end ishly r aves analyse crazy qu ilts using spatial statistics , comparing them to ` normal ’ regular qu ilts . Qu ilts in general are subject to a number of constraints <<that>> would be difficult to capture\\n\\nStep 1.\\nACTIVATING TOKENS: \"that\", \"what\", \"who\".\\nPREVIOUS TOKENS: No clear consistent pattern in the previous tokens.\\n\\nStep 2.\\n- The activating tokens are all relative pronouns.\\n- The examples suggest that the neuron is highly active on relative pronouns like \"that,\" \"what,\" and \"who.\"\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No additional clear patterns beyond the fact that the activating tokens are relative pronouns.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"resulted\", \"accompanies\", \"preceded\", \"resembles\", \"translates\", \"constitutes\", \"governs\", \"comprises\", \"encompasses\", \"lasted\"]\\n\\nStep 1.\\nSIMILAR TOKENS: \"resulted\", \"accompanies\", \"preceded\", \"resembles\", \"translates\", \"constitutes\", \"governs\", \"comprises\", \"encompasses\", \"lasted\".\\n- These tokens are verbs that describe the relationship or outcome of a subject.\\n\\nStep 2.\\nThe tokens boosted in the next token prediction appear to describe actions or characteristics often associated with relative clauses that provide more information about something previously mentioned.\\n\\n[EXPLANATION]: The neuron activates on relative pronouns (that, what, who) which introduce relative clauses providing more information about the subject.', 'interpretation': 'The neuron activates on relative pronouns (that, what, who) which introduce relative clauses providing more information about the subject.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 7025, 'analysis': '#### PART 1: Tokens that the neuron activates highly on in text\\n\\n**Example Analysis:**\\n1. Example 1: \"G itt el was arrested by the Nazis <<for>> being trans and eventually died\" (activation: high)\\n2. Example 2: \"...and do it without too <<much>> bureaucracy and hard times they\" (activation: high)\\n3. Example 3: \"...with a high potential for abuse and potentially <<severe>> psychological and / or physical\" (activation: high)\\n4. Example 4: \"Since there is a lot <<of>> stuff\" (activation: high)\\n5. Example 5: \"...your efforts <<to>> get a square supper are...\" (activation: high)\\n\\n**Step 1:**\\n- ACTIVATING TOKENS (Examples): \"for\", \"much\", \"severe\", \"of\", \"to\"\\n- OBSERVATIONS: Tokens seem to be common words.\\n\\n**Step 2:**\\n- The activating tokens \"for\", \"much\", \"severe\", \"of\", and \"to\" are primarily function words (prepositions, determiners) and a comparative adjective.\\n- The context of these examples often involves complex or negative scenarios (examples mentioning bureaucracy, psychological effects, or being arrested).\\n\\n**General shared features:**\\n- The text examples feature discussions often involving complex, bureaucratic, or negative/emotionally charged contexts.\\n\\n---\\n\\n#### PART 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:**\\n- [\"havoc\", \"worsening\", \"crippling\", \"devastation\", \"violence\", \"instability\", \"harassment\", \"abuse\", \"debilitating\", \"injustice\"]\\n\\n**Pattern:**\\n- The boosted words all describe negative or harmful states, situations, or actions.\\n\\n**Step 3:**\\n**Refined Hypothesis:**\\n- The neuron appears to activate in contexts describing difficult, challenging, or negative scenarios. The neuron boosts words that emphasize harm, instability, or adversity.\\n\\n[EXPLANATION]: The neuron activates on function words within text describing difficult, challenging, or negative scenarios and boosts predictions related to harm, instability, or adversity.', 'interpretation': 'The neuron activates on function words within text describing difficult, challenging, or negative scenarios and boosts predictions related to harm, instability, or adversity.', 'f1_score': 0.29292929292929293}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 7211, 'analysis': '### Part 1: Tokens that the neuron activates highly on in text\\n\\n**Step 1: List a couple activating and contextual tokens you find interesting. Search for patterns in these tokens, if there are any.**\\n- Interesting activating tokens: \"against\", \"no\", \"not\", \"Un\", \"to\".\\n- Previous tokens: These vary significantly and don’t appear to follow a specific pattern.\\n\\n**Step 2: Write down general shared features of the text examples.**\\n- The activation seems to occur with tokens that indicate negation or a direction of argument.\\n- Examples are varied in context, including digital age challenges, law reform, quilting statistics, and blocking/unblocking actions.\\n- The common factor is the use of words that imply either an opposition (\"against\") or negation (\"no\", \"not\", \"Un\").\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits: [\"anymore\", \"necessarily\", \"nor\", \"whatsoever\", \"bothered\", \"bother\", \"necess\", \"anywhere\", \"yet\", \"adequately\"]**\\n- These logits are associated with negative qualifiers or adverbs, words involving some form of negation or qualification of previous statements.\\n\\n**[EXPLANATION]: The neuron activates on tokens that imply negation or opposition.**', 'interpretation': 'The neuron activates on tokens that imply negation or opposition.**', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 7371, 'analysis': '## (Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs out with her trans friends <<.>>   E leanor , played by trans actress Zack ary Dru cker   E leanor leads the support group that Maur a joins when she first comes out <<.>>   G itt el el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and transitioned against the wishes of her mother <<.>> G itt el was arrested by the Nazis for being trans and eventually died in the Holocaust <<.>>   May a\\n\\nExample 2: “ s just good , electronic , psychedelic dance music . That “ s what it is . Also I feel that it “ s a bit of <<a>> “ duty “\\n\\nExample 3: dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD <<.>>   Obama said his focus on reforming laws that punish drug users , noting the racial disparity in drug arrests <<.>>   \" We should up kids or individual users for long stretches of jail time when some of the folks who are writing those laws have probably done the same thing ,\" he said <<.>>   In August ,\\n\\nExample 4: most recent project for a busy post - ret irement Paul who recently launched The Ron Paul Institute , a new think - tank “ for peace and prosperity <<.>> “ “   The Ron Paul Channel is set to launch officially in the first part of August <<.>>   Follow Gabe on\\n\\nExample 5: side at the chim ing of four And is down in the par lor at work on another . Your break f asts are spoiled , And your dinners half <<->> bo iled , And your\\n\\nStep 1.\\n\\n**ACTIVATING TOKENS:** \".\", \"a\", \".\"\\n\\n**PREVIOUS TOKENS:** Various context around dialogues or statements.\\n\\n- The neuron activates on tokens representing the end of declarative sentences (periods), indefinite articles (\"a\"), and hyphens mid-word or mid-line.\\n  \\nStep 2.\\n- The activates are frequently found in text that likely involves statements or end of sentences.\\n- There is a clear pattern of sentence-ending punctuation.\\n- Occasionally, the activating tokens are tied to the beginning of a new clause.\\n\\nThese examples generally feature delimiters (periods, indefinite articles, or mid-line hyphens) which help demarcate the boundary of clauses or statements.\\n\\n## (Part 2) Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"Asked\", \"Officials\", \"READ\", \" spokeswoman\", \"PHOTOS\", \" Reuters\", \" spokesman\", \" Officials\", \"Reuters\", \"Experts\"]\\n\\n**Bottom_logits:** [\"��\", \"��\", \"ベ\", \" unnecess\", \" metic\", \"milo\", \"���\", \" assum\", \"��\", \"��\"]\\n\\n- The neuron boosts anticipatory tokens often associated with official statements, reporting, or authoritative figures. Many tokens suggest journalistic content or entities associated with news and information dissemination.\\n\\n**Similar Tokens:** \"Asked\", \"Officials\", \"spokeswoman\", \"Reuters\", \"Experts\".\\n\\n- The boosted tokens tend to involve formal reporting language or referenced individuals/entities that are authoritative or integral to conveying information, aligning with journalism or press-related contexts.\\n\\n**Explanation Synthesis:**\\n- The activations appear prominently with functions demarcating the end of sentences, introducing new statements, or within transitions in text discussing factual or reported information.\\n\\n[EXPLANATION]: Tokens representing punctuation (especially end of sentences) and indefinite articles that delineate clauses or boundaries in formally structured text, often found in journalistic or report-based content.\\n', 'interpretation': 'Tokens representing punctuation (especially end of sentences) and indefinite articles that delineate clauses or boundaries in formally structured text, often found in journalistic or report-based content.\\n', 'f1_score': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 8479, 'analysis': '## Step 1: Analyzing the Activating and Contextual Tokens\\n\\n### ACTIVATING TOKENS:\\n1. Example 1: \"leads\" \\n2. Example 2: \"push\"\\n3. Example 3: \"writing\"\\n4. Example 4: \"against\"\\n5. Example 5: \"analyse\"\\n\\n### PREVIOUS TOKENS:\\n- \"Eleanor\": Contextual name\\n- \"hidden talents\", \"bureaucracy\": Context of overcoming obstacles\\n- \"not locking up kids\": Context of critique and reform\\n- \"liberal idea\", \"progressives\": Political context\\n- \"fiendishly raves\", \"spatial analysis\": Context of complex actions or critiques\\n\\n## Step 2: General Shared Features of the Text Examples\\n- The activating tokens often involve action verbs or oppositional contexts, such as \"leads\", \"push\", \"writing\", \"against\", \"analyse\".\\n- The surrounding context often involves reform, critique, leadership, or analytical actions.\\n- There is also frequent reference to complex actions or policies.\\n\\n## Part 2: Analyzing the Top Logits\\n\\n### TOP LOGITS:\\n- [\"them\", \"him\", \"anything\", \"these\", \"it\", \"your\", \"yourself\", \"those\", \"oneself\", \"this\"]\\n\\n### Observations:\\n- The top logits are pronouns and determiners, which are generally used to refer to people, objects, or concepts previously mentioned.\\n  \\n## Final Explanation\\n\\nCombining the action-oriented nature of the activating tokens with the pronoun-centric top logits, it appears that this neuron is particularly focused on signaling actions or oppositional discussion contexts and subsequently helping the model to reference those actions or the subjects of those actions in the following text.\\n\\n[EXPLANATION]: Tokens representing actions and oppositional contexts that facilitate subsequent referencing of those actions or subjects through pronouns and determiners.', 'interpretation': 'Tokens representing actions and oppositional contexts that facilitate subsequent referencing of those actions or subjects through pronouns and determiners.', 'f1_score': 0.4949494949494949}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 8480, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"Dru\", \"�\", \"�\", \"�\", \"�\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are mostly non-standard characters or sequences like \"�\".\\n- The previous tokens vary widely and don\\'t seem to follow a specific pattern.\\n\\nStep 2.\\n- The examples contain non-standard characters that may be encoding or typographical errors.\\n- The sequences often include the character \"�\".\\n\\n(Part 2)\\nSIMILAR TOKENS: \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\".\\n- The top logits list exclusively contains the character \"�\".\\n\\n[EXPLANATION]: Non-standard or erroneous characters typically used to represent encoding or typographical errors.', 'interpretation': 'Non-standard or erroneous characters typically used to represent encoding or typographical errors.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 8481, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"played\", \"explained\", \"classified\", \"set\", \"iled\".\\nPREVIOUS TOKENS: No significant patterns detected.\\n\\nStep 1.\\n- The activating tokens seem to be mostly past tense verbs.\\n- There are some sound-based or phonetic similarities among the activating tokens enging in “ed” and \"ied.\"\\n\\nTokens Analysis:\\n1. \"played\" → verb in past tense\\n2. \"explained\" → verb in past tense\\n3. \"classified\" → verb in past tense\\n4. \"set\" → verb in past tense\\n5. \"iled\" → fragment of past tense verbs\\n\\nStep 2.\\n- In each example, the activating tokens appear to be past tense verbs or fragments signifying past tense conjugations.\\n- The tokens are mostly used to provide description or clarification in text.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"by\", \"prominently\", \"separately\", \"elsewhere\", \"herein\", \"solely\", \"therein\", \"alongside\", \"throughout\", \"exclusively\".\\n- The promoted tokens suggest positioning, adverbial descriptions or connective relationships.\\n\\nTop Logits Analysis:\\n- These logits often follow phrases or clauses and give additional information about the action, describing relational or locational context.\\n\\nCombining insights:\\n- Past tense verbs typically indicating events or states that have occurred.\\n- The neuron appears to reinforce the narrative flow by following with adverbs or prepositions that provide context or details about the action.\\n\\n[EXPLANATION]: Past tense verbs or related fragments indicating events or actions that occurred, followed by tokens providing relational or locational context.', 'interpretation': 'Past tense verbs or related fragments indicating events or actions that occurred, followed by tokens providing relational or locational context.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 8694, 'analysis': '(Part 1) Identify patterns in the activating tokens and text examples.\\n\\nACTIVATING TOKENS: \"Nazis\", \"psychedelic\", \"Obama\", \"crazy\".\\nPREVIOUS TOKENS: No specific patterns.\\n\\nStep 1.\\n- The activating tokens are nouns or adjectives with strong emotional or cultural connotations.\\n\\nStep 2.\\n- The examples involve culturally or emotionally charged terms.\\n- The activating tokens often appear in contexts discussing specific topics like politics, music, or behavior.\\n\\nLet me double-check for additional patterns:\\n- All activating tokens are associated with intense or significant social discourse.\\n\\n(Part 2) Analyze the top logits for additional insights.\\n\\nSIMILAR TOKENS: \"\\'s\", \" Labs\", \"®\", \"mania\", \" Technologies\", \"HQ\".\\n- The top logits list includes terms related to entities or brands, as well as some common suffixes.\\n\\n[EXPLANATION]: The neuron activates on culturally or emotionally charged terms often present in discussions on politics, behavior, or specific subcultures.', 'interpretation': 'The neuron activates on culturally or emotionally charged terms often present in discussions on politics, behavior, or specific subcultures.', 'f1_score': 0.45054945054945056}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 8744, 'analysis': '(Part 1)  \\n\\nACTIVATING TOKENS: \"out\", \"duty\", \"kids\", \"retirement\", \"women\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are relatively common words.\\n- The tokens can refer to aspects of life stages, responsibilities, or social roles, but they do not form a clear pattern on their own.\\n\\nStep 2.\\n- The examples often relate to various stages of life and social participation. For instance:\\n  - Coming \"out\" (gender/sexual identity context).\\n  - \"Duty\" tied to a sense of responsibility or role.\\n  - \"Kids\" pertains to family or younger individuals.\\n  - \"Retirement\" relates to an older stage of life.\\n  - \"Women\" as a gender-specific group.\\n\\nThe underlying shared feature might be how these words are associated with particular social roles or stages of life.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"aged\", \"unmarried\", \"daughters\", \"children\", \"grandmother\".\\n- The top logits contain words that describe family roles, relationships, or stages of life.\\n\\nCombining observations from both parts, the neuron appears to activate on tokens that deal with social roles or stages of life, and it boosts words related to familial roles and life stages.\\n\\n[EXPLANATION]: Tokens associated with social roles or stages of life, especially those relating to family and generational roles.', 'interpretation': 'Tokens associated with social roles or stages of life, especially those relating to family and generational roles.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 8765, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"for\", \"for\", \"for\", \"for\", \"get\".\\nPREVIOUS TOKENS: \"being trans\", \"doing so apart from the\", \"abuse and potentially severe psychological focus on\", \"a busy post-retirement Paul who recently launched\", \"efforts to\".\\n\\nStep 1.\\n- The activating tokens are mostly \"for\".\\n- The previous tokens do not show a consistent pattern.\\n\\nStep 2.\\n- In each example, the activating token \"for\" precedes a noun or noun phrase.\\n- Some instances contain lengthy explanations or descriptions following \"for\".\\n\\nLet me reassess: Are there any patterns in the phrases or surrounding context? \\n- Yes, \"for\" often introduces a clarification, justification, or purpose.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"purposes\", \"sake\", \"instance\", \"example\", \"reasons\".\\n- The top logits list contains words related to clarifying, exemplifying, or providing justification.\\n\\n[EXPLANATION]: The preposition \"for\" used to introduce clarifications, justifications, or purposes.', 'interpretation': 'The preposition \"for\" used to introduce clarifications, justifications, or purposes.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 9146, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: by trans actress Har i N ef   G itt el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld <<Institute>> in Berlin and transitioned against\\n\\nExample 2: let some hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward . Running a <<label>> like this in the digital\\n\\nExample 3: currently classified by the Drug Enforcement Administration as a Schedule 1 substance , which the <<DEA>> considers \" the most dangerous\\n\\nExample 4: they now with Obama ? � �   The channel is the most recent project for a busy post - ret irement Paul who recently launched The Ron Paul <<Institute>> , a new think -\\n\\nExample 5: other ? She crept from your side at the chim ing of four And is down in the par <<lor>> at work on another .\\n\\n(Part 1)\\nACTIVATING TOKENS: \"Institute\", \"label\", \"DEA\", \"Institute\", \"lor\".\\nPREVIOUS TOKENS: No common patterns.\\n\\nStep 1.\\n- The activating tokens mostly refer to organizations or institutions (\"Institute\", \"DEA\", \"label\").\\n- \"lor\" seems out of place but might be a truncated version of \"parlor\", which could be related to a type of room or place.\\n\\nStep 2.\\n- The examples include references to organizations, institutions, or specific entities.\\n- The context involves either formal or categorized entities.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, the activating tokens seem to relate to formal entities or specific designations.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"offices\", \"officials\", \"headquarters\", \"Office\", \"spokesman\", \"personnel\", \"staff\", \"spokeswoman\", \"brass\", \"inspector\"]\\n\\nSIMILAR TOKENS: \"offices\", \"officials\", \"headquarters\", \"Office\", \"spokesman\".\\n- The top logits list includes words related to official titles, positions, or places within organizations.\\n\\n[EXPLANATION]: The neuron activates on tokens that are related to organizations or formal entities, triggering predictions of related official or institutional terms.', 'interpretation': 'The neuron activates on tokens that are related to organizations or formal entities, triggering predictions of related official or institutional terms.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 9358, 'analysis': '(Part 1)\\n\\nStep 1.\\nACTIVATING TOKENS: \"Maur\", \"Does\", \"/\", \"Un\", \"Sp\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The activating tokens have no clear shared linguistic or thematic pattern.\\n- The text preceding the activating tokens varies significantly in subject matter and structure.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No evident additional patterns or commonalities are present.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"\\'t\", \"í\", \"´\", \"ned\", \"uts\", \"eness\", \"n\", \"iting\", \"ates\", \"ited\".\\n- The top logits list contains tokens that appear to be parts of words or suffixes.\\n\\n[EXPLANATION]: The neuron activates on fragmented tokens, which might suggest detecting boundaries within or between words.', 'interpretation': 'The neuron activates on fragmented tokens, which might suggest detecting boundaries within or between words.', 'f1_score': 0.5238095238095238}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 9383, 'analysis': '(Part 1) Text Examples Analysis\\n\\nExample 1: the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and transitioned against the wishes of her mother . G itt el was <<arrested>> by the Nazis for being\\n\\nExample 2: s not heavily genre bound . It � � s just good , electronic , psychedelic dance music <<.>> That � � s what it is <<.>> Also I feel that it\\n\\nExample 3: currently classified by the Drug Enforcement Administration as <<a>> Schedule 1 substance , which\\n\\nExample 4: Boot : REST + TD D from scratch   B run o Kre bs Bl ocked Un block Follow Following Mar 2 , 2016   Disclaimer : Since <<there>> is a lot of stuff\\n\\nExample 5: at work on another . Your break f asts are spoiled , And your dinners half - bo iled , And your efforts to get a square supper are fo <<iled>> By the crazy - qu\\n\\nStep 1.\\nACTIVATING TOKENS: \"arrested\", \".\", \"a\", \"there\", \"iled\".\\nPREVIOUS TOKENS: No obvious patterns.\\n\\nStep 2.\\n- The examples have varied contexts, with some involving legal, structural, or narrative elements.\\n- The activating tokens do not appear to have a shared semantic category.\\n- Some tokens appear in mundane, general contexts (e.g., \"there\", \"a\", \".\").\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No additional patterns are apparent from these tokens alone.\\n\\n(Part 2) Tokens Boosted in Next Token Prediction\\n\\nTop_logits: [\"unidentified\", \"suspected\", \"arrested\", \"alleged\", \"detained\", \"allegedly\", \"gunfire\", \"police\", \"gunman\", \"assailants\"]\\n\\nStep 1.\\nSIMILAR TOKENS: \"unidentified\", \"suspected\", \"arrested\", \"alleged\", \"detained\", \"allegedly\", \"gunfire\", \"police\", \"gunman\", \"assailants\".\\n- The top logits indicate terms related to law enforcement, crime, and investigation.\\n\\nFinal Explanation\\nGiven the disjointed nature of the activating tokens in Part 1 and the clear pattern emerging from the Top_logits in Part 2, the neuron seems to play a role in contexts related to law enforcement or crime reportage.\\n\\n[EXPLANATION]: Tokens in contexts related to law enforcement or crime investigation.', 'interpretation': 'Tokens in contexts related to law enforcement or crime investigation.', 'f1_score': 0.6}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 9441, 'analysis': '### PART 1 ###\\n\\n**Step 1: List a couple of activating and contextual tokens you find interesting.**\\n\\nACTIVATING TOKENS: \",\", \"s\", \"August\", \"bo\".\\n\\n**Step 2: Write down general shared features of the text examples.**\\n\\n1. **Punctuation**: The neuron seems to activate on commas, which suggests a linguistic pattern related to punctuation or sentence structures.\\n2. **Letters**: The activation of \"s\" and \"bo\" indicates that specific letters or parts of words might also be a trigger.\\n3. **Months**: \"August\" suggests some activation might be on specific named entities like months.\\n\\n### PART 2 ###\\n\\n**Tokens that the neuron boosts in the next token prediction**\\n\\nSIMILAR TOKENS: \"5\", \"8\", \"9\", \"6\", \"7\".\\n\\nThe top logits list suggests a strong focus on numeric values, particularly single-digit numbers.\\n\\n**Combining observations from Part 1 and Part 2:**\\n\\n1. **Punctuation**: The common activation on commas is clear.\\n2. **Letters and part-words**: The neuron also activates on specific letters, suggesting involvement in word formation or transitions.\\n3. **Named entities**: The month \"August\" might indicate that the neuron is also triggered by specific named entities.\\n4. **Numeric sequences**: The top logits heavily focus on numbers, implying a possible relationship with sequences, dates, enumerations, or lists.\\n\\n[EXPLANATION]: Neuron activates on punctuation marks, specific letters, and named entities, particularly in contexts where numeric sequences or lists are likely to follow.', 'interpretation': 'Neuron activates on punctuation marks, specific letters, and named entities, particularly in contexts where numeric sequences or lists are likely to follow.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 9693, 'analysis': '### Step-by-Step Walkthrough ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich, full life <<where>> she volunteers at a suicide (Activation: High)\\n\\nExample 2: duty – – to let some hidden talents come through, and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it <<forward>>. Running a label like (Activation: High)\\n\\nExample 3: ecstasy and LSD. Obama said his focus on reforming laws that punish drug users, noting the racial disparity in drug arrests. \"We <<should>> not be locking up kids (Activation: High)\\n\\nExample 4: the most recent project for a busy post-retirement Paul who recently launched The Ron Paul Institute, a new think-tank – – for peace and <<prosperity>>. – – (Activation: High)\\n\\nExample 5: four And is down in the parlor at work on another. Your breakfasts are spoiled, And your dinners half-boiled, And your <<efforts>> to get a square supper (Activation: High)\\n\\nStep 1.\\n\\nACTIVATING TOKENS: \"where\", \"forward\", \"should\", \"prosperity\", \"efforts\".\\nPREVIOUS TOKENS: \"life\", \"bureaucracy\", \"laws\", \"think-tank\", \"dinners\".\\n\\nStep 2.\\n\\n- The activating tokens are various forms of conjunctions, modals, and nouns.\\n- The previous tokens often describe activities and conditions related to improvement or progress.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The overall context involves elements of growth, improvement, or positive change.\\n\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"unparalleled\", \"inspiring\", \"excellence\", \"sustainable\", \"invaluable\", \"exemplary\", \"sustainability\", \"empowering\", \"innovative\", \"professionalism\"]\\n\\nSIMILAR TOKENS: \"unparalleled\", \"inspiring\", \"excellence\", \"sustainable\", \"invaluable\", \"exemplary\", \"sustainability\", \"empowering\", \"innovative\", \"professionalism\"\\n- The top logits list contains words associated with positive qualities, improvements, or achievements.\\n\\nCombining insights from Parts 1 and 2:\\n\\n[EXPLANATION]: This neuron activates in contexts that describe actions, conditions, or efforts related to growth, improvement, or positive change, and it boosts words associated with high quality, achievements, or progress.', 'interpretation': 'This neuron activates in contexts that describe actions, conditions, or efforts related to growth, improvement, or positive change, and it boosts words associated with high quality, achievements, or progress.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 9741, 'analysis': '(Part 1)\\n\\nStep 1.\\nACTIVATING TOKENS: \"out\", \"differ\", \"disparity\", \"Disclaimer\", \"comparing\".\\nPREVIOUS TOKENS: No significant patterns.\\n\\nStep 2.\\n- The activating tokens don\\'t seem to show straightforward patterns in their immediate text context.\\n- However, the activating tokens could be linked to contexts involving differentiation, comparison, variance, or presenting a note of caution/advice.\\n\\nLet me review carefully to see if there\\'s any shared linguistic link:\\n- Yes, the activating tokens all relate conceptually to the idea of differentiation or pointing out differences.\\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \"between\", \"with\", \"thereto\", \"between\", \"seamlessly\", \"dots\".\\n- The top logits list contains words that imply connection, conjunction, or differentiation.\\n\\n(Part 1 and Part 2 Analysis)\\nCombining the patterns found in the activating tokens and the top logits:\\n- The neuron activates on words and phrases that are used to denote comparison, differentiation, or transitions.\\n- The predicted tokens further emphasize this observation, as they often describe relationships or transitions (e.g., \"between\", \"with\", \"seamlessly\").\\n\\n[EXPLANATION]: The neuron activates on tokens that indicate comparison, differentiation, or transitions in text.', 'interpretation': 'The neuron activates on tokens that indicate comparison, differentiation, or transitions in text.', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 9944, 'analysis': '## Analysis of the Neuron\\'s Behavior\\n\\n### Part 1: Patterns in Activating Tokens  \\n\\n#### Step 1: Relevant Activating Tokens  \\n1. \"H\" in \"H irschfeld\"\\n2. \"I\" in \"I have\"\\n3. \"his\" in \"Obama said his\"\\n4. \":\" in \"Disclaimer :\"\\n5. \"we\" in \"we analyze\"\\n\\n#### Step 2: Common Features of Text Examples\\n- The activating tokens are mainly pronouns (\"I\", \"we\", \"his\") and punctuation marks (colon \":\") or alphabets that could begin proper nouns (\"H\").\\n- Contexts vary but include explanations, statements, or examples where the activating tokens are performing meaningful roles in the sentence (pronouns standing in for subjects or objects, punctuation indicating a list or explanation).\\n\\n### Part 2: Patterns in Top_logits\\n\\n#### Similar Tokens Promoted by the Neuron\\n1. \"\\'m\"\\n2. \"myself\"\\n3. \"am\"\\n4. \"guess\"\\n5. \"personally\"\\n6. \"\\'ve\"\\n7. \"think\"\\n8. \"suppose\"\\n9. \"believe\"\\n10. \"hope\"\\n\\n#### Observations\\n- The top logits indicate a strong correlation with personal or reflective language.\\n- Tokens promoted are mostly first-person singular pronouns or verbs, emphasizing self-reference or personal opinion (\"I\", \"myself\", \"\\'m\", \"think\", \"believe\", etc.).\\n\\n### Consolidated Explanation\\nBased on the patterns identified in the activating tokens and the tokens boosted in next-token predictions, this neuron\\'s behavior appears to be linked to phrases or contexts that involve personal pronouns or possessive pronouns, as well as punctuation used for clarifying or listing information. The neuron seems to be playing a role in predicting text sequences that include references to the self or personal reflection.\\n\\n[EXPLANATION]: The neuron activates on personal pronouns, possessive pronouns, and clarifying punctuation, promoting tokens that involve self-reference or personal opinion.', 'interpretation': 'The neuron activates on personal pronouns, possessive pronouns, and clarifying punctuation, promoting tokens that involve self-reference or personal opinion.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 10147, 'analysis': '### STEP-BY-STEP WALKTHROUGH ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs out with her trans <<friends>> .   E leanor\\n\\nExample 2: s not heavily genre bound . It � � s just good <<,>> electronic <<,>> psychedelic dance music . That � s what it is . Also I feel that it � � s a bit of a � � duty � � to let some hidden talents come through <<,>> and do it without too\\n\\nExample 3: the DEA considers \" the most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs <<include>> heroin , ecstasy and LSD\\n\\nExample 4: to be a liberal idea . You know , progressives are supposed to be against war , but where are they now with Obama <<?>> � �   The\\n\\nExample 5: other <<?>> She crept from your side\\n\\nStep 1.\\nACTIVATING TOKENS: \"friends\", \",\", \"include\", \"?\", \"?\".\\nPREVIOUS TOKENS: No recurring patterns.\\n\\nStep 2.\\n- The examples contain punctuation marks (comma and question mark).\\n- One example has a noun related to people or relationships (\"friends\").\\n- The last tokens in the examples are frequently punctuation marks.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"!!!!\", \"?!\", \" �\", \"damn\", \"!!!\", \" ;)\", \"fucking\", \"!!\", \"lol\", \" :)\"]\\nBottom_logits: [\"ourses\", \"eatures\", \"icipated\", \"iminary\", \"ccording\", \"atform\", \"actionDate\", \"quartered\", \"everal\", \"principally\"]\\n\\nSIMILAR TOKENS: \"!!!!\", \"?!\", \"damn\", \"!!!\", \"fucking\", \"lol\", \" :)\"\\n\\n- The top logits list primarily contains exclamatory or emotive expressions and punctuation.\\n- The tokens promoted are generally associated with informal, enthusiastic, or emphatic communication.\\n\\n[EXPLANATION]: Punctuation marks and select social or relational nouns in informal contexts, aiming to predict exclamatory or emotive expressions.', 'interpretation': 'Punctuation marks and select social or relational nouns in informal contexts, aiming to predict exclamatory or emotive expressions.', 'f1_score': 0.45054945054945056}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 10192, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: and hangs out with her trans friends .   E leanor , played by trans actress Zack ary Dru cker   E leanor leads the support group that <<Maur>> a joins when she first\\n\\nExample 2: a � � duty � � to let some hidden talents come through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying <<to>> push it forward . Running\\n\\nExample 3: currently classified by the Drug Enforcement Administration as a Schedule 1 substance , which the DEA considers \" the most dangerous class of drugs with a high potential <<for>> abuse and potentially severe psychological\\n\\nExample 4: the first part of August .   Follow Gabe on Twitter Spring Boot : REST + TD D from scratch   B run o Kre bs Bl ocked <<Un>> block Follow Following Mar 2\\n\\nExample 5: And is down in the par lor at work on another . Your break f asts are spoiled , And your dinners half - bo iled , And your efforts <<to>> get a square supper are fo iled By the crazy - qu ilt man ia that fi end ishly r aves , And <<to>> which all the women are In the current study we analyse crazy qu ilts using spatial statistics , comparing them to ` normal � � regular qu ilts . Qu ilts in general are subject <<to>> a number of constraints that would be difficult <<to>> capture in standard random\\n\\n\\nStep 1.\\nACTIVATING TOKENS: \"Maur\", \"to\", \"for\", \"Un\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The activating tokens are often small function words or names.\\n- The context of the activating tokens is quite diverse, spanning different types of sentences and topics.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"afford\", \"easily\", \"\\'t\", \"safely\", \"feas\", \"adian\", \"imagine\", \"plaus\", \"pinpoint\", \"legitimately\"]\\nBottom_logits: [\"furt\", \"rant\", \"rium\", \"Matters\", \"Flags\", \"favors\", \"Mant\", \"doms\", \"��\", \"Lauder\"]\\n\\nThe top logits list contains a mix of adverbs and fragments that hint at somewhat technical or precise vocabulary (e.g., \"afford\", \"easily\", \"safely\", \"imagine\", \"plaus\", \"pinpoint\", \"legitimately\"). \\n\\nConsidering both the patterns from the activating tokens and the top logits:\\n\\n- The activating tokens mostly consist of function words: \"to,\" \"for,\" \"Un.\"\\n- The top logits suggest the neuron helps predict precise or technical vocabulary.\\n\\n[EXPLANATION]: Function words in diverse contexts, likely facilitating the prediction of precise or technical vocabulary in the subsequent text.', 'interpretation': 'Function words in diverse contexts, likely facilitating the prediction of precise or technical vocabulary in the subsequent text.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 10308, 'analysis': '### Step-by-Step Walkthrough ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: sexy Marines and hangs out with her trans friends . E leanor , played by trans actress Zack ary Dru cker E leanor leads the support <<group>> that Maur a joins when\\n\\nExample 2: through , and do it without too much bureaucracy and hard times they might have encountered otherwise trying to push it forward . Running a label like this in the <<digital>> age is a pain in\\n\\nExample 3: for abuse and potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD . Obama said his focus on <<reforming>> laws that punish drug users\\n\\nExample 4: Obama ? � � The channel is the most recent project for a busy post - ret irement Paul who recently launched The Ron Paul Institute , a <<new>> think - tank � �\\n\\nExample 5: ishly r aves , And to which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the current study we analyse crazy qu ilts <<using>> spatial statistics , comparing them\\n\\nACTIVATING TOKENS: \"group\", \"digital\", \"reforming\", \"new\", \"using\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are: \"group\", \"digital\", \"reforming\", \"new\", \"using\".\\n- The previous tokens vary widely, so they don\\'t seem to show a consistent pattern.\\n\\nStep 2.\\n- The activated tokens are associated with progressiveness, change, or modern concepts.\\n- The text often involves forward-thinking actions or innovations.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, the tokens often occur in discussions about modern or progressive topics, including technology, social change, and innovation.\\n\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"technology\", \"smartphones\", \"technologies\", \"platforms\", \"computers\", \"devices\", \"wireless\", \"smartphone\", \"iPhones\", \"software\"]\\nBottom_logits: [\"abal\", \"xual\", \"conclusions\", \"nil\", \"Dialogue\", \"poems\", \"tale\", \"Conclusion\", \"Investigator\", \"retribution\"]\\n\\nSIMILAR TOKENS: \"technology\", \"smartphones\", \"technologies\", \"platforms\", \"computers\", \"devices\", \"wireless\", \"smartphone\", \"iPhones\", \"software\".\\n- The top logits list contains words strongly associated with technology.\\n\\n(Part 2 Conclusion)\\n- The tokens the neuron boosts are related to technology and modern devices.\\n- This reinforces the idea from Part 1 that the neuron activates on concepts associated with progressiveness and modernity.\\n\\n[EXPLANATION]: The neuron activates on tokens related to progressive or modern concepts, often involving technology or innovation.', 'interpretation': 'The neuron activates on tokens related to progressive or modern concepts, often involving technology or innovation.', 'f1_score': 0.6}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 10420, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps <<with>> sexy Marines and hangs out <<with>> her trans friends . \\n\\nExample 2: psychedelic dance music . That � � s what it is . Also I feel that it � � s a bit of a � � duty � � to <<let>> some hidden talents come through\\n\\nExample 3: potentially severe psychological and / or physical dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD .   Obama said his focus on reforming laws that <<punish>> drug users , noting the\\n\\nExample 4: You know , progressives are supposed to be against war , but where are they now with Obama ? � �   The channel is the most recent project <<for>> a busy post - ret irement Paul who recently launched The Ron Paul Institute , a new think - tank � � <<for>> peace and prosperity . �\\n\\nExample 5: bo iled , And your efforts to get a square supper are fo iled By the crazy - qu ilt man ia that fi end ishly r aves , And <<to>> which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the current study we analyse crazy qu ilts using spatial statistics , comparing them <<to>> ` normal � � regular qu ilts . Qu ilts in general are subject <<to>> a number of constraints that would be difficult <<to>> capture in standard random\\n\\n(Part 1)\\nACTIVATING TOKENS: \"with\", \"let\", \"punish\", \"for\", \"to\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are mostly prepositions and some verbs.\\n- The previous tokens vary widely and don\\'t show a clear pattern.\\n\\nStep 2.\\n- The examples contain tokens that are functions words (\"with\", \"for\", \"to\") or common verbs (\"let\", \"punish\").\\n- These tokens are often used to connect sentences or phrases, indicating relationships between actions or entities.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, many of these tokens appear to either precede a person (e.g., \"with her trans friends\") or to describe interactions/relationships.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"fellow\", \"locals\", \"passers\", \"colleagues\", \"journalists\", \"anyone\", \"unsuspecting\", \"listeners\", \"investors\", \"people\"]\\nBottom_logits: [\"iHUD\", \"inventoryQuantity\", \"ワン\", \"wcsstore\", \"inction\", \"mol\", \"claimer\", \"Interstitial\", \"conclud\", \"acci\"]\\n\\nStep 1.\\nSIMILAR TOKENS: \"fellow\", \"locals\", \"colleagues\", \"journalists\", \"anyone\", \"listeners\", \"investors\", \"people\".\\n- The top logits list contains words that refer to people or groups of people.\\n\\nStep 2.\\n- The top logits list suggests the neuron is also concerned with predicting entities involved in social contexts or interactions.\\n- The activating tokens, usually prepositions or verbs, appear to frame these social entities or interactions.\\n\\n[EXPLANATION]: The neuron activates on prepositions and verbs that are used to frame or indicate relationships or interactions involving people.', 'interpretation': 'The neuron activates on prepositions and verbs that are used to frame or indicate relationships or interactions involving people.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 10508, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nStep 1: \\nACTIVATING TOKENS: \"of\", \"of\", \"of\", \"of\", \"of\".\\n\\nStep 2: \\n- The activating tokens are instances of \"of\".\\n- No discernible patterns in the previous tokens.\\n\\nThe neuron strongly activates on the preposition \"of\" in various contexts. \\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nSIMILAR TOKENS: \"the\", \"ours\", \"theirs\", \"these\", \"course\", \"hers\", \"our\", \"those\", \"mankind\", \"humankind\".\\n- The top logits list includes determiners (\"the\", \"these\", \"those\"), possessive pronouns (\"ours\", \"theirs\", \"hers\", \"our\"), and collective nouns (\"mankind\", \"humankind\").\\n\\nThe activation patterns suggest that the neuron is detecting the preposition \"of\" and boosting tokens that typically follow \"of\" in possessive or determiner phrases, as well as collective nouns.\\n\\n[EXPLANATION]: The preposition \"of\", often in the context of introducing possessive or determiner phrases, and collective nouns.', 'interpretation': 'The preposition \"of\", often in the context of introducing possessive or determiner phrases, and collective nouns.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 10605, 'analysis': '### Step-by-Step Walkthrough ###\\n\\n#### Part 1: Tokens that the neuron activates highly on in text ####\\n\\n**Example 1:** a rich, full life where she volunteers <<at>> a suicide hotline, sleeps  \\n**Example 2:** have no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above. Does your approach and set list differ <<at>> indoor  \\n**Example 3:** currently classified <<by>> the Drug Enforcement Administration as  \\n**Example 4:** supposed to be against war, but where are they now with Obama? � � The channel is the most recent project for a busy post-<<ret>>irement Paul who recently launched  \\n**Example 5:** other? She crept from your side <<at>> the chiming of four And is down in the parlor <<at>> work on another. Your\\n\\n**ACTIVATING TOKENS:** \"at\", \"by\", \"ret\"\\n**PREVIOUS TOKENS:** No interesting patterns.\\n\\n**Step 1:**  \\n- The activating tokens include \"at\", \"by\", \"ret\".\\n\\n**Step 2:**  \\n- The examples contain prepositions (\"at\", \"by\").\\n- The activating tokens appear to be function words or parts of words which precede nouns or actions, indicating positions or states of being.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, the activating tokens seem to be used in contexts where they indicate location, time, or role.\\n\\n#### Part 2: Tokens that the neuron boosts in the next token prediction ####\\n\\n**Top_logits:** \"least\", \"onement\", \"behest\", \"times\", \"yp\", \"noon\", \"halftime\", \"stake\", \"rium\", \"dusk\"  \\n**Bottom_logits:** \"ratulations\", \"FTWARE\", \"gans\", \"xual\", \"HTTP\", \"selves\", \"idav\", \"ptive\", \"alogue\", \"sylv\"\\n\\n**SIMILAR TOKENS:**  \\n- \"least\", \"times\", \"noon\", \"dusk\".\\n\\nThe top logits list contains words related to time and role locations or states (\"least\", \"noon\", \"dusk\", \"times\").\\n\\nCombining the patterns from Part 1 and the top logits list:\\n- The neuron activates on prepositions and parts of words that position actions or entities in time, space, or roles.\\n- The top logits list supports this, showing words often related to times of the day or states/positions.\\n\\n#### Final Explanation ####\\n\\n[EXPLANATION]: The neuron activates on prepositions and word parts indicative of location, time, or roles, subsequently predicting tokens related to time or states/positions.', 'interpretation': 'The neuron activates on prepositions and word parts indicative of location, time, or roles, subsequently predicting tokens related to time or states/positions.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 10837, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full <<life>> where she volunteers at a (0.8)\\n\\nExample 2: it’s a bit of a ‘duty’ to let some hidden talents come through, and do it without too much bureaucracy and hard <<times>> they might have encountered otherwise (0.75)\\n\\nExample 3: on reforming laws that punish drug users, noting the racial disparity in drug arrests. \"We should not be locking up kids or individual users for <<long>> stretches of jail time when (0.83)\\n\\nExample 4: to be against war, but where are they now with Obama? The channel is the most recent project for a busy post-ret <<irement>> Paul who recently launched The (0.77)\\n\\nExample 5: four And is down in the parlor at work on another. Your breakfasts are spoiled, And your dinners half-boiled, And your <<efforts>> to get a square supper (0.73)\\n\\nStep 1.\\nACTIVATING TOKENS: \"life\", \"times\", \"long\", \"retirement\", \"efforts\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The activating tokens seem to relate to durations or significant phases/activities in life.\\n- The examples depict moments of reflection, narrative or description involving life\\'s stages or significant efforts.\\n- The activating tokens are often nouns that can be associated with temporal or life-related concepts.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"ago\", \"long\", \"elapsed\", \"overdue\", \"longer\", \"lasting\".\\n- The top logits suggest a focus on durations and the passage of time.\\n\\n[EXPLANATION]: Nouns that relate to temporal durations or significant phases of life.', 'interpretation': 'Nouns that relate to temporal durations or significant phases of life.', 'f1_score': 0.5833333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 10974, 'analysis': '### Step-by-Step Walkthrough\\n\\n#### Part 1: Tokens that the neuron activates highly on in text\\n\\n**Example 1:** \\n- **Rich Context:** Describes various contexts around the term <<trans>>, including roles played by trans actresses and historical events affecting trans individuals.\\n\\n**Example 2:** \\n- **Term:** <<heavily>> – No significant context in this solitary example given alone.\\n\\n**Example 3:** \\n- **Recurrent Phrase:** <<of>> appears multiple times in different contexts discussing legal and societal issues surrounding drug use.\\n\\n**Example 4:**\\n- **Term:** <<lot>> – Included in a disclaimer and contextual details about following someone on a social media platform.\\n\\n**Example 5:**\\n- **Term:** <<half>> – In a narrative context about food and mealtime disruptions.\\n\\n**ACTIVATING TOKENS (Step 1):**\\n1. <<trans>> (repeated in Example 1)\\n2. <<heavily>>\\n3. <<of>> (repeated in Example 3)\\n4. <<lot>>\\n5. <<half>>\\n\\n**Shared Features of Text Examples (Step 2):**\\n- The examples mostly feature commonly used words in everyday language.\\n- Several of the examples contain words discussing quantities or portions (e.g., \"a lot\", \"half\").\\n- At least one example deals with social and identity-related themes (<<trans>> in Example 1).\\n- The neuron appears to be activated by frequently-used functional words (e.g., <<of>>) and words related to quantitative measures.\\n\\n#### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:**\\n- \"apiece\", \"total\", \"per\", \"annually\", \"+.\", \"+\", \"bps\", \"(~\", \"sq\", \"+,\"\\n\\n**Analysis:** \\n- The tokens promoted by the neuron suggest a focus on quantification, metrics, and measurement.\\n- Words like \"apiece,\" \"total,\" \"per,\" \"annually,\" \"bps,\" and symbols like \"+\", \"+.\", \"(~\", and \"sq\" all relate to quantities, measurements, or statistical references.\\n\\n**SIMILAR TOKENS:**\\n- These logits show high relevance to mathematical or quantifiable terms, improving the odds of words or symbols related to quantities or statistics appearing next.\\n\\n**CONCLUSION:**\\nCombining the two parts, it appears that the neuron\\'s activation is closely related to the context of words dealing with quantities, parts, and measures. Both functional words and terms indicating quantity or portions activate this neuron, and it promotes similar quantitative expressions.\\n\\n[EXPLANATION]: The neuron activates on frequent, common functional words or words related to quantities and measurements, promoting the prediction of quantifiable or statistical terms in text.', 'interpretation': 'The neuron activates on frequent, common functional words or words related to quantities and measurements, promoting the prediction of quantifiable or statistical terms in text.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 11209, 'analysis': '### Analysis Steps\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\n#### Step 1: List a couple activating and contextual tokens you find interesting.\\n- ACTIVATING TOKENS: \"feld\", \"atic\", \"1\", \"D\", \"ilt\".\\n- CONTEXTUAL TOKENS: No clear previous token patterns.\\n\\n#### Step 2: Write down general shared features of the text examples.\\n- The tokens all seem to be parts of words or full tokens in a context that appears to have formal, technical, or specialized language.\\n- \"feld\" and \"atic\" could relate to specific fields or terminologies.\\n- \"1\" and \"D\" are characters that might be relevant in enumerations or specializations.\\n- \"ilt\" appears in a context that rhymes with \"quilt\", suggesting a unique token form.\\n\\nLet me look again for patterns in the examples. Are there any links or hidden linguistic commonalities that I missed?\\n- The tokens might be linked to both formal contexts and unique language sequences.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\n#### SIMILAR TOKENS: \"™\", \"®\", \" Productions\", \"®,\", \" aka\", \" (\"\", \" Syndrome\", \" II\", \" LLP\", \" ®\"\\n- The top logits list contains registered symbols (™ and ®), company names (\" Productions\", \" LLP\"), alternate names (\" aka\"), label components for syndromes and enumerations (\" (\"\", \" Syndrome\", \" II\").\\n\\n#### Refine your hypotheses from part 1 in light of the new data\\n- The activating tokens occur in contexts that anticipate formal or enumerated designations, such as trademarked items, company names, registered labels, and enumerations (e.g., \"II\").\\n\\n[EXPLANATION]: Tokens that occur in contexts leading to formal designations, specialized terminologies, or enumerations.', 'interpretation': 'Tokens that occur in contexts leading to formal designations, specialized terminologies, or enumerations.', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 11234, 'analysis': '### Part 1: Tokens that the neuron activates highly on in text\\n\\n#### Step 1: List a couple activating and contextual tokens you find interesting.\\nACTIVATING TOKENS: \"volunteers\", \"business\", \"time\", \"Paul\", \"dinners\"\\nPREVIOUS TOKENS: No interesting patterns\\n\\n#### Step 2: Write down general shared features of the text examples.\\n- The activating tokens tend to be nouns.\\n- In several examples, activating tokens relate to activities or occupations (e.g., \"volunteers\", \"business\").\\n- The neuron also activates on personal names (e.g., \"Paul\") and common activities or time indicators (e.g., \"time\", \"dinners\").\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n#### Analyze the Top_logits\\nSIMILAR TOKENS: \"cash\", \"dollars\", \"money\", \"reimburse\", \"refund\", \"rebate\", \"payable\"\\n- The top logits list contains words associated with financial transactions and monetary terms.\\n\\n### Explanation:\\n\\nLooking at both the activating tokens from Part 1 and the tokens boosted in the next token prediction from Part 2, we see that when the neuron activates, it boosts terms related to money and financial transactions. The activating tokens often involve jobs, activities, or other contexts where financial aspects might be involved or discussed.\\n\\n[EXPLANATION]: Tokens related to occupations, activities, people\\'s names, or temporal indicators that can contextually link to financial transactions or monetary considerations.', 'interpretation': \"Tokens related to occupations, activities, people's names, or temporal indicators that can contextually link to financial transactions or monetary considerations.\", 'f1_score': 0.45054945054945056}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Found input variables with inconsistent numbers of samples: [10, 11]\n",
      "{'feature_index': 11743, 'analysis': '### Part 1: Analyzing Activating Tokens\\n\\n#### Step 1: List a couple activating and contextual tokens\\n**Activating Tokens:**\\n1. \"trans\"\\n2. \"psychedelic\"\\n3. \"racial\"\\n4. \"liberal\"\\n5. \"crazy\"\\n\\n#### Step 2: Write down general shared features of the text examples\\n- The activating tokens are all adjectives or descriptors.\\n- They tend to describe identity (e.g., \"trans\"), cultural or social topics (e.g., \"liberal\", \"racial\"), or unconventional states (e.g., \"psychedelic\", \"crazy\").\\n- The tokens often appear in contexts discussing social issues, identity, or genres.\\n\\n### Part 2: Analyzing Tokens Boosted for Prediction\\n\\n#### Analyzing Top_logits\\n**Top_logits:**\\n- \"extremists\"\\n- \"sects\"\\n- \"groups\"\\n- \"extremist\"\\n- \"extremism\"\\n- \"ideology\"\\n- \"militias\"\\n- \"activists\"\\n- \"supremacist\"\\n- \"factions\"\\n\\n#### Finding Commonalities\\n- The top logits list heavily features terms associated with groups, ideologies, and polarized social or political topics.\\n- These terms suggest a focus on collective identities and potentially controversial or radical ideologies.\\n\\n### Final Explanation\\n\\nCombining patterns from Part 1 and Part 2:\\n- The activating tokens largely describe distinctive identities, unconventional states, or social/cultural attributes. \\n- The neuron seems to detect language related to identity, social and political categorization, and ideological distinctions, often in contexts that may imply controversy or extremity.\\n\\n[EXPLANATION]: Descriptive adjectives and identifiers about social, cultural, and ideological identities, often within contexts that hint at controversy or extremity.', 'interpretation': 'Descriptive adjectives and identifiers about social, cultural, and ideological identities, often within contexts that hint at controversy or extremity.', 'f1_score': 0.4949494949494949}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 11932, 'analysis': '### Part 1: Tokens that the neuron activates highly on in text\\n\\n**Step 1:**\\nACTIVATING TOKENS: \"when\", \"Does\", \"when\", \"where\", \"which\".\\n\\n**Step 2:**\\nGeneral shared features of the text examples:\\n- The activating tokens are questioning or relative pronouns (\"when\", \"where\", \"which\") and auxiliary verbs (\"Does\").\\n- The patterns suggest these tokens often occur in the context of introducing subordinate or dependent clauses.\\n- The text examples often involve complex sentence structures.\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:**\\n- All of the boosted tokens are pronouns or determiners: \"they\", \"you\", \"we\", \"he\", \"she\", \"someone\", \"somebody\", \"it\", \"anyone\".\\n\\n**Refine the hypotheses:**\\n- The neuron boosts tokens that are commonly used as subjects or objects in sentences.\\n- This indicates the neuron\\'s role might be related to contexts where subsequent words are expected to be pronouns or determiners, often following question words, conjunctions, or relative pronouns.\\n\\n**Final Explanation:**\\n[EXPLANATION]: The neuron activates on questioning or relative pronouns and auxiliary verbs that introduce subordinate or dependent clauses, anticipating contexts where the next word is likely to be a pronoun or determiner.', 'interpretation': 'The neuron activates on questioning or relative pronouns and auxiliary verbs that introduce subordinate or dependent clauses, anticipating contexts where the next word is likely to be a pronoun or determiner.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "name 'six' is not defined\n",
      "{'feature_index': 12186, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"wishes\", \"some\", \"stretches\", \"part\", \"number\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens don\\'t show an immediately apparent pattern.\\n\\nStep 2.\\n- The examples show activating words in different contexts involving various nouns and abstract quantities.\\n- The text typically deals with advisory, descriptive, or conversational content.\\n\\nNo additional linguistic commonalities are visible here.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"of\", \"thereof\", \"Of\", \" OF\".\\n- The top logits list is dominated by different forms of the word \"of\".\\n\\nDespite the lack of an apparent common theme among the activating tokens, the strengthened prediction of \"of\" in varying forms suggests the neuron is activated by contexts leading into genitive or partitive constructions. These usually involve quantities, parts, or relationships that naturally lead into phrases starting with \"of\".\\n\\n[EXPLANATION]: Tokens in contexts leading into genitive or partitive constructions, which often result in the prediction of \"of\".', 'interpretation': 'Tokens in contexts leading into genitive or partitive constructions, which often result in the prediction of \"of\".', 'f1_score': 0.8}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 12318, 'analysis': '### Step-by-Step Walkthrough for Our Neuron\\n\\n#### (Part 1) Tokens that the neuron activates highly on in text:\\n\\n**Example 1:**\\n`itt el , played by trans actress Har i N ef   G itt el is an ancestor of the P fe ff erm ans who lived in the <<famous>> H irsch feld Institute in` (activation value)\\n\\n**Example 2:**\\n`music . That � � s what it is . Also I feel that it � � s a bit of a � � duty � � to let some <<hidden>> talents come through , and` (activation value)\\n\\n**Example 3:**\\n`by the Drug Enforcement Administration as a Schedule 1 substance , which the DEA considers \" the most dangerous class of drugs with a high potential for abuse and potentially <<severe>> psychological and / or physical` (activation value)\\n\\n**Example 4:**\\n`to be against war , but where are they now with Obama ? � �   The channel is the most recent project for a busy post - ret <<irement>> Paul who recently launched The` (activation value)\\n\\n**Example 5:**\\n`qu ilt man ia that fi end ishly r aves , And to which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the <<current>> study we analyse crazy qu` (activation value)\\n\\n---\\n\\n**Step 1:** Identifying interesting activating tokens and looking for patterns.\\n- ACTIVATING TOKENS: \"famous\", \"hidden\", \"severe\", \"retirement\", \"current\".\\n- PREVIOUS TOKENS: No particular pattern observed.\\n\\n**Step 2:** General shared features of the text examples.\\n- The activating tokens seem to be adjectives or descriptive terms.\\n- These terms are often used to describe something with clear importance or intensity.\\n\\n---\\n\\n#### (Part 2) Tokens that the neuron boosts in the next token prediction:\\n\\nTop_logits: [\"arrangement\", \"endeavor\", \"version\", \"contribution\", \"sized\", \"solution\", \"basis\", \"entity\", \"role\", \"approach\"]\\nBottom_logits: [\"ensibly\", \"ometers\", \"ammers\", \"utics\", \"another\", \"aido\", \"predictably\", \"hops\", \"uden\", \"ernels\"]\\n\\n**Step 3:** Analyzing the similarity in top logits.\\n- SIMILAR TOKENS: \"arrangement\", \"endeavor\", \"version\", \"contribution\", \"role\", \"solution\".\\n- These tokens are nouns that often summarize or encapsulate important concepts, roles, or states.\\n\\n**Step 4:** Synthesizing final explanation based on Part 1 and Part 2:\\n- The activating tokens in Part 1 are descriptive adjectives.\\n- The top logits in Part 2 are nouns suggesting states, concepts, or roles.\\n\\n---\\n\\n**Final Explanation:**\\n\\n[EXPLANATION]: Adjectives highlighting importance or intensity, often leading to nouns summarizing states, concepts, or roles.', 'interpretation': 'Adjectives highlighting importance or intensity, often leading to nouns summarizing states, concepts, or roles.', 'f1_score': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 12325, 'analysis': '## Step-by-Step Walkthrough:\\n\\n### Part 1: Tokens that the neuron activates highly on in text\\n\\n#### Step 1: List a couple activating and contextual tokens you find interesting. Search for patterns in these tokens. \\n\\nACTIVATING TOKENS: \"out\", \"to\", \"we\", \"prosperity\".\\nPREVIOUS TOKENS: Trans actress references, mentions of reform, groups, and communal activities.\\n\\n#### Step 2: Write down general shared features of the text examples.\\n\\n- The activating tokens often appear in contexts relating to social actions or reforms.\\n- There are references to communal or social contexts, including support groups, duties, reforms, and peace/prosperity initiatives.\\n- Some examples reference taking actions or discussing changes within a communal or social context.\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n#### Observations from Top_logits\\n\\nSIMILAR TOKENS: \" ourselves\", \" collectively\", \" discuss\", \" asses\".\\n- The top logits list contains words that are related to communal activities, shared responsibility, or group discussions.\\n- Words like \"ourselves\" and \"collectively\" indicate joint or group actions, discussions, or assessments.\\n- There is a notable absence of words indicating solitude or individual actions.\\n\\n#### Final Explanation\\n\\nThe neuron seems to focus on social reforms, communal activities, or group dynamics. The activating tokens appear in contexts referring to reformative actions or collective endeavors, and the neuron boosts tokens that continue this communal or collective narrative.\\n\\n[EXPLANATION]: Tokens that appear in contexts related to joint, social, or communal activities and reforms.', 'interpretation': 'Tokens that appear in contexts related to joint, social, or communal activities and reforms.', 'f1_score': 0.5833333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 12515, 'analysis': '### Step-by-Step Walkthrough ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\n**Example 1:** G itt el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and transitioned against the <<wishes>> of her mother . G\\n\\n**Example 2:** I feel that it � � s a bit of a � � duty � � to let some hidden talents come through , and do it without too much <<bureaucracy>> and hard times they might\\n\\n**Example 3:** dependence .\" Other Schedule 1 drugs include heroin , ecstasy and LSD . Obama said his focus on reforming laws that punish drug users , noting the racial <<disparity>> in drug arrests . \\n\\n**Example 4:** on Twitter Spring Boot : REST + TD D from scratch B run o Kre bs Bl ocked Un block Follow Following Mar 2 , 2016 <<Disclaimer>> : Since there is a\\n\\n**Example 5:** we analyse crazy qu ilts using spatial statistics , comparing them to ` normal � � regular qu ilts . Qu ilts in general are subject to a number of <<constraints>> that would be difficult to\\n\\nStep 1:\\n**ACTIVATING TOKENS:** \"wishes\", \"bureaucracy\", \"disparity\", \"disclaimer\", \"constraints\".\\n\\n**PREVIOUS TOKENS:** No obvious patterns in previous tokens.\\n\\n### Analysis:\\n- The activating tokens are mostly abstract nouns representing concepts or conditions (\"wishes,\" \"bureaucracy,\" \"disparity,\" \"disclaimer,\" \"constraints\").\\n- There is no clear pattern in the previous tokens.\\n\\nStep 2:\\n**Shared Features:**\\n- The examples all contain nouns representing abstract concepts or conditions.\\n- The activating tokens often relate to rules, conditions, or notable distinctions in various contexts (social, legal, personal, etc.).\\n\\n### Further Consideration:\\n- There do not seem to be any additional linguistic or structural features beyond the abstract nature of the activating tokens.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"lessness\", \"fulness\", \" regarding\", \" surrounding\", \" stemming\", \" inherent\", \" arising\", \"iness\", \"ful\", \" towards\"]\\n\\n### Analysis of Logits:\\n- The promoted tokens include suffixes that turn words into abstract nouns or adjectives (\"lessness,\" \"fulness\", \"iness,\" \"ful\").\\n- Other tokens relate to contextual or relational words that specify origins or states (\"regarding,\" \"surrounding,\" \"stemming,\" \"inherent,\" \"arising,\" \"towards\").\\n\\n### Conclusion:\\n- The neuron is likely activated by abstract nouns representing conditions or states, and it boosts continuations that further specify or elaborate on these conditions or states.\\n\\n**[EXPLANATION]: The neuron activates on abstract nouns representing conditions or states, and it promotes continuations that further elaborate or contextualize these conditions or states.**', 'interpretation': 'The neuron activates on abstract nouns representing conditions or states, and it promotes continuations that further elaborate or contextualize these conditions or states.**', 'f1_score': 0.6}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 12691, 'analysis': '(Part 1) Analyzing Activating Tokens\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps <<with>> sexy Marines and hangs out <<with>> her trans friends .\\nExample 2: s not heavily genre bound . It ’ ’ s just good , electronic <<,>> psychedelic dance music . That\\nExample 3: Administration as a Schedule 1 substance , which the DEA considers \" the most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / <<or>> physical dependence .\" Other Schedule Obama said his focus on reforming laws that punish drug users , noting the racial disparity in drug arrests . \" We should not be locking up kids <<or>> individual users for long stretches\\nExample 4: is the most recent project for a busy post - ret irement Paul who recently launched The Ron Paul Institute , a new think - tank \" \\' for peace <<and>> prosperity .\\nExample 5: crept from your side at the chim ing of four And is down in the par lor at work on another . Your break f asts are spoiled , And <<your>> dinners half - bo iled , And <<your>> efforts to get a square\\n\\nStep 1:\\nACTIVATING TOKENS: \"with\", \",\", \"or\", \"and\", \"your\"\\nPREVIOUS TOKENS: No immediate patterns discernible.\\n\\nStep 2:\\n- The examples contain common conjunctions and prepositions such as \"with\", \"or\", \"and\", as well as punctuation marks like \",\";\\n- These tokens serve to connect clauses, phrases, or words.\\n- The presence indicates standard sentence structuring elements or possession indicators (\"your\").\\n\\nPART 2: Analyzing Top_logits\\n\\nTop_logits: [\"etc\", \" etc\", \" assorted\", \" other\", \" Flavoring\", \" others\", \" whatever\", \" Miscellaneous\", \" elsewhere\", \" occasional\"]\\nBottom_logits: [\"Based\", \"IGHTS\", \"actively\", \"orthy\", \"uid\", \"said\", \"igned\", \"rave\", \"((\", \"emen\"]\\n\\n- Top logits include terms that may serve to list, group, generalize, or append information.\\n- These terms suggest enumeration or inclusion of additional, often unspecified, items or categories.\\n\\n[EXPLANATION]: This neuron activates on common conjunctions, prepositions, punctuation marks, and possessives within a sentence, suggesting it plays a role in handling the structuring and listing of information.', 'interpretation': 'This neuron activates on common conjunctions, prepositions, punctuation marks, and possessives within a sentence, suggesting it plays a role in handling the structuring and listing of information.', 'f1_score': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 12771, 'analysis': '### STEP-BY-STEP WALKTHROUGH\\n\\n#### Part 1: Tokens that the neuron activates highly on in text\\n\\n**Example 1**: a rich, full life where she volunteers at a suicide hotline, sleeps with sexy Marines and hangs out with her trans friends.   <<E>>leanor, played by trans actress Zackary Drucker   <<E>>leanor leads the support group\\n\\n**Example 2**: I have no sufficient reasoning for doing so apart from the \"karmatic\" one explained above.   Does your approach and set list <<differ>> at indoor\\n\\n**Example 3**: the folks who are writing those laws have probably done the same thing,\" he said. In August, the Obama administration announced it would not stop <<Washington>> and\\n\\n**Example 4**: the first part of August. Follow Gabe on Twitter Spring Boot: REST + TDD from scratch   Bruno Krebs Blocked <<Un>> block Follow Following Mar 2\\n\\n**Example 5**: in the parlor at work on another. Your breakfasts are spoiled, And your dinners half-boiled, And your efforts to get a <<square>> supper are foiled By\\n\\n**ACTIVATING TOKENS**: \"E\", \"differ\", \"Washington\", \"Un\", \"square\"\\n\\n**PREVIOUS TOKENS**: \"Elean\", \"set list\", \"announced\", \"Bruno Krebs\", \"efforts to get a\"\\n\\n### Step 1: Analyze Activating Tokens and Previous Tokens\\n- Several activating tokens are fragmented with the context suggesting an individualized entity or proper noun.\\n- Many tokens involve names or parts of names (e.g., \"Elean\", \"Washington\").\\n\\n### Step 2: Shared Features of Text Examples\\n- The examples often include proper nouns or fragments that are capitalized, signifying some significant or naming entity.\\n- Contextual tokens frequently involve introductions of names, titles, or changes in subjects.\\n- Prior tokens frame the scene or introduce entities.\\n\\n### Part 2: Tokens the neuron boosts in the next token prediction\\n\\n**Top_logits**: [\"States\", \"Nations\", \"STATES\", \"Airlines\", \"Soccer\", \"States\", \"zbek\", \"lyss\", \"Centers\", \"prising\"]\\n\\n### Analyze the Top_logits\\n- Many boost tokens are related to proper nouns, especially those involving organizations, locations, or titles (e.g., \"States\", \"Nations\").\\n- These tokens frequently capitalize on beginnings of words or names, suggesting some connection to entities or titles.\\n- Patterns suggest associations with significant or identifiable entities (e.g., countries, companies).\\n\\n### Refine Hypothesis\\nCombining insights from both parts:\\n- The neuron likely activates on tokens that are parts of proper names or significant titles.\\n- The next-token predictions further suggest the neuron has a role in completing or anticipating the beginning of proper nouns or important organizational entities.\\n\\n**[EXPLANATION]: Tokens that are fragments of proper nouns or significant titles, suggesting a recognizable entity or name.**', 'interpretation': 'Tokens that are fragments of proper nouns or significant titles, suggesting a recognizable entity or name.**', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 12815, 'analysis': '### Part 1: Tokens that the neuron activates highly on in text\\n\\n#### Step 1. List a couple activating and contextual tokens you find interesting. Search for patterns in these tokens, if there are any.\\n- ACTIVATING TOKENS: \"Nazis\", \"Also\", \"and\", \"war\", \"is\".\\n\\n#### Step 2. Write down general shared features of the text examples.\\n- The activating tokens include a mix of conjunctions (\"and\", \"also\") and nouns (\"Nazis\", \"war\").\\n- Both \"Nazis\" and \"war\" are related to conflict or global historical events.\\n\\n- The contexts of the activation examples involve historical references, political statements, and transitioning/connecting clauses:\\n\\n1. Reference to \"Nazis\" in the context of persecution during WWII.\\n2. Transitional phrase \"Also\" in a discussion.\\n3. Usage of \"and\" to connect points in a statement about drugs and laws.\\n4. Reference to \"war\" in a political statement.\\n5. The word \"is\" in a narrative description.\\n\\nThese observations suggest the neuron might respond to text involving historical events, conflicts, and transitional statements.\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n#### Patterns in Top_logits:\\n- The promoted tokens are related to international or organizational entities and conflicts: \" NATO\", \"NATO\", \"abroad\", \"Albania\", \"UNESCO\", \"Iran\", \"Syria\", \"Organisation\", \"Islamist\", \"imperialist\".\\n- These tokens generally pertain to global institutions, geopolitical contexts, or international conflicts.\\n\\n### Conclusion\\n\\nThe neuron appears to react to tokens associated with historical or geopolitical events, conflicts, and transitional linkage within narrative or descriptive contexts. This is reflected in both the activation patterns and the next-token predictions that emphasize terms related to international or organizational contexts, as well as conflict-related topics.\\n\\n[EXPLANATION]: Tokens related to historical or geopolitical events, conflicts, and narrative or transitional discourse contexts.', 'interpretation': 'Tokens related to historical or geopolitical events, conflicts, and narrative or transitional discourse contexts.', 'f1_score': 0.29292929292929293}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 13095, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy <<Marines>> and hangs out with her\\n\\nExample 2: s not heavily genre bound . It � � s just good , electronic , <<psychedelic>> dance music . That �\\n\\nExample 3: currently classified by the Drug Enforcement Administration as a <<Schedule>> 1 substance , which the DEA considers \" the most dangerous class of drugs with a high potential for abuse and potentially severe psychological and / or physical dependence .\" Other <<Schedule>> 1 drugs include heroin ,\\n\\nExample 4: to be a liberal idea . You know , progressives are supposed to be against war , but where are they now with <<Obama>> ? � �\\n\\nExample 5: other ? She crept from your <<side>> at the chim ing of\\n\\nStep 1.\\nACTIVATING TOKENS: \"Marines\", \"psychedelic\", \"Schedule\", \"Obama\", \"side\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The examples involve various context-rich nouns.\\n  \\nThe examples include:\\n1) Names of people or organizations (\"Marines\", \"Obama\"),\\n2) Specific terms or classifications (\"psychedelic\", \"Schedule\"),\\n3) Spatial words (\"side\").\\n\\n---\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"scam\", \"\\xa0\", \"¶\", \" \\u200e\", \"Naruto\", \"›\", \"OnePlus\", \"Belfast\", \"\\xad\", \"natureconservancy\"]\\n\\nStep 1.\\nSIMILAR TOKENS: \"scam\", \"Naruto\", \"OnePlus\", \"Belfast\", \"natureconservancy\".\\n- The logits list contains a range of proper nouns (e.g., \"Naruto\", \"OnePlus\", \"Belfast\", \"natureconservancy\").\\n\\n[EXPLANATION]: The neuron activates on specific and contextually rich nouns, including names of people, organizations, proper nouns, and specific terms.', 'interpretation': 'The neuron activates on specific and contextually rich nouns, including names of people, organizations, proper nouns, and specific terms.', 'f1_score': 0.45054945054945056}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 13100, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"volunteers\", \"trying\", \"considers\", \"supposed\", \"difficult\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are all verbs.\\n- The previous tokens do not show any clear patterns.\\n\\nStep 2.\\n- The examples contain verbs in various contexts.\\n- The tokens are closely related to describing actions or considerations.\\n- The activating tokens are verbs related to actions or evaluations.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The sentences involving the activating tokens are complex and often describe duties, decisions, or suppositions.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"to\", \"thereto\", \"toward\", \"towards\".\\n- The top logits list includes variations of the word \"to\", which is often used with verbs to form infinitives or to indicate direction or purpose (\"towards\", \"thereto\").\\n\\n[EXPLANATION]: Verbs related to actions or evaluations within complex sentences, often leading into infinitive constructions or indicating direction/purpose.', 'interpretation': 'Verbs related to actions or evaluations within complex sentences, often leading into infinitive constructions or indicating direction/purpose.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 13111, 'analysis': '(Part 1)\\nACTIVATING TOKENS: \"Maur\", \"arm\", \"or\", \"ret\", \"chim\".\\n\\nStep 1.\\n- The activating tokens do not form a clear, thematic pattern. They are short segments that do not convey distinct meaning independently.\\n- The previous tokens also do not exhibit a clear pattern. They appear in various contexts with no obvious shared features.\\n\\nStep 2.\\n- The specific tokens triggering the neuron are varied and appear arbitrary in isolation.\\n- Contextual clues do not suggest any distinct semantic or syntactic similarity.\\n- The activating tokens are small parts of longer words or names, which may suggest affix or morpheme detection.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No additional clear linkages or commonalities that notably stand out between the examples.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"os\", \"ag\", \"eless\", \"il\", \"ilit\", \"ar\", \"og\", \"ol\", \"oc\", \"at\".\\n- The top logits list consists of fragments typically found within or at the end of words.\\n- These fragments indicate the neuron\\'s possible role in identifying or predicting parts of words, such as suffixes, or potentially even towards morphological patterns within words.\\n\\n[EXPLANATION]: This neuron appears to activate on parts of words, potentially recognizing morphological patterns or affixes within tokens.', 'interpretation': 'This neuron appears to activate on parts of words, potentially recognizing morphological patterns or affixes within tokens.', 'f1_score': 0.6703296703296704}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 13120, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \",\", \",\", \"1 substance <<,>>\", \",\", \",\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 1.\\n- The activating tokens are all commas (\",\").\\n- The previous tokens do not present a clear pattern.\\n\\nStep 2.\\n- The neuron activates on commas in a variety of contexts.\\n- The activating tokens are located in sentences that provide additional information or clarification.\\n- The text examples feature complex sentence structures where commas are used to separate clauses or phrases.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, the activating commas often indicate a brief pause in thought or add supplementary information.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"albeit\", \"namely\", \"although\", \"which\", \"though\", \"but\", \"however\", \"including\", \"whereas\", \"wherein\".\\n- The top logits list consists of conjunctions and transition words, which commonly appear after a comma in complex sentences.\\n\\n[EXPLANATION]: Commas used in complex sentences to separate clauses or provide additional context or clarification.', 'interpretation': 'Commas used in complex sentences to separate clauses or provide additional context or clarification.', 'f1_score': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 13272, 'analysis': '### ANALYSIS OF THE NEURON\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: <<a>> rich, full life where she volunteers at <<a>> suicide hotline, sleeps with and transitioned against the wishes of her mother. Gittel was arrested by the Nazis for being trans and eventually died in the Holocaust. May <<a>>\\n\\nExample 2: s <<not>> heavily genre bound. It\\n\\nExample 3: <<currently>> classified by the Drug Enforcement\\n\\nExample 4: <<to>> be a liberal idea.\\n\\nExample 5: <<other>> ? She crept from your\\n\\n\\n**Step 1: ACTIVATING TOKENS and PREVIOUS TOKENS**\\n\\n- The activating tokens are \"a\", \"not\", \"currently\", \"to\", and \"other\".\\n- The previous tokens do not display a strong or clear pattern.\\n\\n**Step 2: GENERAL SHARED FEATURES**\\n\\n- The activating tokens are common words that perform grammatical functions: articles, adverbs, conjunctions, prepositions, pronouns.\\n- The sentences they occur in are generally straightforward and informational.\\n\\n**Let me check for any other patterns:**\\n- The activating tokens do not seem to have any strong semantic connection or co-occurrence patterns.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"actionDate\", \" ][\", \" ):\", \" ]\", \"malink\", \" ·\", \"zbollah\", \" )]\", \" ::\", \" largeDownload\"]\\nBottom_logits: [\" Vaugh\", \" Seym\", \" Instr\", \" Azerb\", \"ーテ\", \" Hub\", \" frontline\", \" McKay\", \" Gaw\", \" advoc\"]\\n\\n\\n**Step 2: SIMILAR TOKENS**\\n\\n- The top logits list contains tokens that resemble formatting symbols, code-related terms, or uncommon large words.\\n- There is no clear semantic connection to the activating tokens.\\n\\n**Final Analysis:**\\n\\nThe neuron seems to activate on very common grammatical words in a sentence (\"a\", \"not\", \"currently\", \"to\", \"other\"), which function as articles, adverbs, conjunctions, prepositions, and pronouns. The promoted logits suggest a potential emphasis on document structuring or token sequencing, possibly linked to formatting or technical text contexts like data schemas or specialized notations.\\n\\n[EXPLANATION]: The neuron activates on common grammatical words, possibly indicating a focus on structural or formatting elements in text.', 'interpretation': 'The neuron activates on common grammatical words, possibly indicating a focus on structural or formatting elements in text.', 'f1_score': 0.5238095238095238}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 13314, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nACTIVATING TOKENS: \"transitioned\", \"explained\", \"said\", \"launched\", \"of (two instances)\".\\n\\nStep 1.\\n- The activating tokens are mostly verbs in past tense (\"transitioned\", \"explained\", \"said\", \"launched\").\\n- The token \"of\" is less informative, but it appears twice.\\n\\nStep 2.\\n- The text examples often contain verbs in past tense within or following descriptive contexts.\\n- Some of the examples include reported speech or narration (\"said\", \"explained\").\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- There may be a focus on verbs related to communication or significant actions in a narrative context.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"sarcast\", \"bluntly\", \"quoted\", \"rhet\", \"angrily\", \"remarks\", \"incred\", \"referring\", \"adding\", \"said\".\\n\\n- The top logits list suggests a focus on words representing speech or ways of communicating emotions or statements, such as \"sarcast\", \"quoted\", \"angrily\", \"remarks\", \"referring\", \"adding\", \"said\".\\n\\n[EXPLANATION]: Verbs, especially in past tense, often related to communication or significant actions in a narrative context.', 'interpretation': 'Verbs, especially in past tense, often related to communication or significant actions in a narrative context.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 13561, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs <<out>> with her trans friends .\\n\\nExample 2: no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above . Does your approach and set list differ at <<indoor>>\\n\\nExample 3: of the folks who are writing those laws have probably done the same thing ,\" he said . In August , the Obama administration announced it would not <<stop>> Washington and\\n\\nExample 4: recently launched The Ron Paul Institute , a new think - tank � � for peace and prosperity . � � The Ron Paul Channel is set to <<launch>> officially in the first part\\n\\nExample 5: other ? She crept from your side at the chim ing of four And is down in the par lor at work on another . Your break f <<asts>> are spoiled , And your\\n\\nStep 1.\\nACTIVATING TOKENS: \"out\", \"indoor\", \"stop\", \"launch\", \"asts\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nThe activating tokens vary widely in context and usage. There are no immediate obvious textual patterns connecting them based purely on their intrinsic features.\\n\\nStep 2.\\n- The examples contain a mix of verbs (\"out\", \"stop\", \"launch\") and a location descriptor (\"indoor\").\\n- The neuron seems to activate on tokens related to actions or states of being (\"out\", \"stop\", \"launch\"), and perhaps some less clear cases (\"asts\").\\n\\n(Part 2)\\nSIMILAR TOKENS: \"festivities\", \"meetings\", \"celebrations\", \"Saturday\", \"ceremony\", \"celebration\", \"midday\", \"dinner\", \"Friday\", \"rehearsal\".\\n- The top logits list suggests a focus on terms related to events, gatherings, or specific times and dates (e.g., \"Saturday\", \"dinner\", \"rehearsal\").\\n\\n[EXPLANATION]: Tokens related to events, gatherings, and specific activities or states of being.', 'interpretation': 'Tokens related to events, gatherings, and specific activities or states of being.', 'f1_score': 0.6}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 13849, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \"she\", \"it\", \"she\", \"it\", \"there\", \"She\".\\nPREVIOUS TOKENS: No consistent patterns.\\n\\nStep 1.\\n- The activating tokens are pronouns.\\n- The previous tokens do not show consistent patterns.\\n\\nStep 2.\\n- In each example, the activating token is a pronoun (\"she\", \"it\", \"there\").\\n- The pronouns appear in contexts with varying subjects and objects, but often with preceding context describing or setting up the pronoun.\\n\\nUpon further reflection, I see no additional linguistic patterns or context-specific commonalities that I missed.\\n\\n(Part 2)\\nSIMILAR TOKENS: [\" seems\", \"chy\", \"\\'s\", \"\\'ll\", \" ain\", \" wasn\"].\\n- The top logits list contains words that often follow pronouns and are likely to be auxiliary verbs or contractions (e.g., \"\\'s\", \"\\'ll\", \" ain\", \" wasn\").\\n\\n[EXPLANATION]: Pronouns in text, often followed by auxiliary verbs or contractions.', 'interpretation': 'Pronouns in text, often followed by auxiliary verbs or contractions.', 'f1_score': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 14167, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nStep 1.\\nACTIVATING TOKENS: \"itt\", \"�\", \"Schedule\", \"Ron\", \"qu\".\\nCONTEXTUAL TOKENS: The neuron activates on parts of proper nouns and alphanumerical prefixes, as well as unusual characters in the text.\\n\\nStep 2.\\n- The activating tokens include parts of proper nouns (e.g., \"itt\" in \"Gittel\", \"Ron\" in \"Ron Paul\") and specific sections of longer words (e.g., \"Schedule\", \"qu\").\\n- The segments can also appear in fragmented or unusual character contexts (e.g., the special character \"�\").\\n- In some cases, the activation appears within technical or scientific terms (e.g., \"Schedule 1 drugs\").\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nSIMILAR TOKENS: \"rex\", \"enum\", \"ificate\", \"rower\", \"ipal\".\\n\\n- The top logits list contains word segments (prefixes, suffixes, infixes) suggesting the neuron helps in predicting parts of complex words.\\n\\n[EXPLANATION]: This neuron activates on fragments of proper nouns, alphanumerical prefixes, and special characters, aiding the model in predicting parts of complex words.', 'interpretation': 'This neuron activates on fragments of proper nouns, alphanumerical prefixes, and special characters, aiding the model in predicting parts of complex words.', 'f1_score': 0.5238095238095238}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 14237, 'analysis': '### STEP-BY-STEP WALKTHROUGH ###\\n\\n**(Part 1) Tokens that the neuron activates highly on in text**\\n\\n**Example 1:**\\n``` \\ni N ef   G itt el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and <<transitioned>> against the wishes of her\\n```\\n\\n**Example 2:**\\n```\\nI have no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above. Does your approach and set list <<differ>> at indoor\\n```\\n\\n**Example 3:**\\n```\\nfor abuse and potentially severe psychological and / or physical dependence.\" Other Schedule 1 drugs include heroin, ecstasy and LSD.   Obama said his focus on <<reforming>> laws that punish drug users\\n```\\n\\n**Example 4:**\\n```\\nin the first part of August. Follow Gabe on Twitter Spring Boot: REST + TD D from scratch B run o Kre bs Bl <<ocked>> un block Follow Following Mar\\n```\\n\\n**Example 5:**\\n```\\nother? She crept from your side at the chim ing of four And is <<down>> in the par lor at\\n```\\n\\n**Step 1:**\\nACTIVATING TOKENS: \"transitioned\", \"reforming\", \"differ\", \"ocked\", \"down\".\\n\\n**Features & Patterns:**\\n1. The tokens often involve a form of change or action.\\n2. They contain verbs that indicate a state of being or change.\\n3. \"differ\" and \"reforming\" are verbs suggesting a process or alteration.\\n4. The text contextually seems to deal with transformation or movement.\\n\\n**Step 2:**\\n- Activating tokens convey actions or transformations.\\n- Examples are mostly using verbs that suggest a change or motion.\\n\\n\\n**(Part 2) Tokens that the neuron boosts in the next token prediction**\\n\\n**Top_logits:**\\n```\\n[\" abruptly\", \" momentum\", \" morale\", \" limbs\", \" equilibrium\", \" drastically\", \" inhib\", \" prematurely\", \" badly\", \" dramatically\"]\\n```\\n\\n**Patterns:**\\n1. Adverbs like \"abruptly\", \"drastically\", \"prematurely\", \"badly\", \"dramatically\" describe the manner or intensity of an action.\\n2. Nouns like \"momentum\", \"morale\", \"equilibrium\", \"limbs\" are somewhat related to states or physical conditions.\\n3. \"inhib\" suggests the beginning of a word like \"inhibit\" which ties to restriction.\\n\\nCombining insights from Part 1 and Part 2:\\n\\n1. Activations are focused around verbs indicative of change or state.\\n2. Boosted logits indicate adverbs and nouns that characterize or modify these states.\\n\\n**[EXPLANATION]:**\\nThe neuron activates on verbs related to transformation or change and boosts tokens that describe the manner or intensity of these actions.', 'interpretation': '### STEP-BY-STEP WALKTHROUGH ###\\n\\n**(Part 1) Tokens that the neuron activates highly on in text**\\n\\n**Example 1:**\\n``` \\ni N ef   G itt el is an ancestor of the P fe ff erm ans who lived in the famous H irsch feld Institute in Berlin and <<transitioned>> against the wishes of her\\n```\\n\\n**Example 2:**\\n```\\nI have no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above. Does your approach and set list <<differ>> at indoor\\n```\\n\\n**Example 3:**\\n```\\nfor abuse and potentially severe psychological and / or physical dependence.\" Other Schedule 1 drugs include heroin, ecstasy and LSD.   Obama said his focus on <<reforming>> laws that punish drug users\\n```\\n\\n**Example 4:**\\n```\\nin the first part of August. Follow Gabe on Twitter Spring Boot: REST + TD D from scratch B run o Kre bs Bl <<ocked>> un block Follow Following Mar\\n```\\n\\n**Example 5:**\\n```\\nother? She crept from your side at the chim ing of four And is <<down>> in the par lor at\\n```\\n\\n**Step 1:**\\nACTIVATING TOKENS: \"transitioned\", \"reforming\", \"differ\", \"ocked\", \"down\".\\n\\n**Features & Patterns:**\\n1. The tokens often involve a form of change or action.\\n2. They contain verbs that indicate a state of being or change.\\n3. \"differ\" and \"reforming\" are verbs suggesting a process or alteration.\\n4. The text contextually seems to deal with transformation or movement.\\n\\n**Step 2:**\\n- Activating tokens convey actions or transformations.\\n- Examples are mostly using verbs that suggest a change or motion.\\n\\n\\n**(Part 2) Tokens that the neuron boosts in the next token prediction**\\n\\n**Top_logits:**\\n```\\n[\" abruptly\", \" momentum\", \" morale\", \" limbs\", \" equilibrium\", \" drastically\", \" inhib\", \" prematurely\", \" badly\", \" dramatically\"]\\n```\\n\\n**Patterns:**\\n1. Adverbs like \"abruptly\", \"drastically\", \"prematurely\", \"badly\", \"dramatically\" describe the manner or intensity of an action.\\n2. Nouns like \"momentum\", \"morale\", \"equilibrium\", \"limbs\" are somewhat related to states or physical conditions.\\n3. \"inhib\" suggests the beginning of a word like \"inhibit\" which ties to restriction.\\n\\nCombining insights from Part 1 and Part 2:\\n\\n1. Activations are focused around verbs indicative of change or state.\\n2. Boosted logits indicate adverbs and nouns that characterize or modify these states.\\n\\n**[EXPLANATION]:**\\nThe neuron activates on verbs related to transformation or change and boosts tokens that describe the manner or intensity of these actions.', 'f1_score': 0.8}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 14359, 'analysis': '### STEP-BY-STEP WALKTHROUGH ###\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich, full life where she volunteers at a suicide hotline, sleeps with sexy <<Marines>> and hangs out with her\\n\\nExample 2: That’s what it is. Also I feel that it\\'s a bit of a \"duty\" to let some hidden <<talents>> come through, and do\\n\\nExample 3: currently classified by the Drug Enforcement Administration as a Schedule 1 substance, which the DEA considers \"the most dangerous class of <<drugs>> with a high potential for abuse and potentially severe psychological and/or physical dependence.\" Other Schedule 1 <<drugs>> include heroin, ecstasy and\\n\\nExample 4: to be a liberal idea. You know, <<progressives>> are supposed to be against\\n\\nExample 5: end ishly r aves, And to which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the current study we analyse crazy qu <<ilts>> using spatial statistics, comparing them to ` normal ` regular qu <<ilts>>. Qu <<ilts>> in general are subject to\\n\\nStep 1:\\nACTIVATING TOKENS: \"Marines\", \"talents\", \"drugs\", \"progressives\", \"quilts\".\\n\\nStep 2:\\nGENERAL FEATURES SHARED:\\n- The neuron appears to activate on plural nouns, representing specific groups or objects.\\n- Some of these nouns have cultural or societal significance (e.g., \"Marines\", \"progressives\", \"drugs\").\\n- The token contexts vary widely, but there is a presence of quotation marks or additional clarifiers in the sentences.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"hips\", \"mith\", \"paces\", \"hip\", \"pace\", \"chool\", \"pring\", \"cale\", \"hare\", \"uits\"]\\n\\nSIMILAR TOKENS:\\n- The promoted logits suggest shortened or modified forms of plural nouns (e.g., \"hips\", \"paces\", \"uits\").\\n- These logits hint towards the plural form or parts of constructs that could be plural nouns.\\n\\nCombining Part 1 Observations and Part 2 Logits:\\n\\n- The neuron is likely focusing on plural nouns that either represent culturally significant groups or objects.\\n- The tokens boosted are often parts of words that can become pluralized forms, suggesting a pattern in how these nouns could form or appear plural.\\n\\n[EXPLANATION]: The neuron activates on plural nouns, particularly those with cultural or societal significance, and promotes plural or partially pluralized forms in the next token prediction.', 'interpretation': 'The neuron activates on plural nouns, particularly those with cultural or societal significance, and promotes plural or partially pluralized forms in the next token prediction.', 'f1_score': 0.696969696969697}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 14460, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \"actress\", \"like\", \"the\", \",\".\\n\\nStep 1.\\n- The activating tokens appear in varied contexts, from the word \"actress\" to common words like \"the\" and \",.\"\\n\\nStep 2.\\n- The examples don\\'t share a clear context or thematic pattern.\\n- The activating tokens include proper nouns and common conjunctions, articles, and punctuation.\\n\\nLooking for any hidden commonalities in the examples:\\n- Some activating tokens like \"actress\" and other context-specific terms might suggest some sort of role identification or importance in a narrative.\\n\\n(Part 2)\\n\\nSIMILAR TOKENS: \"director\", \"founder\", \"Associate\", \"director\", \"retired\", \"William\", \"Robert\", \"Managing\", \"Laure\", \"Georgetown\".\\n\\n- The top logits list contains titles, names, and positions of significance.\\n\\n[EXPLANATION]: The neuron activates on tokens that frequently precede or follow titles and names, indicating a focus on identifying key roles or important entities in a narrative.', 'interpretation': 'The neuron activates on tokens that frequently precede or follow titles and names, indicating a focus on identifying key roles or important entities in a narrative.', 'f1_score': 0.5238095238095238}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 15428, 'analysis': '### Step-by-Step Walkthrough\\n\\n(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , sleeps with sexy Marines and hangs <<out>> with her trans friends . E leanor , played by trans actress Zack ary Dru cker E leanor leads the support group that Maur a joins when she first comes <<out>> . G itt\\n\\nExample 2: not heavily genre bound . It � � s just good , electronic , psychedelic dance music . That � � s what it is . Also I feel that <<it>> � � s a bit of a � � duty � � to let some hidden talents come through , and do <<it>> without too much bureaucracy and hard times they might have encountered otherwise trying to push <<it>> forward . Running a label\\n\\nExample 3:  Obama said his focus on reforming laws that punish drug users , noting the racial disparity in drug arrests . \" We should not be locking up <<kids>> or individual users for long\\n\\nExample 4: for peace and prosperity . � � The Ron Paul Channel is set to launch officially in the first part of August . Follow Gabe on <<Twitter>> Spring Boot : REST +\\n\\nExample 5: other ? She crept from your <<side>> at the chim ing of\\n\\nStep 1.\\nACTIVATING TOKENS: \"out\", \"it\", \"kids\", \"Twitter\", \"side\".\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The examples include various contexts, such as hanging out with friends, discussing music, legal reform, and social media.\\n- Activating tokens are common words used in conversational or narrative contexts.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The activating tokens often mark prominent moments or actions in the narrative.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"whenever\", \"whilst\", \"because\", \"while\", \"anytime\", \"again\", \"sooner\", \"ASAP\", \"when\", \"during\"]\\n\\nStep 1.\\nSIMILAR TOKENS: \"whenever\", \"whilst\", \"because\", \"while\", \"anytime\", \"again\", \"sooner\", \"ASAP\", \"when\", \"during\".\\n- The top logits list primarily consists of temporal and causal conjunctions/adverbs.\\n\\n[EXPLANATION]: The neuron activates on common words that often precede clauses expressing temporal or causal relationships.', 'interpretation': 'The neuron activates on common words that often precede clauses expressing temporal or causal relationships.', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 15464, 'analysis': '### (Part 1) Analysis of Activating Tokens\\n\\n**Step 1: List a couple activating and contextual tokens you find interesting.**\\n- ACTIVATING TOKENS: \"is\", \"announced\", \"launched\", \"analyse\"\\n- PREVIOUS TOKENS: No discernible pattern.\\n\\n**Step 2: Shared features of the text examples**\\n- The activating tokens are action verbs or states of being: \"is\", \"announced\", \"launched\", \"analyse\".\\n- These verbs are in the past or present tense.\\n- The sentences involve introductory or definitive statements about actions taken or conditions stated.\\n\\nLet me look again for additional patterns or linguistic similarities:\\n- The activating tokens often appear in contexts where important actions or declarations are highlighted.\\n\\n### (Part 2) Analysis of Top_logits\\n\\n**Top_logits: [\"WASHINGTON\", \"Welcome\", \"reetings\", \" Donald\", \" bombshell\", \" celebrate\", \" embattled\", \"hello\", \"ONDON\", \" iconic\"]**\\n- The top logits include proper nouns (e.g., \"WASHINGTON\", \"ONDON\", \" Donald\"), greetings (e.g., \"elcome\", \"hello\"), and impactful or vivid words (e.g., \"bombshell\", \"celebrate\", \"iconic\").\\n\\n**Patterns in Top_logits:**\\n- The promoted tokens often signify important entities (proper nouns) or impactful actions/events.\\n- Many of the promoted tokens suggest the beginning of new information or greet someone, indicating introductions or announcements.\\n\\n### Final Explanation\\n[EXPLANATION]: Action verbs or states of being in sentences describing significant actions, declarations, or conditions.', 'interpretation': 'Action verbs or states of being in sentences describing significant actions, declarations, or conditions.', 'f1_score': 0.3333333333333333}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 15485, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: she volunteers at a suicide hotline, sleeps with sexy Marines and hangs out with her trans friends. E leanor, played by trans actress Zack ary <<Dru>> cker E leanor\\n\\nExample 2: I have no sufficient reasoning for doing so apart from the � � karm atic � � one explained above. Does your approach and set list <<differ>> at indoor\\n\\nExample 3: currently classified by the Drug Enforcement Administration as a Schedule 1 substance, which the DEA considers \" the most dangerous class of drugs with a <<high>> potential for abuse and potentially\\n\\nExample 4: to be a liberal idea. You know, progressives are supposed to be against war, but where are <<they>> now with Obama? �\\n\\nExample 5: And to which all the women are absolute slaves [ … ]. Sp atial Analysis of Patterns In the current study we analyse crazy qu ilts using spatial statistics, <<comparing>> them to ` normal �\\n\\nStep 1.\\nACTIVATING TOKENS: \"Dru\", \"differ\", \"high\", \"they\", \"comparing\".\\nPREVIOUS TOKENS: No clear common patterns emerged from previous tokens.\\n\\nStep 2.\\n- The examples show a variety of contexts ranging from personal descriptions and explanations to formal statistical analysis.\\n- Many of the activating tokens are part of words that denote comparison, measurement, or evaluation (\"differ\", \"high\", \"comparing\").\\n\\n(Part 2)\\nSIMILAR TOKENS: \"averages\", \"average\", \"median\", \"percentage\", \"Average\", \"averaged\", \"totals\", \"percentages\".\\n- The top logits list emphasizes statistical terms and numerical measures.\\n\\n[EXPLANATION]: Tokens related to comparison, measurement, and statistical evaluation.', 'interpretation': 'Tokens related to comparison, measurement, and statistical evaluation.', 'f1_score': 0.8}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 15553, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: a rich , full life where she volunteers at a suicide hotline , <<sleeps>> with sexy Marines and hangs\\n\\nExample 2: what it is . Also I feel that it � � s a bit of a � � duty � � to let some hidden talents come through , and <<do>> it without too much bureaucracy\\n\\nExample 3:  \" We should not be locking up kids or individual users for long stretches of jail time when some of the folks who are writing those laws have probably <<done>> the same thing ,\" he\\n\\nExample 4: to be a liberal idea . You know , progressives are supposed to be against <<war>> , but where are they\\n\\nExample 5: other ? She crept from your side at the chim ing of four And is down in the par lor at <<work>> on another . Your break\\n\\n(Part 1)\\n\\nACTIVATING TOKENS: \"sleeps\", \"do\", \"done\", \"war\", \"work\".\\nPREVIOUS TOKENS: No particularly striking patterns.\\n\\n\\nStep 1.\\n- The activating tokens include several verbs, both in simple and past tense.\\n- The previous tokens generally provide context, but don\\'t have a shared feature that stands out.\\n\\nStep 2.\\n- The examples feature action-oriented verbs.\\n- The verbs cover activities and actions carried out by subjects (people, in this case).\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- The activating tokens are not only action-oriented verbs but also contextualized within broader actions or activities.\\n\\n(Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nTop_logits: [\"outreach\", \"differently\", \"tirelessly\", \"diligently\", \"collabor\", \"wonders\", \"pez\", \"grunt\", \"overtime\", \"intensive\"]\\nBottom_logits: [\"antha\", \"opened\", \"bearer\", \"etus\", \"DragonMagazine\", \"isu\", \"ildo\", \"boarded\", \"ipient\", \"opol\"]\\n\\nSIMILAR TOKENS: \"tirelessly\", \"diligently\", \"overtime\", \"outreach\", \"intensive\".\\n- The top logits list includes words that describe the manner or intensity of actions, primarily in a positive context (e.g., \"tirelessly\", \"diligently\").\\n\\n\\n[EXPLANATION]: This neuron activates on verbs related to action or work, especially in contexts that highlight continuous effort or tasks.', 'interpretation': 'This neuron activates on verbs related to action or work, especially in contexts that highlight continuous effort or tasks.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 15656, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nACTIVATING TOKENS: \"being\", \"it\", \"it\", \":\", \":\" and \"from\".\\n\\nStep 1.\\n- The activating tokens are common pronouns (\"it\"), punctuation marks (\":\"), and a preposition (\"from\").\\n- The previous tokens have nothing in common.\\n\\nStep 2.\\n- The common tokens \"it\" and \"being\" are often connectors or pronouns within a sentence.\\n- The punctuation marks \":\" often appear in technical or structured text formats.\\n- The preposition \"from\" indicates a source or starting point.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No obvious patterns emerge from such diverse tokens.\\n\\n(Part 2) \\n\\nSIMILAR TOKENS: \"\\'\", \"\"\", \"#\", \"\\'\\'\", \"unfairly\", \"[\", \"…\", \"...\", \"\"\", \"inappropriate\".\\n- The top logits list suggests a focus on producing punctuation marks and certain adverbs or adjectives.\\n\\n[EXPLANATION]: Common pronouns, punctuation marks, and prepositions occurring in diverse sentence structures.', 'interpretation': 'Common pronouns, punctuation marks, and prepositions occurring in diverse sentence structures.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 15736, 'analysis': '(Part 1)\\n\\nACTIVATING TOKENS: \"she\", \"it\", \"he\", \"who\".\\nPREVIOUS TOKENS: No clear patterns.\\n\\nStep 1.\\n- The activating tokens are all pronouns (\"she\", \"it\", \"he\", \"who\").\\n- The previous tokens do not show a clear commonality.\\n\\nStep 2.\\n- The examples include various pronouns.\\n- Pronouns can refer to individuals or objects previously mentioned or implied in the context.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- No additional patterns identified.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"himself\", \"vowed\", \"testified\", \"apologized\", \"swore\", \"wrote\", \"oversaw\", \"resigned\", \"tweeted\", \"confessed\".\\n- The top logits list consists largely of verbs that describe actions typically performed by an individual, many carrying a sense of personal involvement or testimony.\\n\\n[EXPLANATION]: The neuron activates on pronouns referring to individuals, often prefacing verbs related to personal actions or declarations.', 'interpretation': 'The neuron activates on pronouns referring to individuals, often prefacing verbs related to personal actions or declarations.', 'f1_score': 0.7916666666666667}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'split'\n",
      "{'feature_index': 16724, 'analysis': '## Step-by-Step Walkthrough\\n\\n### Part 1: Tokens that the neuron activates highly on in text\\n\\nExamples:\\n1. G itt el , played by trans actress Har i N ef   G itt el is an ancestor of the P fe ff erm ans who lived <<in>> the famous H irsch feld Institute <<in>> Berlin and transitioned against the wishes of her mother . G itt el was arrested by the Nazis for being trans and eventually died <<in>> the Holocaust.\\n2. no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above .   Does your approach and set list differ at <<indoor>>\\n3. folks who are writing those laws have probably done the same thing ,\" he said .   In August , the Obama administration announced it would not stop Washington <<and>>\\n4. The Ron Paul Institute , a new think-tank � � for peace and prosperity . � �   The Ron Paul Channel is set to launch officially <<in>> the first part of August.\\n5. other ? She crept from your side at the chim ing of four And is down in the par <<lor>> at work on another.\\n\\n#### Step 1\\n\\n**ACTIVATING TOKENS:** \"in\", \"indoor\", \"and\", \"in\", \"lor\".\\n\\n**PREVIOUS TOKENS:** \"famous Hirsch feld Institute in Berlin\", \"set list differ at\", \"stop Washington and\", \"launch officially in the first\", \"down in the par lor at work\".\\n\\n### General Shared Features of the Text Examples:\\n\\n1. The activating tokens \"in\", \"indoor\", \"and\", all appear in prepositional or locational contexts. \\n2. These tokens often pertain to places or locations.\\n3. There are multiple instances of the token \"in\" being activated.\\n4. The previous tokens often denote a place or location (e.g., \"Institute in Berlin\", \"Washington\", \"lor\").\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"northwest\", \"southwest\", \"downtown\", \"northeast\", \"east\", \"suburbs\", \"suburb\", \"north\", \"south\", \"southeast\"]\\n\\n### Analysis:\\n\\n**SIMILAR TOKENS:** \"northwest\", \"southwest\", \"downtown\", \"northeast\", \"east\", \"suburbs\", \"suburb\", \"north\", \"south\", \"southeast\".\\n\\n- The top logits promoted by the neuron are all associated with directions or specific locations.\\n- This corresponds well with the observed activating tokens, which frequently involve locational context.\\n\\n### Final Explanation\\n\\n**[EXPLANATION]:** Tokens indicating locations or directions, especially involving the concept of being inside or within particular places, such as \"in\" or \"indoor\".', 'interpretation': '## Step-by-Step Walkthrough\\n\\n### Part 1: Tokens that the neuron activates highly on in text\\n\\nExamples:\\n1. G itt el , played by trans actress Har i N ef   G itt el is an ancestor of the P fe ff erm ans who lived <<in>> the famous H irsch feld Institute <<in>> Berlin and transitioned against the wishes of her mother . G itt el was arrested by the Nazis for being trans and eventually died <<in>> the Holocaust.\\n2. no sufficient reasoning for doing so apart from the � � k arm atic � � one explained above .   Does your approach and set list differ at <<indoor>>\\n3. folks who are writing those laws have probably done the same thing ,\" he said .   In August , the Obama administration announced it would not stop Washington <<and>>\\n4. The Ron Paul Institute , a new think-tank � � for peace and prosperity . � �   The Ron Paul Channel is set to launch officially <<in>> the first part of August.\\n5. other ? She crept from your side at the chim ing of four And is down in the par <<lor>> at work on another.\\n\\n#### Step 1\\n\\n**ACTIVATING TOKENS:** \"in\", \"indoor\", \"and\", \"in\", \"lor\".\\n\\n**PREVIOUS TOKENS:** \"famous Hirsch feld Institute in Berlin\", \"set list differ at\", \"stop Washington and\", \"launch officially in the first\", \"down in the par lor at work\".\\n\\n### General Shared Features of the Text Examples:\\n\\n1. The activating tokens \"in\", \"indoor\", \"and\", all appear in prepositional or locational contexts. \\n2. These tokens often pertain to places or locations.\\n3. There are multiple instances of the token \"in\" being activated.\\n4. The previous tokens often denote a place or location (e.g., \"Institute in Berlin\", \"Washington\", \"lor\").\\n\\n### Part 2: Tokens that the neuron boosts in the next token prediction\\n\\n**Top_logits:** [\"northwest\", \"southwest\", \"downtown\", \"northeast\", \"east\", \"suburbs\", \"suburb\", \"north\", \"south\", \"southeast\"]\\n\\n### Analysis:\\n\\n**SIMILAR TOKENS:** \"northwest\", \"southwest\", \"downtown\", \"northeast\", \"east\", \"suburbs\", \"suburb\", \"north\", \"south\", \"southeast\".\\n\\n- The top logits promoted by the neuron are all associated with directions or specific locations.\\n- This corresponds well with the observed activating tokens, which frequently involve locational context.\\n\\n### Final Explanation\\n\\n**[EXPLANATION]:** Tokens indicating locations or directions, especially involving the concept of being inside or within particular places, such as \"in\" or \"indoor\".', 'f1_score': 0.5238095238095238}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 16805, 'analysis': '### STEP-BY-STEP WALKTHROUGH ###\\n\\n#### (Part 1) Tokens that the neuron activates highly on in text\\n\\nStep 1: List a couple activating and contextual tokens you find interesting. \\n- ACTIVATING TOKENS: \"suicide\", \"dance\", \"drug\", \"war\", \"supper\".\\n- PREVIOUS TOKENS: \"psychological\", \"music\", \"progressives\", \"square\".\\n\\nStep 2: Write down general shared features of the text examples.\\n- The activating tokens appear to fall under varied categories, but there\\'s a pronounced emphasis on emotionally or socially charged words like \"suicide\", \"drug\", and \"war\".\\n- The context often involves descriptions, categorization, or discussion about these activating words.\\n\\n#### (Part 2) Tokens that the neuron boosts in the next token prediction\\n\\nThe neuron seems to promote tokens that are potentially related to objects or paraphernalia, especially focusing on words that have more informal, slang-like or substance-related meanings.\\n\\nTop_logits: [\"bag\", \"weed\", \"works\", \"balls\", \"wheel\", \"pipe\", \"seed\", \"powder\", \"maker\", \"bowl\"]\\n\\n- These tokens are generally nouns.\\n- They commonly represent physical objects.\\n- Many of these objects are associated with substance use (e.g., \"weed\", \"pipe\", \"powder\") or mundane items that could have informal or slang interpretations (e.g., \"balls\", \"bag\", \"maker\").\\n\\n#### (Final Explanation)\\nCombining insights from the text examples and the next token predictions:\\n\\nWhile the neuron isn\\'t contextually bound to any single domain, it frequently activates on tokens with significant social, emotional, or cultural meaning, often relating to vice, conflict, and lifestyle. Subsequent token predictions suggest a focus on nouns representing objects, frequently those associated with substance use or informal settings.\\n\\n---\\n[EXPLANATION]: Tokens representing socially, emotionally, or culturally charged words, especially those related to substance use, conflict, and lifestyle.', 'interpretation': 'Tokens representing socially, emotionally, or culturally charged words, especially those related to substance use, conflict, and lifestyle.', 'f1_score': 0.16666666666666666}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'feature_index': 16892, 'analysis': '(Part 1) Tokens that the neuron activates highly on in text\\n\\nExample 1: friends .   E leanor , played by trans actress Zack ary Dru cker   E leanor leads the support group that Maur a joins when she first <<comes>> out .   G\\n\\nExample 2: That � � s what it is . Also I feel that it � � s a bit of a � � duty � � to let some hidden talents <<come>> through , and do it\\n\\nExample 3: .   Obama said his focus on reforming laws that punish drug users , noting the racial disparity in drug arrests .   \" We should not be <<locking>> up kids or individual users\\n\\nExample 4: Paul who recently launched The Ron Paul Institute , a new think - tank � � for peace and prosperity . � �   The Ron Paul Channel is <<set>> to launch officially in the\\n\\nExample 5: other ? She <<crept>> from your side at the\\n\\nStep 1.\\nACTIVATING TOKENS: \"comes\", \"come\", \"locking\", \"set\", \"crept\"\\nPREVIOUS TOKENS: No interesting patterns.\\n\\nStep 2.\\n- The activating tokens are mostly verbs.\\n- They indicate actions or movements.\\n\\nLet me think carefully. Did I miss any patterns in the text examples? Are there any more linguistic similarities?\\n- Yes, I missed one: The activating verbs often indicate some initiation or state of change/movement.\\n\\n(Part 2)\\nSIMILAR TOKENS: \"into\", \"away\", \"forward\", \"down\", \"forth\", \"onto\", \"out\", \"INTO\", \"overboard\", \"Into\".\\n- The top logits list contains prepositions or adverbs that indicate direction, movement, or transition.\\n\\n[EXPLANATION]: Verbs indicating an initiation, state of change, or movement followed by direction or transition indicating tokens.', 'interpretation': 'Verbs indicating an initiation, state of change, or movement followed by direction or transition indicating tokens.', 'f1_score': 0.898989898989899}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interp_results_mlp = []\n",
    "\n",
    "for f in tqdm(range(len(feature_indices))):\n",
    "    #print(f\"Analyzing Feature Index: {f} (original feature: {feature_indices[f]})\")\n",
    "    try:\n",
    "            # formatted_prompt, analysis, interp_text, scoring_text, false_text = analyze_feature(f, sae, transformer_model, owt_tokens_torch, scores, \n",
    "            #                                                 top_k_indices, top_k_batch_indices, top_k_tokens, top_k_tokens_str, top_k_scores_per_seq,\n",
    "            #                                                 config)\n",
    "        formatted_prompt, analysis, interp_text, scoring_text, false_text = analyze_feature(f, mlp, transformer_model, owt_tokens_torch, scores, top_k_indices, top_k_batch_indices, top_k_tokens, top_k_tokens_str, top_k_scores_per_seq, config)\n",
    "\n",
    "        # print(formatted_prompt)\n",
    "        # print(analysis)\n",
    "        interpretation = analysis.split(\"[EXPLANATION]: \")[-1]\n",
    "        scoring_prompt = format_score_prompt(interpretation=interpretation, examples=scoring_text, false_examples=false_text)\n",
    "        scoring_response = get_ai_response(scoring_prompt, config)\n",
    "        f1 = calculate_f1_score(scoring_response, scoring_text, false_text)\n",
    "\n",
    "        results_dict = {\n",
    "            \"feature_index\": feature_indices[f],\n",
    "            \"analysis\": analysis,\n",
    "            \"interpretation\": interpretation,\n",
    "            \"f1_score\": f1\n",
    "        }\n",
    "        print(results_dict)\n",
    "        print('\\n\\n\\n')\n",
    "        interp_results_mlp.append(results_dict)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results dict as json\n",
    "import json\n",
    "\n",
    "with open('interp_results_mlp.json', 'w') as f:\n",
    "    json.dump(interp_results_mlp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "x=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "nbinsx": 15,
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "histogram",
         "x": [
          0.6703296703296704,
          0.6703296703296704,
          1,
          0.4,
          0.696969696969697,
          0.3333333333333333,
          0.3333333333333333,
          0.5833333333333333,
          1,
          0.7916666666666667,
          0.6703296703296704,
          0.6,
          0.7916666666666667,
          0.4,
          0.6703296703296704,
          0.898989898989899,
          0.898989898989899,
          0.5238095238095238,
          0.696969696969697,
          0.898989898989899,
          0.3333333333333333,
          0.29292929292929293,
          0.898989898989899,
          0.45054945054945056,
          0.375,
          0.7916666666666667,
          0.4,
          0.3333333333333333,
          0.6703296703296704,
          0.6,
          0.7916666666666667,
          0.898989898989899,
          0.898989898989899,
          0.7916666666666667,
          0.5238095238095238,
          0.7916666666666667,
          0.898989898989899,
          0.6703296703296704,
          0.898989898989899,
          0.898989898989899,
          0.7916666666666667,
          0.7916666666666667,
          0.696969696969697,
          0.375,
          0.898989898989899,
          0.898989898989899,
          0.898989898989899,
          0.6703296703296704,
          0.7916666666666667,
          0.7916666666666667,
          0.29292929292929293,
          0.6703296703296704,
          1,
          0.4949494949494949,
          0.7916666666666667,
          0.696969696969697,
          0.45054945054945056,
          0.6703296703296704,
          0.6703296703296704,
          0.898989898989899,
          0.5238095238095238,
          0.6,
          0.7916666666666667,
          0.696969696969697,
          0.3333333333333333,
          0.6703296703296704,
          0.45054945054945056,
          0.696969696969697,
          0.6,
          0.898989898989899,
          0.898989898989899,
          0.898989898989899,
          0.5833333333333333,
          0.696969696969697,
          0.3333333333333333,
          0.45054945054945056,
          0.4949494949494949,
          0.6703296703296704,
          0.8,
          1,
          0.5833333333333333,
          0.6,
          1,
          0.3333333333333333,
          0.29292929292929293,
          0.45054945054945056,
          0.898989898989899,
          0.6703296703296704,
          1,
          0.5238095238095238,
          0.696969696969697,
          0.6,
          1,
          0.5238095238095238,
          0.8,
          0.696969696969697,
          0.5238095238095238,
          0.3333333333333333,
          0.3333333333333333,
          0.8,
          0.898989898989899,
          0.898989898989899,
          0.7916666666666667,
          0.5238095238095238,
          0.16666666666666666,
          0.898989898989899
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "Mean F1: 0.67",
          "x": 0.6689928781909914,
          "xanchor": "left",
          "xref": "x",
          "y": 1,
          "yanchor": "top",
          "yref": "y domain"
         }
        ],
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "shapes": [
         {
          "line": {
           "color": "red",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0.6689928781909914,
          "x1": 0.6689928781909914,
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Histogram of F1 Scores"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "f1_scores = [r[\"f1_score\"] for r in interp_results_mlp]\n",
    "\n",
    "# Histogram of F1 scores\n",
    "n_bins = 15\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "fig = px.histogram(x=f1_scores, nbins=n_bins, title=\"Histogram of F1 Scores\", width=600)\n",
    "# Red vertical line for the mean\n",
    "fig.add_vline(x=mean_f1_score, line_dash=\"dash\", line_color=\"red\", annotation_text=f\"Mean F1: {mean_f1_score:.2f}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading complete f1 scores from Vast.ai (the previous scores are a subset of these, but didn't save the full output from Vast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Reset defaults\n",
    "plt.rcdefaults()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "BASE_FONT_SIZE = 16\n",
    "DASHED_LINE_WIDTH = 2\n",
    "\n",
    "# Load combined f1 scores mlp\n",
    "combined_f1_scores_mlp = np.load('f1_scores_mlp.npy')\n",
    "# Load combined f1 scores sae\n",
    "combined_f1_scores_sae = np.load('f1_scores_sae.npy')\n",
    "\n",
    "# Create figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Function to plot histogram and add mean label\n",
    "def plot_histogram(ax, data, mean, color, title):\n",
    "    n, bins, patches = ax.hist(data, bins=15, alpha=0.7, color=color)  # Removed edgecolor argument\n",
    "    ax.axvline(mean, color='red', linestyle='dashed', linewidth=DASHED_LINE_WIDTH)\n",
    "    \n",
    "    # Adjust the position of the mean label\n",
    "    label_x = mean + 0.02  # Move label slightly to the right\n",
    "    label_y = ax.get_ylim()[1] * 1.1  # Move label slightly down from the top\n",
    "    \n",
    "    ax.text(label_x, label_y, f'Mean: {mean:.2f}',\n",
    "            horizontalalignment='left', verticalalignment='top', fontsize=BASE_FONT_SIZE,\n",
    "            bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "    \n",
    "    ax.set_title(title, fontsize=BASE_FONT_SIZE+2)\n",
    "    ax.set_xlabel('F1 Score', fontsize=BASE_FONT_SIZE)\n",
    "    ax.tick_params(labelsize=BASE_FONT_SIZE-5)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "mean_f1_score_sae = np.mean(combined_f1_scores_sae)\n",
    "std_f1_score_sae = np.std(combined_f1_scores_sae)\n",
    "mean_f1_score_mlp = np.mean(combined_f1_scores_mlp)\n",
    "std_f1_score_mlp = np.std(combined_f1_scores_mlp)\n",
    "\n",
    "# Plot MLP histogram\n",
    "plot_histogram(ax1, combined_f1_scores_mlp, mean_f1_score_mlp, 'skyblue', 'MLP')\n",
    "ax1.set_ylabel('Frequency', fontsize=BASE_FONT_SIZE)\n",
    "\n",
    "# Plot SAE histogram\n",
    "plot_histogram(ax2, combined_f1_scores_sae, mean_f1_score_sae, 'lightgreen', 'SAE')\n",
    "\n",
    "# Set the same y-axis range for both subfigures\n",
    "max_count = max(\n",
    "    max(np.histogram(combined_f1_scores_mlp, bins=15)[0]),\n",
    "    max(np.histogram(combined_f1_scores_sae, bins=15)[0])\n",
    ")\n",
    "ax1.set_ylim(0, max_count * 1.5)\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "ax2.set_ylim(0, max_count * 1.5)\n",
    "ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Use MaxNLocator to get integer tick locations on y-axis\n",
    "ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax2.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"f1_score_distribution.pdf\", bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 650x325 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sae = combined_f1_scores_sae\n",
    "mlp = combined_f1_scores_mlp\n",
    "\n",
    "# plt.figure(figsize=.45 * np.array((10, 5)))\n",
    "# plt.boxplot([sae, mlp])\n",
    "# plt.xticks([1, 2], ['SAE', 'MLP'])\n",
    "# plt.title('Interpretability F1 Score')\n",
    "# plt.ylabel('F1 scores')\n",
    "# plt.text(1.15, .75, f'N={len(sae)}\\n' + r'$\\varnothing$=%.2f' % np.mean(sae))\n",
    "# plt.text(2.15, .75, f'N={len(mlp)}\\n' + r'$\\varnothing$=%.2f' % np.mean(mlp))\n",
    "# plt.ylim(0, 1)\n",
    "# plt.grid()\n",
    "# sns.despine(offset=10)\n",
    "# plt.savefig('fig_wild.pdf')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=0.65 * np.array((10, 5)))  # Increased figure size slightly\n",
    "plt.boxplot([sae, mlp])\n",
    "plt.xticks([1, 2], ['SAE', 'MLP'])\n",
    "plt.title('Interpretability F1 Score')\n",
    "plt.ylabel('F1 scores')\n",
    "\n",
    "# Adjust text positioning slightly to avoid overlapping\n",
    "plt.text(1.15, .75, f'N={len(sae)}\\n' + r'$\\varnothing$=%.2f' % np.mean(sae))\n",
    "plt.text(2.15, .75, f'N={len(mlp)}\\n' + r'$\\varnothing$=%.2f' % np.mean(mlp))\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "sns.despine(offset=10)\n",
    "\n",
    "# Use bbox_inches='tight' to avoid cutting off parts\n",
    "plt.savefig('fig_wild.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
