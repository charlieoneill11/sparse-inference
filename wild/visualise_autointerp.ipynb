{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "from IPython.display import HTML, display\n",
    "import numpy\n",
    "\n",
    "def convert_clean_text(clean_text, k=1, tokens_left=30, tokens_right=5):\n",
    "    \"\"\"\n",
    "    Wraps the top k scoring tokens in `<< >>` within the clean_text.\n",
    "    Also, keeps tokens_left tokens before and tokens_right tokens after each wrapped token.\n",
    "    If there are fewer than k non-zero tokens, wraps as many as there are.\n",
    "    \n",
    "    Parameters:\n",
    "    - clean_text (str): The input string containing tokens and their scores, separated by \" | \".\n",
    "    - k (int): The number of top scoring tokens to wrap. Defaults to 5.\n",
    "    - tokens_left (int): Number of tokens to keep before each top token. Defaults to 30.\n",
    "    - tokens_right (int): Number of tokens to keep after each top token. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The modified text with top k tokens wrapped in `<< >>` and surrounding context.\n",
    "    \"\"\"\n",
    "    # Split the clean text on the \"|\" separator\n",
    "    token_score_pairs = clean_text.split(\" | \")\n",
    "\n",
    "    # Remove the first token if present\n",
    "    if token_score_pairs:\n",
    "        token_score_pairs = token_score_pairs[1:]\n",
    "\n",
    "    # Initialize a list to hold tuples of (token, score)\n",
    "    tokens_with_scores = []\n",
    "\n",
    "    # Define regex to capture tokens with scores\n",
    "    token_score_pattern = re.compile(r\"^(.+?) \\((\\d+\\.\\d+)\\)$\")\n",
    "\n",
    "    for token_score in token_score_pairs:\n",
    "        match = token_score_pattern.match(token_score.strip())\n",
    "        if match:\n",
    "            token = match.group(1)\n",
    "            score = float(match.group(2))\n",
    "            tokens_with_scores.append((token, score))\n",
    "        else:\n",
    "            # Handle cases where score is zero or absent\n",
    "            token = token_score.split(' (')[0].strip()\n",
    "            tokens_with_scores.append((token, 0.0))\n",
    "\n",
    "    # Sort tokens by score in descending order\n",
    "    sorted_tokens = sorted(tokens_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top k tokens with non-zero scores\n",
    "    top_k_tokens = [token for token, score in sorted_tokens if score > 0][:k]\n",
    "\n",
    "    # Find all indices of top k tokens\n",
    "    top_k_indices = [i for i, (token, score) in enumerate(tokens_with_scores) if token in top_k_tokens and score >0]\n",
    "\n",
    "    # Define windows around each top token\n",
    "    windows = []\n",
    "    for idx in top_k_indices:\n",
    "        start = max(0, idx - tokens_left)\n",
    "        end = min(len(tokens_with_scores) - 1, idx + tokens_right)\n",
    "        windows.append((start, end))\n",
    "\n",
    "    # Merge overlapping windows\n",
    "    merged_windows = []\n",
    "    for window in sorted(windows, key=lambda x: x[0]):\n",
    "        if not merged_windows:\n",
    "            merged_windows.append(window)\n",
    "        else:\n",
    "            last_start, last_end = merged_windows[-1]\n",
    "            current_start, current_end = window\n",
    "            if current_start <= last_end + 1:\n",
    "                # Overlapping or adjacent windows, merge them\n",
    "                merged_windows[-1] = (last_start, max(last_end, current_end))\n",
    "            else:\n",
    "                merged_windows.append(window)\n",
    "\n",
    "    # Collect all unique indices within the merged windows\n",
    "    selected_indices = set()\n",
    "    for start, end in merged_windows:\n",
    "        selected_indices.update(range(start, end + 1))\n",
    "\n",
    "    # Create the converted tokens list with wrapping\n",
    "    converted_tokens = []\n",
    "    for i, (token, score) in enumerate(tokens_with_scores):\n",
    "        if i in selected_indices:\n",
    "            if token in top_k_tokens and score > 0:\n",
    "                token = f\"<<{token}>>\"\n",
    "            converted_tokens.append(token)\n",
    "        # Else, skip tokens outside the selected windows\n",
    "\n",
    "    # Join the converted tokens into a single string\n",
    "    converted_text = \" \".join(converted_tokens)\n",
    "    return converted_text\n",
    "\n",
    "def highlight_scores_in_html(\n",
    "    token_strs,\n",
    "    scores,\n",
    "    seq_idx,\n",
    "    max_color=\"#ff8c00\",\n",
    "    zero_color=\"#ffffff\",\n",
    "    show_score=True,\n",
    "):\n",
    "    if len(token_strs) != len(scores):\n",
    "        print(\"Length mismatch between tokens and scores\")\n",
    "        return \"\", \"\"\n",
    "    scores_min = min(scores)\n",
    "    scores_max = max(scores)\n",
    "    scores_normalized = (np.array(scores) - scores_min) / (scores_max - scores_min)\n",
    "    max_color_vec = np.array(\n",
    "        [int(max_color[1:3], 16), int(max_color[3:5], 16), int(max_color[5:7], 16)]\n",
    "    )\n",
    "    zero_color_vec = np.array(\n",
    "        [int(zero_color[1:3], 16), int(zero_color[3:5], 16), int(zero_color[5:7], 16)]\n",
    "    )\n",
    "    color_vecs = np.einsum(\"i, j -> ij\", scores_normalized, max_color_vec) + np.einsum(\n",
    "        \"i, j -> ij\", 1 - scores_normalized, zero_color_vec\n",
    "    )\n",
    "    color_strs = [f\"#{int(x[0]):02x}{int(x[1]):02x}{int(x[2]):02x}\" for x in color_vecs]\n",
    "    if show_score:\n",
    "        tokens_html = \"\".join(\n",
    "            [\n",
    "                f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}<span class='feature_val'> ({scores[i]:.2f})</span></span>\"\"\"\n",
    "                for i, token_str in enumerate(token_strs)\n",
    "            ]\n",
    "        )\n",
    "        clean_text = \" | \".join(\n",
    "            [f\"{token_str} ({scores[i]:.2f})\" for i, token_str in enumerate(token_strs)]\n",
    "        )\n",
    "    else:\n",
    "        tokens_html = \"\".join(\n",
    "            [\n",
    "                f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}</span>\"\"\"\n",
    "                for i, token_str in enumerate(token_strs)\n",
    "            ]\n",
    "        )\n",
    "        clean_text = \" | \".join(token_strs)\n",
    "    head = \"\"\"\n",
    "    <style>\n",
    "        span.token {\n",
    "            font-family: monospace;\n",
    "            border-style: solid;\n",
    "            border-width: 1px;\n",
    "            border-color: #dddddd;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    return head + tokens_html, convert_clean_text(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_html = []\n",
    "examples_clean_text = []\n",
    "j = 14\n",
    "for i in range(k):\n",
    "    #for j in range(100):\n",
    "    try:\n",
    "        example_html, clean_text = highlight_scores_in_html(top_k_tokens_str[i], top_k_scores_per_seq[i][j], top_k_seq_indices[i], show_score=True)\n",
    "        examples_html.append(example_html)\n",
    "        examples_clean_text.append(clean_text)\n",
    "        print(f\"Got one! i={i}, j={j}\")\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples_html:\n",
    "    display(HTML(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
